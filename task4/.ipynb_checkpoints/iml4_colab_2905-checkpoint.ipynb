{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJjuEWBZIfzs",
    "outputId": "7158cb70-4dd3-4047-ab2b-bea51aeed126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'iml-hs21': No such file or directory\n",
      "User name: hanyao8\n",
      "Password: ··········\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN ONCE TO SETUP THE ENVIRONMENT\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "import os\n",
    "from random import sample\n",
    "from glob import glob\n",
    "\n",
    "# cleanup previouiml runs\n",
    "!rm -r iml-hs21\n",
    "\n",
    "# load data from github repo\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password) # your password is converted into url format\n",
    "repo_name = 'iml-hs21'\n",
    "cmd_string = 'git clone https://{0}:{1}@github.com/{2}/{3}.git'.format(user, password, 'hanyao8', repo_name)\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOhyXBIZOovQ",
    "outputId": "a22b97b3-e2f9-423c-acb0-42cfdad05aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/iml-hs21/task4\n",
      "custom_objects_2.py  data  preprocessor.py   task4_2005.py    task4_utils.py\n",
      "custom_objects.py    jobs  task4_2005.ipynb  task4_models.py\n",
      "2.5.0\n",
      "2.5.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "%cd /content/iml-hs21/task4\n",
    "!ls\n",
    "\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(keras.__version__==\"2.5.0\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41uKi40VXoRX"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from task4_utils import *\n",
    "import task4_models\n",
    "import custom_objects\n",
    "import custom_objects_2\n",
    "\n",
    "import preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6WA2dmdO8si",
    "outputId": "3b5b0847-9df9-4e9f-dd1c-a961fe1c67f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "/content/iml-hs21/task4\n",
      "Job Start Time = 05_29_11_09_03\n",
      "\n",
      "Current Job Path: /content/iml-hs21/task4/jobs/job_05_29_11_09_03 \n",
      "\n",
      "Tensorflow ver. 2.5.0\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "\n",
    "#setup logging\n",
    "#setup new job directory\n",
    "CWD = os.getcwd()\n",
    "print(CWD)\n",
    "\n",
    "now = datetime.now()\n",
    "job_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"Job Start Time =\", job_time)\n",
    "\n",
    "if not(os.path.exists(\"jobs\")):\n",
    "    os.mkdir(\"jobs\")\n",
    "CURRENT_JOB_PATH = os.path.join(*[CWD,\"jobs\",\"job_\"+job_time])\n",
    "os.mkdir(CURRENT_JOB_PATH)\n",
    "JOB_LOG = open(os.path.join(CURRENT_JOB_PATH,\"log.txt\"),\"a\")\n",
    "JOB_LOG.write(\"init\\n\")\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "os. environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "\n",
    "# automatically chooses parameters\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tpus = tf.config.experimental.list_physical_devices('TPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "#elif tpus:\n",
    "#    print(\"TPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTsTBAfV6y2m"
   },
   "outputs": [],
   "source": [
    "JOB_LOG.write(\"Current job path: \")\n",
    "JOB_LOG.write(CURRENT_JOB_PATH)\n",
    "JOB_LOG.write(\"\\n\")\n",
    "\n",
    "POS_TRAIN_DATASET_SETTING = \"default\"\n",
    "#POS_TRAIN_DATASET_SETTING = \"no_hold\"\n",
    "#POS_TRAIN_DATASET_SETTING = 1000\n",
    "\n",
    "HOLD_FRAC = 0.01\n",
    "\n",
    "#MODEL_CHOICE = \"siamese1\"\n",
    "#MODEL_CHOICE = \"siamese2\"\n",
    "#MODEL_CHOICE = \"siamese_xception_2\"\n",
    "#MODEL_CHOICE = \"siamese_xception_3\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot_2\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot_3\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot_4\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot_5\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dist\"\n",
    "#MODEL_CHOICE = \"siamese_xception_dot\"\n",
    "MODEL_CHOICE = \"siamese_xception_dot_2\"\n",
    "#MODEL_CHOICE = \"siamese_xception_dot_3\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_ADAM_STEP = 3.0e-5\n",
    "TRAIN_EPOCHS = 10\n",
    "\n",
    "MULTITASK = True\n",
    "LAMBDA_1 = 0.0\n",
    "LAMBDA_2 = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDnBsEi-Oqrb"
   },
   "outputs": [],
   "source": [
    "if \"xception\" in MODEL_CHOICE:\n",
    "    TARGET_SHAPE = (299,299)\n",
    "else:\n",
    "    TARGET_SHAPE = (224,224)\n",
    "\n",
    "MT_WEIGHTS = [LAMBDA_1,LAMBDA_2]\n",
    "\n",
    "experiment_params = {\n",
    "    \"HOLD_FRAC\": HOLD_FRAC,\n",
    "    \"POS_TRAIN_DATASET_SETTING\": POS_TRAIN_DATASET_SETTING,\n",
    "    \"MODEL_CHOICE\": MODEL_CHOICE,\n",
    "    \"TARGET_SHAPE\":TARGET_SHAPE,\n",
    "    \"MULTITASK_WEIGHTS\":MT_WEIGHTS,\n",
    "    \"TRAIN_ADAM_STEP\": TRAIN_ADAM_STEP,\n",
    "    \"TRAIN_EPOCHS\": TRAIN_EPOCHS,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"MULTITASK\": MULTITASK\n",
    "}\n",
    "\n",
    "\n",
    "JOB_LOG.write(str(experiment_params))\n",
    "JOB_LOG.write(\"\\n\")\n",
    "\n",
    "prep = preprocessor.Preprocessor(\n",
    "    target_shape=TARGET_SHAPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    multitask=MULTITASK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNhDfwEsxs0Q",
    "outputId": "4001d727-9938-456c-c400-df6257a54baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Time = 05_29_11_09_03\n",
      "1192\n",
      "117838\n",
      "59544\n",
      "This Time = 05_29_11_09_52\n",
      "\n",
      "Current Job Path: /content/iml-hs21/task4/jobs/job_05_29_11_09_03 \n",
      "\n",
      "117838\n",
      "117838\n"
     ]
    }
   ],
   "source": [
    "### Get data\n",
    "sample = pd.read_table(\"data/sample.txt\",header=None)\n",
    "pos_train_hold_triplets = pd.read_table(\"data/train_triplets.txt\",delimiter=\" \",header=None,dtype=str)\n",
    "pos_train_triplets, pos_hold_triplets = train_test_split(pos_train_hold_triplets,test_size=HOLD_FRAC)\n",
    "test_triplets = pd.read_table(\"data/test_triplets.txt\",delimiter=\" \",header=None,dtype=str)\n",
    "\n",
    "if POS_TRAIN_DATASET_SETTING == \"default\":\n",
    "    pos_train_dataset_size = pos_train_triplets.shape[0]\n",
    "\n",
    "elif POS_TRAIN_DATASET_SETTING == \"no_hold\":\n",
    "    hold_codes = pd.unique(pos_hold_triplets[0])\n",
    "    hold_codes = np.concatenate((hold_codes,pd.unique(pos_hold_triplets[1])))\n",
    "    hold_codes = np.concatenate((hold_codes,pd.unique(pos_hold_triplets[2])))\n",
    "    hold_codes = pd.unique(hold_codes)\n",
    "    print(hold_codes.shape[0])\n",
    "\n",
    "    df = pos_train_triplets.copy()\n",
    "\n",
    "    print(df.shape)\n",
    "    for i in range(hold_codes.shape[0]):\n",
    "        for j in range(3):\n",
    "            df = df[df[j] != hold_codes[i]]\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    print(df.shape)\n",
    "    pos_train_triplets = df\n",
    "    pos_train_dataset_size = pos_train_triplets.shape[0]\n",
    "    print(pos_train_dataset_size)\n",
    "\n",
    "elif type(POS_TRAIN_DATASET_SETTING)==int:\n",
    "    pos_train_dataset_size = POS_TRAIN_DATASET_SETTING\n",
    "    pos_train_triplets = pos_train_triplets[:pos_train_dataset_size]\n",
    "\n",
    "else:\n",
    "    raise (Exception)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "if MULTITASK:\n",
    "    train_triplets = triplets_from_pos(pos_train_triplets)\n",
    "    y_train_groundtruth = gt_from_df(train_triplets)\n",
    "else:\n",
    "    train_triplets = pos_train_triplets\n",
    "    y_train_groundtruth = []\n",
    "\n",
    "train_dataset_size = train_triplets.shape[0]\n",
    "\n",
    "hold_triplets = triplets_from_pos(pos_hold_triplets)\n",
    "hold_dataset_size = hold_triplets.shape[0]\n",
    "hold_dataset = hold_dataset_from_df(hold_triplets,prep=prep)\n",
    "y_hold_groundtruth = gt_from_df(hold_triplets)\n",
    "\n",
    "if POS_TRAIN_DATASET_SETTING == \"no_hold\":\n",
    "    train_dataset = train_val_dataset_from_df(train_triplets,y_train_groundtruth,\n",
    "                            train_dataset_size,val_frac=0,prep=prep)\n",
    "    #val_dataset = hold_dataset_from_df(pos_hold_triplets,prep=prep)\n",
    "    val_dataset = train_val_dataset_from_df(hold_triplets,y_hold_groundtruth,\n",
    "                            hold_dataset_size,val_frac=0,prep=prep)\n",
    "else:\n",
    "    train_dataset, val_dataset = train_val_dataset_from_df(train_triplets,\n",
    "                            y_train_groundtruth,\n",
    "                            train_dataset_size,val_frac=0.01,prep=prep)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = test_dataset_from_df(test_triplets,prep=prep)\n",
    "test_dataset_size = test_triplets.shape[0]\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "print(y_train_groundtruth.shape[0])\n",
    "print(train_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-LEmGforkCt",
    "outputId": "66dac010-edd3-4333-e3e7-12c25553599c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, hold, test .__len__(): (indicates # batches in dataset)\n",
      "tf.Tensor(5892, shape=(), dtype=int64)\n",
      "tf.Tensor(1473, shape=(), dtype=int64)\n",
      "tf.Tensor(75, shape=(), dtype=int64)\n",
      "tf.Tensor(3722, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"train, val, hold, test .__len__(): (indicates # batches in dataset)\")\n",
    "print(train_dataset.__len__())\n",
    "print(val_dataset.__len__())\n",
    "print(hold_dataset.__len__())\n",
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXvM3duXqED_",
    "outputId": "60eff175-d9d9-426a-e1c2-5bddf01f24cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels.h5\n",
      "91889664/91884032 [==============================] - 1s 0us/step\n",
      "91897856/91884032 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#siamese_model = SiameseModel(siamese_network)\n",
    "if MODEL_CHOICE==\"siamese1\":\n",
    "    siamese_network = create_siamese()\n",
    "elif MODEL_CHOICE==\"siamese2\":\n",
    "    siamese_network = task4_models.create_siamese2()\n",
    "elif MODEL_CHOICE==\"siamese_xception\":\n",
    "    siamese_network = task4_models.create_siamese_xception()\n",
    "elif MODEL_CHOICE==\"siamese_xception_2\":\n",
    "    siamese_network = create_siamese_xception_2()\n",
    "elif MODEL_CHOICE==\"siamese_xception_3\":\n",
    "    siamese_network = create_siamese_xception_3()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot\":\n",
    "    siamese_network = create_siamese_mobilenet_dot()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot_2\":\n",
    "    siamese_network = task4_models.create_siamese_mobilenet_dot_2()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot_3\":\n",
    "    siamese_network = task4_models.create_siamese_mobilenet_dot_3()\n",
    "    #siamese_network = create_siamese_mobilenet_dot_4()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot_4\":\n",
    "    siamese_network = task4_models.create_siamese_mobilenet_dot_4()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot_5\":\n",
    "    siamese_network = task4_models.create_siamese_mobilenet_dot_5()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dist\":\n",
    "    siamese_network = create_siamese_mobilenet_dist()\n",
    "elif MODEL_CHOICE==\"siamese_xception_dot\":\n",
    "    siamese_network = task4_models.create_siamese_xception_dot()\n",
    "elif MODEL_CHOICE==\"siamese_xception_dot_2\":\n",
    "    siamese_network = task4_models.create_siamese_xception_dot_2()\n",
    "elif MODEL_CHOICE==\"siamese_xception_dot_3\":\n",
    "    siamese_network = task4_models.create_siamese_xception_dot_3()\n",
    "\n",
    "#siamese_model = custom_objects_2.SiameseModel5(siamese_network,margin=1.0,mt_weights=MT_WEIGHTS)\n",
    "#siamese_model = custom_objects_2.SiameseModel6(siamese_network,margin=1.0)\n",
    "#siamese_model = custom_objects_2.SiameseModel7(siamese_network,margin=1.0,mt_weights=MT_WEIGHTS)\n",
    "#siamese_model = SiameseModel8(siamese_network,margin=1.0,mt_weights=MT_WEIGHTS)\n",
    "#siamese_model = SiameseModel9(siamese_network,margin=1.0,mt_weights=MT_WEIGHTS)\n",
    "siamese_model = custom_objects_2.SiameseModel10(siamese_network,margin=1.0,mt_weights=MT_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72SvUGi0BnKI",
    "outputId": "b308df0b-ea1d-4410-f5e0-7379fbf0611c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Time = 05_29_11_09_57\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "d_hold = get_d(siamese_model,hold_dataset,hold_dataset_size,prep)\n",
    "d_hold_save_path = os.path.join(CURRENT_JOB_PATH,\"d_hold.npy\")\n",
    "with open(d_hold_save_path,'wb') as f:\n",
    "    np.save(f,d_hold)\n",
    "\n",
    "y_hold_pred = np.sign(d_hold[:,1]-d_hold[:,0])\n",
    "y_hold_pred = (y_hold_pred+1)/2\n",
    "y_hold_pred = y_hold_pred.astype(int)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "#job_05_22_19_12_28\n",
    "\n",
    "acc = accuracy_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"acc %f\"%acc)\n",
    "\n",
    "rec = recall_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"rec %f\"%rec)\n",
    "\n",
    "prec = precision_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"prec %f\"%prec)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "#y.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zQb4VBLUPRa"
   },
   "outputs": [],
   "source": [
    "#Experiment\n",
    "if TRAIN_EPOCHS > 0:\n",
    "    now = datetime.now()\n",
    "    this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    print(\"This Time =\", this_time)\n",
    "    print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(CURRENT_JOB_PATH,\"save_at_{epoch}.ckpt\")),]\n",
    "\n",
    "    siamese_model.compile(optimizer=optimizers.Adam(TRAIN_ADAM_STEP))\n",
    "    history = siamese_model.fit(train_dataset, epochs=TRAIN_EPOCHS,callbacks=callbacks,\n",
    "                                validation_data=val_dataset)\n",
    "\n",
    "    now = datetime.now()\n",
    "    this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    print(\"This Time =\", this_time)\n",
    "    print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "    print(history)\n",
    "    print(history.history.keys())\n",
    "\n",
    "    JOB_LOG.write(str(history.params))\n",
    "    JOB_LOG.write(\"\\n\")\n",
    "    JOB_LOG.write(\"Loss: \")\n",
    "    JOB_LOG.write(str(history.history[\"loss\"]))\n",
    "    JOB_LOG.write(\"\\n\")\n",
    "    JOB_LOG.write(\"Val Loss: \")\n",
    "    JOB_LOG.write(str(history.history[\"val_loss\"]))\n",
    "    JOB_LOG.write(\"\\n\")\n",
    "    JOB_LOG.write(\"Acc: \")\n",
    "    JOB_LOG.write(str(history.history[\"acc\"]))\n",
    "    JOB_LOG.write(\"\\n\")\n",
    "    JOB_LOG.write(\"Val Acc: \")\n",
    "    JOB_LOG.write(str(history.history[\"val_acc\"]))\n",
    "    JOB_LOG.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "radzOgn6MM2T"
   },
   "outputs": [],
   "source": [
    "load_weights=False\n",
    "if load_weights:\n",
    "    siamese_model2.load_weights(os.path.join(*[CURRENT_JOB_PATH,\"save_at_2.ckpt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGlt2Cj1Ma9X"
   },
   "outputs": [],
   "source": [
    "if load_weights:\n",
    "    d_hold = get_d(siamese_model2,hold_dataset,hold_dataset_size,prep)\n",
    "    d_hold_save_path = os.path.join(CURRENT_JOB_PATH,\"d_hold.npy\")\n",
    "    with open(d_hold_save_path,'wb') as f:\n",
    "        np.save(f,d_hold)\n",
    "\n",
    "    y_hold_pred = np.sign(d_hold[:,1]-d_hold[:,0])\n",
    "    y_hold_pred = (y_hold_pred+1)/2\n",
    "    y_hold_pred = y_hold_pred.astype(int)\n",
    "\n",
    "    now = datetime.now()\n",
    "    this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    print(\"This Time =\", this_time)\n",
    "\n",
    "    print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "    #job_05_22_19_12_28\n",
    "\n",
    "    acc = accuracy_score(y_hold_groundtruth,y_hold_pred)\n",
    "    print(\"acc %f\"%acc)\n",
    "\n",
    "    rec = recall_score(y_hold_groundtruth,y_hold_pred)\n",
    "    print(\"rec %f\"%rec)\n",
    "\n",
    "    prec = precision_score(y_hold_groundtruth,y_hold_pred)\n",
    "    print(\"prec %f\"%prec)\n",
    "\n",
    "    now = datetime.now()\n",
    "    this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    print(\"This Time =\", this_time)\n",
    "\n",
    "    print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "    #y.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB4JR2vr2yTf"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JNvxLeH1SZs"
   },
   "outputs": [],
   "source": [
    "use_early_weights = False\n",
    "if use_early_weights:\n",
    "    iterator = iter(hold_dataset)\n",
    "    sample = iterator.get_next()\n",
    "\n",
    "    siamese_model.predict(sample)\n",
    "\n",
    "    epoch_choice = 5\n",
    "    h5_path = os.path.join(*[CURRENT_JOB_PATH,\"save_at_%d.h5\"%(epoch_choice)])\n",
    "    siamese_model.load_weights(h5_path)\n",
    "    x.append(epoch_choice)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z30Jeq1-4RLM"
   },
   "outputs": [],
   "source": [
    "d_hold = get_d(siamese_model,hold_dataset,hold_dataset_size,prep)\n",
    "d_hold_save_path = os.path.join(CURRENT_JOB_PATH,\"d_hold.npy\")\n",
    "with open(d_hold_save_path,'wb') as f:\n",
    "    np.save(f,d_hold)\n",
    "\n",
    "y_hold_pred = np.sign(d_hold[:,1]-d_hold[:,0])\n",
    "y_hold_pred = (y_hold_pred+1)/2\n",
    "y_hold_pred = y_hold_pred.astype(int)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN74BOoB4mQz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "#job_05_22_19_12_28\n",
    "\n",
    "acc = accuracy_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"acc %f\"%acc)\n",
    "\n",
    "rec = recall_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"rec %f\"%rec)\n",
    "\n",
    "prec = precision_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"prec %f\"%prec)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "#y.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BngUlRIJPNM"
   },
   "outputs": [],
   "source": [
    "d_test = get_d(siamese_model,test_dataset,test_dataset_size,prep)\n",
    "d_test_save_path = os.path.join(CURRENT_JOB_PATH,\"d_test.npy\")\n",
    "with open(d_test_save_path,'wb') as f:\n",
    "    np.save(f,d_test)\n",
    "y_test = np.sign(d_test[:,1]-d_test[:,0])\n",
    "y_test = (y_test+1)/2\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "result_str = \"\"\n",
    "for i in range(len(y_test)):\n",
    "    result_str+=str(int(y_test[i]))\n",
    "    result_str+=\"\\n\"\n",
    "\n",
    "result_file = open(os.path.join(CURRENT_JOB_PATH,\"y_test.txt\"),\"w\")\n",
    "result_file.write(result_str)\n",
    "result_file.close()\n",
    "\n",
    "JOB_LOG.close()\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "!ls\n",
    "!ls jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6C_wDnwTcKV"
   },
   "outputs": [],
   "source": [
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "!git config --global user.email \"choong.hanyao@gmail.com\"\n",
    "!git config --global user.name \"hanyao8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfSvab4aftzk"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVKB4TuVQnuY"
   },
   "outputs": [],
   "source": [
    "cmd_string = \"git add %s\"%(os.path.join(CURRENT_JOB_PATH,\"d_test.npy\"))\n",
    "os.system(cmd_string)\n",
    "cmd_string = \"git add %s\"%(os.path.join(CURRENT_JOB_PATH,\"log.txt\"))\n",
    "os.system(cmd_string)\n",
    "cmd_string = \"git add %s\"%(os.path.join(CURRENT_JOB_PATH,\"y_test.txt\"))\n",
    "os.system(cmd_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwPRKyX_QoG1"
   },
   "outputs": [],
   "source": [
    "commit_message = \"add colab job %s\"%(CURRENT_JOB_PATH)\n",
    "cmd_string = \"git commit -m \\\"%s\\\"\"%(commit_message)\n",
    "os.system(cmd_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4gTZuntTFw0"
   },
   "outputs": [],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKYQ-GebF7Al"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPAyob2sF7ca"
   },
   "outputs": [],
   "source": [
    "raise (Exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt49EXG8OUDC"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/iml-hs21/task4/jobs/job_05_28_18_45_33_ckpt9.zip   /content/iml-hs21/task4/jobs/job_05_28_18_45_33/save_at_9.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22cuCRq-FGqR"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/iml-hs21/task4/jobs/job_05_28_18_45_33_ckpt5.zip   /content/iml-hs21/task4/jobs/jobb_05_28_18_45_33/save_at_5.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e3ihYyfmQWT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def image_pred(image_code,model):\n",
    "    image_path = 'data/food/'+image_code+'.jpg'\n",
    "    #image_path = os.path.join(project_dir,image_path)\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = mobilenet.preprocess_input(image)\n",
    "    yhat = model.predict(image)\n",
    "    #print(\"yhat:\")\n",
    "    #print(np.shape(yhat))\n",
    "    #yhat = yhat[0]\n",
    "    return(yhat)\n",
    "\n",
    "\n",
    "def triplet_pred(triplet,model):\n",
    "    triplet_feats_shape = tuple([3]+list((model.layers[-1].output_shape)[1:]))\n",
    "    #print(triplet_feats_shape)\n",
    "    triplet_feats = np.zeros(triplet_feats_shape)\n",
    "    for i in range(len(triplet)):\n",
    "        triplet_feats[i] = image_pred(triplet[i],model)\n",
    "\n",
    "    dot_01 = np.dot(triplet_feats[0].flatten(),triplet_feats[1].flatten())\n",
    "    dot_02 = np.dot(triplet_feats[0].flatten(),triplet_feats[2].flatten())\n",
    "    return (np.array([dot_01,dot_02]))\n",
    "\n",
    "def triplets_set_inference(triplets_df,model):\n",
    "    #n = 200\n",
    "    n = triplets_df.shape[0]\n",
    "    d = np.zeros((n,2))\n",
    "    #y = np.zeros(n)\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    JOB_LOG.write(\"inference start\")\n",
    "    JOB_LOG.write(str(current_time)+\"\\n\")\n",
    "\n",
    "    for i in range(n):\n",
    "        triplet = triplets_df.iloc[i].values\n",
    "        d[i,:] = triplet_pred(triplet,model)\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "            print(\"\\n\")  \n",
    "        \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    JOB_LOG.write(\"inference end\")\n",
    "    JOB_LOG.write(str(current_time)+\"\\n\")\n",
    "    return(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeOGNw3VrnY1"
   },
   "outputs": [],
   "source": [
    "model1 = MobileNetV2(weights=\"imagenet\")\n",
    "model2 = Model(inputs=model1.inputs, outputs=model1.layers[-3].output)\n",
    "\n",
    "d_hold_pred_2 = triplets_set_inference(hold_triplets,model1)\n",
    "#y_hold_pred = y_hold_pred.astype(int)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYe1bO90jGas"
   },
   "outputs": [],
   "source": [
    "from keras.applications import Xception\n",
    "from keras.applications import xception\n",
    "\n",
    "MOBILENET_INPUT_SHAPE = (224,224,3)\n",
    "XCEPTION_INPUT_SHAPE = (299,299,3)\n",
    "\n",
    "feature_cnn = Xception(weights=\"imagenet\",\n",
    "                              input_shape=XCEPTION_INPUT_SHAPE,\n",
    "                              include_top=True)\n",
    "\n",
    "feature_cnn.summary()\n",
    "\n",
    "feature_cnn.output\n",
    "\n",
    "# x has a shape of (2, 3) (two rows and three columns):\n",
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "x.numpy()\n",
    "\n",
    "\n",
    "# sum all the elements\n",
    "# 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
    "tf.reduce_sum(x).numpy()\n",
    "\n",
    "# reduce along the first dimension\n",
    "# the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
    "tf.reduce_sum(x, 0).numpy()\n",
    "\n",
    "# reduce along the second dimension\n",
    "# the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
    "tf.reduce_sum(x, 1).numpy()\n",
    "\n",
    "# keep the original dimensions\n",
    "tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
    "\n",
    "\n",
    "# reduce along both dimensions\n",
    "# the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
    "# or, equivalently, reduce along rows, then reduce the resultant array\n",
    "# [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
    "# 2 + 2 + 2 = 6\n",
    "tf.reduce_sum(x, [0, 1]).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R2CGun5_GPl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu5y9zFnCeam"
   },
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV2\n",
    "from keras.applications import mobilenet\n",
    "\n",
    "MOBILENET_INPUT_SHAPE = (224,224,3)\n",
    "XCEPTION_INPUT_SHAPE = (299,299,3)\n",
    "\n",
    "feature_cnn = MobileNetV2(weights=\"imagenet\",\n",
    "                              input_shape=MOBILENET_INPUT_SHAPE,\n",
    "                              include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qhh7Qo9u5SqT"
   },
   "outputs": [],
   "source": [
    "from keras.applications import Xception\n",
    "from keras.applications import xception\n",
    "\n",
    "MOBILENET_INPUT_SHAPE = (224,224,3)\n",
    "XCEPTION_INPUT_SHAPE = (299,299,3)\n",
    "\n",
    "feature_cnn = Xception(weights=\"imagenet\",\n",
    "                              input_shape=XCEPTION_INPUT_SHAPE,\n",
    "                              include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrsPp1kB5S52"
   },
   "outputs": [],
   "source": [
    "feature_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yEOQX6s5TD3"
   },
   "outputs": [],
   "source": [
    "a = \"abcdefg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF2HKj4m5TM-"
   },
   "outputs": [],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUJEbPL25SXq"
   },
   "outputs": [],
   "source": [
    "a = ((1,2,3),(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7yDu12369MD"
   },
   "outputs": [],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkf8Kja1BPOG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The nested structure of the `datasets` argument determines the\n",
    "# structure of elements in the resulting dataset.\n",
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
    "ds = tf.data.Dataset.zip((a, b))\n",
    "list(ds.as_numpy_iterator())\n",
    "\n",
    "ds = tf.data.Dataset.zip((b, a))\n",
    "list(ds.as_numpy_iterator())\n",
    "\n",
    "\n",
    "# The `datasets` argument may contain an arbitrary number of datasets.\n",
    "c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
    "                                           #       [9, 10],\n",
    "                                           #       [11, 12] ]\n",
    "ds = tf.data.Dataset.zip((a, b, c))\n",
    "for element in ds.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The number of elements in the resulting dataset is the same as\n",
    "# the size of the smallest dataset in `datasets`.\n",
    "d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
    "ds = tf.data.Dataset.zip((a, d))\n",
    "list(ds.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT88dfG6BSBg"
   },
   "outputs": [],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
    "ds = tf.data.Dataset.zip((a, b))\n",
    "c = tf.data.Dataset.zip((ds, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vnvew-ZaBtv8"
   },
   "outputs": [],
   "source": [
    "d = list(c.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnlHq1RdCCrA"
   },
   "outputs": [],
   "source": [
    "d[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMaxavE1G6t4"
   },
   "outputs": [],
   "source": [
    "f = tf.data.Dataset.from_tensor_slices(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDLujvnPG_OT"
   },
   "outputs": [],
   "source": [
    "list(f.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JcSvAdkkZk0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "iml4_colab_2905.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
