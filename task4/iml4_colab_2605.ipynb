{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJjuEWBZIfzs",
    "outputId": "6585bf20-969c-46e4-a432-8e56d30deac9"
   },
   "outputs": [],
   "source": [
    "# ONLY RUN ONCE TO SETUP THE ENVIRONMENT\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "import os\n",
    "from random import sample\n",
    "from glob import glob\n",
    "\n",
    "# cleanup previouiml runs\n",
    "!rm -r iml-hs21\n",
    "\n",
    "# load data from github repo\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password) # your password is converted into url format\n",
    "repo_name = 'iml-hs21'\n",
    "cmd_string = 'git clone https://{0}:{1}@github.com/{2}/{3}.git'.format(user, password, 'hanyao8', repo_name)\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOhyXBIZOovQ",
    "outputId": "12aa5271-b309-48b8-9da4-d1f869cf475c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/iml-hs21/task4'\n",
      "/Users/hchoong/Desktop/github/iml-hs21/task4\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m             iml4_colab_2605.ipynb   task4_2005.py\n",
      "custom_objects.py       \u001b[34mjobs\u001b[m\u001b[m                    task4_models.py\n",
      "\u001b[34mdata\u001b[m\u001b[m                    preprocessor.py         task4_utils.py\n",
      "iml4_colab_2005.ipynb   snippets.py             task4_utils.py.bk\n",
      "iml4_colab_2305_2.ipynb task4_2005.ipynb\n",
      "2.0.0\n",
      "2.2.4-tf\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "%cd /content/iml-hs21/task4\n",
    "!ls\n",
    "\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(keras.__version__==\"2.5.0\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "41uKi40VXoRX",
    "outputId": "8f1a9552-3e98-4c1f-e339-c91ced15cd05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from task4_utils import *\n",
    "import task4_models\n",
    "import custom_objects\n",
    "\n",
    "import preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g6WA2dmdO8si"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: untrusted X11 forwarding setup failed: xauth key data not generated\n",
      "Already up to date.\n",
      "/Users/hchoong/Desktop/github/iml-hs21/task4\n",
      "Job Start Time = 05_26_08_57_11\n",
      "\n",
      "Current Job Path: /Users/hchoong/Desktop/github/iml-hs21/task4/jobs/job_05_26_08_57_11 \n",
      "\n",
      "Tensorflow ver. 2.0.0\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "\n",
    "#setup logging\n",
    "#setup new job directory\n",
    "CWD = os.getcwd()\n",
    "print(CWD)\n",
    "\n",
    "now = datetime.now()\n",
    "job_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"Job Start Time =\", job_time)\n",
    "\n",
    "if not(os.path.exists(\"jobs\")):\n",
    "    os.mkdir(\"jobs\")\n",
    "CURRENT_JOB_PATH = os.path.join(*[CWD,\"jobs\",\"job_\"+job_time])\n",
    "os.mkdir(CURRENT_JOB_PATH)\n",
    "JOB_LOG = open(os.path.join(CURRENT_JOB_PATH,\"log.txt\"),\"a\")\n",
    "JOB_LOG.write(\"init\\n\")\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "os. environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "\n",
    "# automatically chooses parameters\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tpus = tf.config.experimental.list_physical_devices('TPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "#elif tpus:\n",
    "#    print(\"TPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RTsTBAfV6y2m"
   },
   "outputs": [],
   "source": [
    "JOB_LOG.write(\"Current job path: \")\n",
    "JOB_LOG.write(CURRENT_JOB_PATH)\n",
    "JOB_LOG.write(\"\\n\")\n",
    "\n",
    "#POS_TRAIN_DATASET_SETTING = \"default\"\n",
    "#POS_TRAIN_DATASET_SETTING = \"no_hold\"\n",
    "POS_TRAIN_DATASET_SETTING = 1000\n",
    "\n",
    "HOLD_FRAC = 0.01\n",
    "\n",
    "#MODEL_CHOICE = \"siamese1\"\n",
    "#MODEL_CHOICE = \"siamese2\"\n",
    "#MODEL_CHOICE = \"siamese_xception_2\"\n",
    "#MODEL_CHOICE = \"siamese_xception_3\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dot_2\"\n",
    "MODEL_CHOICE = \"siamese_mobilenet_dot_3\"\n",
    "#MODEL_CHOICE = \"siamese_mobilenet_dist\"\n",
    "#MODEL_CHOICE = \"siamese_xception_dot\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_ADAM_STEP = 1.0e-6\n",
    "TRAIN_EPOCHS = 1\n",
    "\n",
    "LAMBDA_1 = 1.0\n",
    "LAMBDA_2 = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PNhDfwEsxs0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Time = 05_26_08_57_11\n",
      "117838\n",
      "1192\n",
      "59544\n",
      "train, val, hold, test .__len__(): (indicates # batches in dataset)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PrefetchDataset' object has no attribute '__len__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0a92fc04a0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train, val, hold, test .__len__(): (indicates # batches in dataset)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrefetchDataset' object has no attribute '__len__'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if \"xception\" in MODEL_CHOICE:\n",
    "    TARGET_SHAPE = (299,299)\n",
    "else:\n",
    "    TARGET_SHAPE = (224,224)\n",
    "\n",
    "MT_WEIGHTS = [LAMBDA_1,LAMBDA_2]\n",
    "\n",
    "experiment_params = {\n",
    "    \"HOLD_FRAC\": HOLD_FRAC,\n",
    "    \"POS_TRAIN_DATASET_SETTING\": POS_TRAIN_DATASET_SETTING,\n",
    "    \"MODEL_CHOICE\": MODEL_CHOICE,\n",
    "    \"TARGET_SHAPE\":TARGET_SHAPE,\n",
    "    \"MULTITASK_WEIGHTS\":MT_WEIGHTS,\n",
    "    \"TRAIN_ADAM_STEP\": TRAIN_ADAM_STEP,\n",
    "    \"TRAIN_EPOCHS\": TRAIN_EPOCHS,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE\n",
    "}\n",
    "\n",
    "\n",
    "JOB_LOG.write(str(experiment_params))\n",
    "JOB_LOG.write(\"\\n\")\n",
    "\n",
    "prep = preprocessor.Preprocessor(\n",
    "    target_shape=TARGET_SHAPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    multitask=True)\n",
    "\n",
    "### Get data\n",
    "sample = pd.read_table(\"data/sample.txt\",header=None)\n",
    "pos_train_hold_triplets = pd.read_table(\"data/train_triplets.txt\",delimiter=\" \",header=None,dtype=str)\n",
    "pos_train_triplets, pos_hold_triplets = train_test_split(pos_train_hold_triplets,test_size=HOLD_FRAC)\n",
    "test_triplets = pd.read_table(\"data/test_triplets.txt\",delimiter=\" \",header=None,dtype=str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if POS_TRAIN_DATASET_SETTING == \"default\":\n",
    "    pos_train_dataset_size = pos_train_triplets.shape[0]\n",
    "\n",
    "elif POS_TRAIN_DATASET_SETTING == \"no_hold\":\n",
    "    hold_codes = pd.unique(pos_hold_triplets[0])\n",
    "    hold_codes = np.concatenate((hold_codes,pd.unique(pos_hold_triplets[1])))\n",
    "    hold_codes = np.concatenate((hold_codes,pd.unique(pos_hold_triplets[2])))\n",
    "    hold_codes = pd.unique(hold_codes)\n",
    "    print(hold_codes.shape[0])\n",
    "\n",
    "    df = pos_train_triplets.copy()\n",
    "\n",
    "    print(df.shape)\n",
    "    for i in range(hold_codes.shape[0]):\n",
    "        for j in range(3):\n",
    "            df = df[df[j] != hold_codes[i]]\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    print(df.shape)\n",
    "    pos_train_triplets = df\n",
    "    pos_train_dataset_size = pos_train_triplets.shape[0]\n",
    "    print(pos_train_dataset_size)\n",
    "\n",
    "elif type(POS_TRAIN_DATASET_SETTING)==int:\n",
    "    pos_train_dataset_size = POS_TRAIN_DATASET_SETTING\n",
    "\n",
    "else:\n",
    "    raise (Exception)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "train_triplets = triplets_from_pos(pos_train_triplets)\n",
    "train_dataset_size = train_triplets.shape[0]\n",
    "y_train_groundtruth = gt_from_df(train_triplets)\n",
    "\n",
    "if POS_TRAIN_DATASET_SETTING == \"no_hold\":\n",
    "    train_dataset = train_val_dataset_from_df(train_triplets,y_train_groundtruth,\n",
    "                            train_dataset_size,val_frac=0,prep=prep)\n",
    "    val_dataset = hold_dataset_from_df(pos_hold_triplets,prep=prep)\n",
    "else:\n",
    "    train_dataset, val_dataset = train_val_dataset_from_df(train_triplets,\n",
    "                            y_train_groundtruth,\n",
    "                            train_dataset_size,val_frac=0.2,prep=prep)\n",
    "\n",
    "hold_triplets = triplets_from_pos(pos_hold_triplets)\n",
    "hold_dataset_size = hold_triplets.shape[0]\n",
    "hold_dataset = hold_dataset_from_df(hold_triplets,prep=prep)\n",
    "y_hold_groundtruth = gt_from_df(hold_triplets)\n",
    "\n",
    "test_dataset = test_dataset_from_df(test_triplets,prep=prep)\n",
    "test_dataset_size = test_triplets.shape[0]\n",
    "\n",
    "print(\"train, val, hold, test .__len__(): (indicates # batches in dataset)\")\n",
    "print(train_dataset.__len__())\n",
    "print(val_dataset.__len__())\n",
    "print(hold_dataset.__len__())\n",
    "print(test_dataset.__len__())\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9m80jR69mBl"
   },
   "outputs": [],
   "source": [
    "TARGET_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXvM3duXqED_"
   },
   "outputs": [],
   "source": [
    "#siamese_model = SiameseModel(siamese_network)\n",
    "if MODEL_CHOICE==\"siamese1\":\n",
    "    siamese_network = create_siamese()\n",
    "elif MODEL_CHOICE==\"siamese2\":\n",
    "    siamese_network = task4_models.create_siamese2()\n",
    "elif MODEL_CHOICE==\"siamese_xception\":\n",
    "    siamese_network = task4_models.create_siamese_xception()\n",
    "elif MODEL_CHOICE==\"siamese_xception_2\":\n",
    "    siamese_network = create_siamese_xception_2()\n",
    "elif MODEL_CHOICE==\"siamese_xception_3\":\n",
    "    siamese_network = create_siamese_xception_3()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot\":\n",
    "    siamese_network = create_siamese_mobilenet_dot()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot_2\":\n",
    "    siamese_network = task4_models.create_siamese_mobilenet_dot_2()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dot_3\":\n",
    "    siamese_network = task4_models.create_siamese_mobilenet_dot_3()\n",
    "elif MODEL_CHOICE==\"siamese_mobilenet_dist\":\n",
    "    siamese_network = create_siamese_mobilenet_dist()\n",
    "elif MODEL_CHOICE==\"siamese_xception_dot\":\n",
    "    siamese_network = task4_models.create_siamese_xception_dot()\n",
    "\n",
    "#siamese_model = custom_objects.SiameseModel2(siamese_network,margin=1.0)\n",
    "siamese_model = custom_objects.SiameseModel3(siamese_network,margin=1.0,mt_weights=MT_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zQb4VBLUPRa"
   },
   "outputs": [],
   "source": [
    "#Experiment\n",
    "if TRAIN_EPOCHS > 0:\n",
    "    now = datetime.now()\n",
    "    this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    print(\"This Time =\", this_time)\n",
    "    print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(CURRENT_JOB_PATH,\"save_at_{epoch}.h5\")),]\n",
    "\n",
    "    siamese_model.compile(optimizer=optimizers.Adam(TRAIN_ADAM_STEP))\n",
    "\n",
    "    history = siamese_model.fit(train_dataset, epochs=TRAIN_EPOCHS,\n",
    "                                callbacks=callbacks, validation_data=val_dataset)\n",
    "\n",
    "    now = datetime.now()\n",
    "    this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    print(\"This Time =\", this_time)\n",
    "    print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "    print(history)\n",
    "    print(history.history.keys())\n",
    "\n",
    "    JOB_LOG.write(str(history.params))\n",
    "    JOB_LOG.write(\"\\n\")\n",
    "    JOB_LOG.write(\"Loss: \")\n",
    "    JOB_LOG.write(str(history.history[\"loss\"]))\n",
    "    JOB_LOG.write(\"\\n\")\n",
    "    JOB_LOG.write(\"Val Loss: \")\n",
    "    JOB_LOG.write(str(history.history[\"val_loss\"]))\n",
    "    JOB_LOG.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB4JR2vr2yTf"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JNvxLeH1SZs"
   },
   "outputs": [],
   "source": [
    "use_early_weights = False\n",
    "if use_early_weights:\n",
    "    iterator = iter(hold_dataset)\n",
    "    sample = iterator.get_next()\n",
    "\n",
    "    siamese_model.predict(sample)\n",
    "\n",
    "    epoch_choice = 5\n",
    "    h5_path = os.path.join(*[CURRENT_JOB_PATH,\"save_at_%d.h5\"%(epoch_choice)])\n",
    "    siamese_model.load_weights(h5_path)\n",
    "    x.append(epoch_choice)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z30Jeq1-4RLM"
   },
   "outputs": [],
   "source": [
    "d_hold = get_d(siamese_model,hold_dataset,hold_dataset_size,prep)\n",
    "d_hold_save_path = os.path.join(CURRENT_JOB_PATH,\"d_hold.npy\")\n",
    "with open(d_hold_save_path,'wb') as f:\n",
    "    np.save(f,d_hold)\n",
    "\n",
    "y_hold_pred = np.sign(d_hold[:,1]-d_hold[:,0])\n",
    "y_hold_pred = (y_hold_pred+1)/2\n",
    "y_hold_pred = y_hold_pred.astype(int)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN74BOoB4mQz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "#job_05_22_19_12_28\n",
    "\n",
    "acc = accuracy_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"acc %f\"%acc)\n",
    "\n",
    "rec = recall_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"rec %f\"%rec)\n",
    "\n",
    "prec = precision_score(y_hold_groundtruth,y_hold_pred)\n",
    "print(\"prec %f\"%prec)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "#y.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC2aYS7CtTBZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6TZSOSjtSWs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BngUlRIJPNM"
   },
   "outputs": [],
   "source": [
    "d_test = get_d(siamese_model,test_dataset,test_dataset_size,prep)\n",
    "d_test_save_path = os.path.join(CURRENT_JOB_PATH,\"d_test.npy\")\n",
    "with open(d_test_save_path,'wb') as f:\n",
    "    np.save(f,d_test)\n",
    "y_test = np.sign(d_test[:,1]-d_test[:,0])\n",
    "y_test = (y_test+1)/2\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "result_str = \"\"\n",
    "for i in range(len(y_test)):\n",
    "    result_str+=str(int(y_test[i]))\n",
    "    result_str+=\"\\n\"\n",
    "\n",
    "result_file = open(os.path.join(CURRENT_JOB_PATH,\"y_test.txt\"),\"w\")\n",
    "result_file.write(result_str)\n",
    "result_file.close()\n",
    "\n",
    "JOB_LOG.close()\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "\n",
    "!ls\n",
    "!ls jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6C_wDnwTcKV"
   },
   "outputs": [],
   "source": [
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n",
    "!git config --global user.email \"choong.hanyao@gmail.com\"\n",
    "!git config --global user.name \"hanyao8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfSvab4aftzk"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVKB4TuVQnuY"
   },
   "outputs": [],
   "source": [
    "cmd_string = \"git add %s\"%(os.path.join(CURRENT_JOB_PATH,\"d_test.npy\"))\n",
    "os.system(cmd_string)\n",
    "cmd_string = \"git add %s\"%(os.path.join(CURRENT_JOB_PATH,\"log.txt\"))\n",
    "os.system(cmd_string)\n",
    "cmd_string = \"git add %s\"%(os.path.join(CURRENT_JOB_PATH,\"y_test.txt\"))\n",
    "os.system(cmd_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwPRKyX_QoG1"
   },
   "outputs": [],
   "source": [
    "commit_message = \"add colab job %s\"%(CURRENT_JOB_PATH)\n",
    "cmd_string = \"git commit -m \\\"%s\\\"\"%(commit_message)\n",
    "os.system(cmd_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4gTZuntTFw0"
   },
   "outputs": [],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKYQ-GebF7Al"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPAyob2sF7ca"
   },
   "outputs": [],
   "source": [
    "raise (Exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e3ihYyfmQWT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def image_pred(image_code,model):\n",
    "    image_path = 'data/food/'+image_code+'.jpg'\n",
    "    #image_path = os.path.join(project_dir,image_path)\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = mobilenet.preprocess_input(image)\n",
    "    yhat = model.predict(image)\n",
    "    #print(\"yhat:\")\n",
    "    #print(np.shape(yhat))\n",
    "    #yhat = yhat[0]\n",
    "    return(yhat)\n",
    "\n",
    "\n",
    "def triplet_pred(triplet,model):\n",
    "    triplet_feats_shape = tuple([3]+list((model.layers[-1].output_shape)[1:]))\n",
    "    #print(triplet_feats_shape)\n",
    "    triplet_feats = np.zeros(triplet_feats_shape)\n",
    "    for i in range(len(triplet)):\n",
    "        triplet_feats[i] = image_pred(triplet[i],model)\n",
    "\n",
    "    dot_01 = np.dot(triplet_feats[0].flatten(),triplet_feats[1].flatten())\n",
    "    dot_02 = np.dot(triplet_feats[0].flatten(),triplet_feats[2].flatten())\n",
    "    return (np.array([dot_01,dot_02]))\n",
    "\n",
    "def triplets_set_inference(triplets_df,model):\n",
    "    #n = 200\n",
    "    n = triplets_df.shape[0]\n",
    "    d = np.zeros((n,2))\n",
    "    #y = np.zeros(n)\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    JOB_LOG.write(\"inference start\")\n",
    "    JOB_LOG.write(str(current_time)+\"\\n\")\n",
    "\n",
    "    for i in range(n):\n",
    "        triplet = triplets_df.iloc[i].values\n",
    "        d[i,:] = triplet_pred(triplet,model)\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "            print(\"\\n\")  \n",
    "        \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    JOB_LOG.write(\"inference end\")\n",
    "    JOB_LOG.write(str(current_time)+\"\\n\")\n",
    "    return(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeOGNw3VrnY1"
   },
   "outputs": [],
   "source": [
    "model1 = MobileNetV2(weights=\"imagenet\")\n",
    "model2 = Model(inputs=model1.inputs, outputs=model1.layers[-3].output)\n",
    "\n",
    "d_hold_pred_2 = triplets_set_inference(hold_triplets,model1)\n",
    "#y_hold_pred = y_hold_pred.astype(int)\n",
    "\n",
    "now = datetime.now()\n",
    "this_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "print(\"This Time =\", this_time)\n",
    "\n",
    "print(\"\\nCurrent Job Path: %s \\n\"%CURRENT_JOB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYe1bO90jGas"
   },
   "outputs": [],
   "source": [
    "from keras.applications import Xception\n",
    "from keras.applications import xception\n",
    "\n",
    "MOBILENET_INPUT_SHAPE = (224,224,3)\n",
    "XCEPTION_INPUT_SHAPE = (299,299,3)\n",
    "\n",
    "feature_cnn = Xception(weights=\"imagenet\",\n",
    "                              input_shape=XCEPTION_INPUT_SHAPE,\n",
    "                              include_top=True)\n",
    "\n",
    "feature_cnn.summary()\n",
    "\n",
    "feature_cnn.output\n",
    "\n",
    "# x has a shape of (2, 3) (two rows and three columns):\n",
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "x.numpy()\n",
    "\n",
    "\n",
    "# sum all the elements\n",
    "# 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
    "tf.reduce_sum(x).numpy()\n",
    "\n",
    "# reduce along the first dimension\n",
    "# the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
    "tf.reduce_sum(x, 0).numpy()\n",
    "\n",
    "# reduce along the second dimension\n",
    "# the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
    "tf.reduce_sum(x, 1).numpy()\n",
    "\n",
    "# keep the original dimensions\n",
    "tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
    "\n",
    "\n",
    "# reduce along both dimensions\n",
    "# the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
    "# or, equivalently, reduce along rows, then reduce the resultant array\n",
    "# [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
    "# 2 + 2 + 2 = 6\n",
    "tf.reduce_sum(x, [0, 1]).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R2CGun5_GPl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu5y9zFnCeam"
   },
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV2\n",
    "from keras.applications import mobilenet\n",
    "\n",
    "MOBILENET_INPUT_SHAPE = (224,224,3)\n",
    "XCEPTION_INPUT_SHAPE = (299,299,3)\n",
    "\n",
    "feature_cnn = MobileNetV2(weights=\"imagenet\",\n",
    "                              input_shape=MOBILENET_INPUT_SHAPE,\n",
    "                              include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qhh7Qo9u5SqT"
   },
   "outputs": [],
   "source": [
    "from keras.applications import Xception\n",
    "from keras.applications import xception\n",
    "\n",
    "MOBILENET_INPUT_SHAPE = (224,224,3)\n",
    "XCEPTION_INPUT_SHAPE = (299,299,3)\n",
    "\n",
    "feature_cnn = Xception(weights=\"imagenet\",\n",
    "                              input_shape=XCEPTION_INPUT_SHAPE,\n",
    "                              include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrsPp1kB5S52"
   },
   "outputs": [],
   "source": [
    "feature_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yEOQX6s5TD3"
   },
   "outputs": [],
   "source": [
    "a = \"abcdefg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF2HKj4m5TM-"
   },
   "outputs": [],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUJEbPL25SXq"
   },
   "outputs": [],
   "source": [
    "a = ((1,2,3),(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7yDu12369MD"
   },
   "outputs": [],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkf8Kja1BPOG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The nested structure of the `datasets` argument determines the\n",
    "# structure of elements in the resulting dataset.\n",
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
    "ds = tf.data.Dataset.zip((a, b))\n",
    "list(ds.as_numpy_iterator())\n",
    "\n",
    "ds = tf.data.Dataset.zip((b, a))\n",
    "list(ds.as_numpy_iterator())\n",
    "\n",
    "\n",
    "# The `datasets` argument may contain an arbitrary number of datasets.\n",
    "c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
    "                                           #       [9, 10],\n",
    "                                           #       [11, 12] ]\n",
    "ds = tf.data.Dataset.zip((a, b, c))\n",
    "for element in ds.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The number of elements in the resulting dataset is the same as\n",
    "# the size of the smallest dataset in `datasets`.\n",
    "d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
    "ds = tf.data.Dataset.zip((a, d))\n",
    "list(ds.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT88dfG6BSBg"
   },
   "outputs": [],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
    "ds = tf.data.Dataset.zip((a, b))\n",
    "c = tf.data.Dataset.zip((ds, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vnvew-ZaBtv8"
   },
   "outputs": [],
   "source": [
    "d = list(c.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnlHq1RdCCrA"
   },
   "outputs": [],
   "source": [
    "d[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMaxavE1G6t4"
   },
   "outputs": [],
   "source": [
    "f = tf.data.Dataset.from_tensor_slices(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDLujvnPG_OT"
   },
   "outputs": [],
   "source": [
    "list(f.as_numpy_iterator())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "iml4_colab_2605.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
