{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score, recall_score, \\\n",
    "    accuracy_score, precision_score, r2_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(\"data/sample.csv\")\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feats_dict(df_train):\n",
    "    fv = df_train['Sequence'].values\n",
    "    \n",
    "\n",
    "    fv_1plex_list = []\n",
    "    for i in range(len(fv)):\n",
    "        for j in range(0,4):\n",
    "            fv_1plex_list.append(fv[i][j:j+1])\n",
    "    feats1 = list(set(fv_1plex_list))\n",
    "    #print(i)\n",
    "    #print(j)\n",
    "    #print(len(feats1))\n",
    "    \n",
    "    feats_dict = {}\n",
    "    \n",
    "    feats_dict['1plex_pos0']=feats1\n",
    "    feats_dict['1plex_pos1']=feats1\n",
    "    feats_dict['1plex_pos2']=feats1\n",
    "    feats_dict['1plex_pos3']=feats1\n",
    "        \n",
    "    return(feats_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_occ(df,feats_dict):\n",
    "    \n",
    "    fv = df['Sequence'].values\n",
    "    \n",
    "    occ = {}\n",
    "    \n",
    "    fv_1plex_list = []\n",
    "    for i in range(len(fv)):\n",
    "        for j in range(0,4):\n",
    "            fv_1plex_list.append(fv[i][j:j+1])\n",
    "    #1-plex\n",
    "    for i in range(len(feats_dict['1plex'])):\n",
    "        feat = feats_dict['1plex'][i]\n",
    "        count = fv_1plex_list.count(feat)\n",
    "        occ[feat] = count\n",
    "        print(\"%s: %d\"%(feat,count))\n",
    "    \n",
    "    return(occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_X(df,feats_dict):\n",
    "    n = df.shape[0]\n",
    "    d = 0\n",
    "    for feat_type in list(feats_dict):\n",
    "        d+=len(feats_dict[feat_type])\n",
    "    X = np.zeros((n,d))\n",
    "    print(np.shape(X))\n",
    "    \n",
    "    seqs = df[\"Sequence\"].values\n",
    "    i=0\n",
    "    for feat_type in list(feats_dict):\n",
    "        pos = int(feat_type[-1])\n",
    "        plex = int(feat_type[0])\n",
    "        for j in range(len(feats_dict[feat_type])):\n",
    "\n",
    "            for k in range(len(seqs)):\n",
    "                if feats_dict[feat_type][j] == seqs[k][pos:pos+plex]:\n",
    "                    X[k][i] = 1\n",
    "\n",
    "            if i%10==0:\n",
    "                print(i)\n",
    "            i+=1\n",
    "\n",
    "    return(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111999\n",
      "3\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "feats_dict = gen_feats_dict(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112000, 80)\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "(48000, 80)\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "X_train_raw = gen_X(df_train,feats_dict)\n",
    "X_test_raw = gen_X(df_test,feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112000, 80)\n",
      "(112000,)\n",
      "(48000, 80)\n",
      "(112000, 80)\n",
      "(112000,)\n",
      "(48000, 80)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "holdout=False\n",
    "feat_sel1=False\n",
    "feat_sel2=False\n",
    "\n",
    "if holdout:\n",
    "    #make hold out\n",
    "    X_train,X_hold,y_train,y_hold = train_test_split(\n",
    "        X_train_raw,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "    print(np.shape(X_train))\n",
    "    print(np.shape(X_hold))\n",
    "    print(np.sum(y_train))\n",
    "    print(np.sum(y_hold))\n",
    "    print(\"\\n\")\n",
    "else:\n",
    "    X_train = X_train_raw.copy()\n",
    "    \n",
    "X_test = X_test_raw.copy()\n",
    "    \n",
    "y_train = df_train['Active'].values\n",
    "n = float(np.shape(y_train)[0])    \n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "if feat_sel1:\n",
    "    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_train = model.transform(X_train)\n",
    "    X_test = model.transform(X_test)\n",
    "    \n",
    "if feat_sel2:\n",
    "    #feature selection by conditional probability threshold\n",
    "    a = np.sum(X_train,axis=0)\n",
    "    b = np.sum(np.transpose(X_train)*y_train,axis=1)\n",
    "    print(np.shape(a))\n",
    "    print(np.shape(b))\n",
    "    c = b/a\n",
    "    d = np.argwhere(c>0.2).flatten()\n",
    "    \n",
    "    X_train = X_train[:,d]\n",
    "    X_test = X_test[:,d]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.065 GB of training data: 0.356 s\n",
      "Binning 0.007 GB of validation data: 0.010 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 1 tree, 75 leaves, max depth = 17, train loss: 0.10573, val loss: 0.11347, in 0.082s\n",
      "[2/200] 1 tree, 124 leaves, max depth = 23, train loss: 0.07860, val loss: 0.08241, in 0.170s\n",
      "[3/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.05738, val loss: 0.06261, in 0.178s\n",
      "[4/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.04672, val loss: 0.05207, in 0.132s\n",
      "[5/200] 1 tree, 127 leaves, max depth = 49, train loss: 0.03853, val loss: 0.04384, in 0.212s\n",
      "[6/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.03437, val loss: 0.03967, in 0.112s\n",
      "[7/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.03200, val loss: 0.03783, in 0.115s\n",
      "[8/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.02909, val loss: 0.03518, in 0.109s\n",
      "[9/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.02709, val loss: 0.03374, in 0.169s\n",
      "[10/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.02538, val loss: 0.03248, in 0.097s\n",
      "[11/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.02398, val loss: 0.03097, in 0.089s\n",
      "[12/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.02282, val loss: 0.03000, in 0.109s\n",
      "[13/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.02176, val loss: 0.02902, in 0.138s\n",
      "[14/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.02067, val loss: 0.02801, in 0.137s\n",
      "[15/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01978, val loss: 0.02729, in 0.096s\n",
      "[16/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.01879, val loss: 0.02660, in 0.103s\n",
      "[17/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01817, val loss: 0.02618, in 0.093s\n",
      "[18/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.01756, val loss: 0.02577, in 0.130s\n",
      "[19/200] 1 tree, 127 leaves, max depth = 45, train loss: 0.01693, val loss: 0.02534, in 0.093s\n",
      "[20/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.01632, val loss: 0.02513, in 0.100s\n",
      "[21/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01578, val loss: 0.02484, in 0.134s\n",
      "[22/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01524, val loss: 0.02405, in 0.264s\n",
      "[23/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01480, val loss: 0.02387, in 0.138s\n",
      "[24/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.01437, val loss: 0.02387, in 0.166s\n",
      "[25/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.01391, val loss: 0.02356, in 0.152s\n",
      "[26/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01357, val loss: 0.02352, in 0.148s\n",
      "[27/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.01316, val loss: 0.02315, in 0.142s\n",
      "[28/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01279, val loss: 0.02297, in 0.092s\n",
      "[29/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.01248, val loss: 0.02281, in 0.108s\n",
      "[30/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01214, val loss: 0.02283, in 0.094s\n",
      "[31/200] 1 tree, 127 leaves, max depth = 22, train loss: 0.01182, val loss: 0.02260, in 0.134s\n",
      "[32/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.01153, val loss: 0.02250, in 0.093s\n",
      "[33/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01128, val loss: 0.02242, in 0.090s\n",
      "[34/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01099, val loss: 0.02234, in 0.089s\n",
      "[35/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01071, val loss: 0.02221, in 0.122s\n",
      "[36/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01046, val loss: 0.02207, in 0.095s\n",
      "[37/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.01018, val loss: 0.02173, in 0.090s\n",
      "[38/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00995, val loss: 0.02169, in 0.093s\n",
      "[39/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00971, val loss: 0.02162, in 0.151s\n",
      "[40/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00952, val loss: 0.02169, in 0.089s\n",
      "[41/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00932, val loss: 0.02162, in 0.098s\n",
      "[42/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00907, val loss: 0.02149, in 0.109s\n",
      "[43/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.00887, val loss: 0.02142, in 0.209s\n",
      "[44/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00866, val loss: 0.02122, in 0.100s\n",
      "[45/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.00847, val loss: 0.02114, in 0.136s\n",
      "[46/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00832, val loss: 0.02116, in 0.094s\n",
      "[47/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00813, val loss: 0.02099, in 0.160s\n",
      "[48/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00795, val loss: 0.02094, in 0.162s\n",
      "[49/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00777, val loss: 0.02077, in 0.097s\n",
      "[50/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00763, val loss: 0.02070, in 0.103s\n",
      "[51/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00749, val loss: 0.02067, in 0.092s\n",
      "[52/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.00734, val loss: 0.02056, in 0.186s\n",
      "[53/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00721, val loss: 0.02049, in 0.159s\n",
      "[54/200] 1 tree, 127 leaves, max depth = 48, train loss: 0.00706, val loss: 0.02050, in 0.093s\n",
      "[55/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00692, val loss: 0.02045, in 0.132s\n",
      "[56/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00677, val loss: 0.02032, in 0.150s\n",
      "[57/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00664, val loss: 0.02029, in 0.098s\n",
      "[58/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00653, val loss: 0.02033, in 0.094s\n",
      "[59/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00639, val loss: 0.02024, in 0.103s\n",
      "[60/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00626, val loss: 0.02015, in 0.167s\n",
      "[61/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00615, val loss: 0.02016, in 0.096s\n",
      "[62/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00604, val loss: 0.02006, in 0.094s\n",
      "[63/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00594, val loss: 0.01999, in 0.101s\n",
      "[64/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00584, val loss: 0.01990, in 0.167s\n",
      "[65/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00574, val loss: 0.01998, in 0.162s\n",
      "[66/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00564, val loss: 0.01987, in 0.101s\n",
      "[67/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00554, val loss: 0.01990, in 0.095s\n",
      "[68/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00545, val loss: 0.01987, in 0.091s\n",
      "[69/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00536, val loss: 0.01984, in 0.322s\n",
      "[70/200] 1 tree, 127 leaves, max depth = 22, train loss: 0.00529, val loss: 0.01986, in 0.102s\n",
      "[71/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00520, val loss: 0.01987, in 0.093s\n",
      "[72/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00511, val loss: 0.01972, in 0.096s\n",
      "[73/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00503, val loss: 0.01981, in 0.161s\n",
      "[74/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00495, val loss: 0.01981, in 0.090s\n",
      "[75/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00488, val loss: 0.01973, in 0.098s\n",
      "[76/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.00480, val loss: 0.01968, in 0.090s\n",
      "[77/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00473, val loss: 0.01978, in 0.091s\n",
      "[78/200] 1 tree, 127 leaves, max depth = 54, train loss: 0.00465, val loss: 0.01975, in 0.121s\n",
      "[79/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00458, val loss: 0.01969, in 0.088s\n",
      "[80/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00451, val loss: 0.01973, in 0.090s\n",
      "[81/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.00445, val loss: 0.01973, in 0.097s\n",
      "[82/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00438, val loss: 0.01973, in 0.165s\n",
      "[83/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00432, val loss: 0.01973, in 0.096s\n",
      "[84/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00425, val loss: 0.01966, in 0.090s\n",
      "[85/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00419, val loss: 0.01958, in 0.089s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00413, val loss: 0.01943, in 0.112s\n",
      "[87/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00407, val loss: 0.01947, in 0.098s\n",
      "[88/200] 1 tree, 127 leaves, max depth = 45, train loss: 0.00401, val loss: 0.01947, in 0.097s\n",
      "[89/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00394, val loss: 0.01943, in 0.098s\n",
      "[90/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00389, val loss: 0.01943, in 0.132s\n",
      "[91/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00383, val loss: 0.01945, in 0.087s\n",
      "[92/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00378, val loss: 0.01945, in 0.099s\n",
      "[93/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00373, val loss: 0.01944, in 0.099s\n",
      "[94/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00367, val loss: 0.01952, in 0.091s\n",
      "[95/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00362, val loss: 0.01950, in 0.134s\n",
      "[96/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00358, val loss: 0.01960, in 0.089s\n",
      "[97/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00353, val loss: 0.01953, in 0.091s\n",
      "[98/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00348, val loss: 0.01961, in 0.092s\n",
      "[99/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00344, val loss: 0.01961, in 0.122s\n",
      "[100/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00340, val loss: 0.01958, in 0.092s\n",
      "Fit 100 trees in 12.395 s, (12645 total leaves)\n",
      "Time spent computing histograms: 6.446s\n",
      "Time spent finding best splits:  1.248s\n",
      "Time spent applying splits:      2.638s\n",
      "Time spent predicting:           0.039s\n",
      "Binning 0.052 GB of training data: 0.266 s\n",
      "Binning 0.006 GB of validation data: 0.006 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 1 tree, 77 leaves, max depth = 19, train loss: 0.10180, val loss: 0.09912, in 0.064s\n",
      "[2/200] 1 tree, 120 leaves, max depth = 22, train loss: 0.07073, val loss: 0.07210, in 0.085s\n",
      "[3/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.05586, val loss: 0.05923, in 0.079s\n",
      "[4/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.04467, val loss: 0.04893, in 0.119s\n",
      "[5/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.03917, val loss: 0.04466, in 0.080s\n",
      "[6/200] 1 tree, 127 leaves, max depth = 52, train loss: 0.03411, val loss: 0.04021, in 0.080s\n",
      "[7/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.03127, val loss: 0.03857, in 0.087s\n",
      "[8/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.02893, val loss: 0.03688, in 0.117s\n",
      "[9/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.02686, val loss: 0.03563, in 0.078s\n",
      "[10/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.02550, val loss: 0.03477, in 0.077s\n",
      "[11/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.02405, val loss: 0.03355, in 0.126s\n",
      "[12/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.02271, val loss: 0.03275, in 0.145s\n",
      "[13/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.02164, val loss: 0.03188, in 0.141s\n",
      "[14/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.02065, val loss: 0.03135, in 0.148s\n",
      "[15/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01981, val loss: 0.03105, in 0.134s\n",
      "[16/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01886, val loss: 0.03068, in 0.146s\n",
      "[17/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.01821, val loss: 0.03021, in 0.280s\n",
      "[18/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01745, val loss: 0.02955, in 0.092s\n",
      "[19/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.01677, val loss: 0.02917, in 0.082s\n",
      "[20/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01611, val loss: 0.02884, in 0.101s\n",
      "[21/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01550, val loss: 0.02849, in 0.112s\n",
      "[22/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01501, val loss: 0.02822, in 0.084s\n",
      "[23/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.01462, val loss: 0.02800, in 0.158s\n",
      "[24/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01417, val loss: 0.02786, in 0.138s\n",
      "[25/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.01372, val loss: 0.02770, in 0.175s\n",
      "[26/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01329, val loss: 0.02749, in 0.148s\n",
      "[27/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01292, val loss: 0.02733, in 0.137s\n",
      "[28/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01254, val loss: 0.02739, in 0.154s\n",
      "[29/200] 1 tree, 127 leaves, max depth = 50, train loss: 0.01213, val loss: 0.02699, in 0.197s\n",
      "[30/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01179, val loss: 0.02698, in 0.095s\n",
      "[31/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01151, val loss: 0.02675, in 0.087s\n",
      "[32/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01119, val loss: 0.02652, in 0.083s\n",
      "[33/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01085, val loss: 0.02676, in 0.089s\n",
      "[34/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.01054, val loss: 0.02662, in 0.120s\n",
      "[35/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01027, val loss: 0.02658, in 0.090s\n",
      "[36/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01002, val loss: 0.02655, in 0.094s\n",
      "[37/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00976, val loss: 0.02646, in 0.117s\n",
      "[38/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00951, val loss: 0.02631, in 0.124s\n",
      "[39/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00928, val loss: 0.02617, in 0.145s\n",
      "[40/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00907, val loss: 0.02605, in 0.150s\n",
      "[41/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00886, val loss: 0.02608, in 0.095s\n",
      "[42/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00867, val loss: 0.02600, in 0.115s\n",
      "[43/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00846, val loss: 0.02595, in 0.083s\n",
      "[44/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.00826, val loss: 0.02602, in 0.097s\n",
      "[45/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00806, val loss: 0.02594, in 0.084s\n",
      "[46/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00791, val loss: 0.02586, in 0.113s\n",
      "[47/200] 1 tree, 127 leaves, max depth = 20, train loss: 0.00775, val loss: 0.02572, in 0.125s\n",
      "[48/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00756, val loss: 0.02552, in 0.107s\n",
      "[49/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00739, val loss: 0.02544, in 0.091s\n",
      "[50/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00724, val loss: 0.02527, in 0.104s\n",
      "[51/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00709, val loss: 0.02528, in 0.146s\n",
      "[52/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00695, val loss: 0.02518, in 0.096s\n",
      "[53/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00682, val loss: 0.02525, in 0.090s\n",
      "[54/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00666, val loss: 0.02540, in 0.115s\n",
      "[55/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00651, val loss: 0.02541, in 0.138s\n",
      "[56/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00640, val loss: 0.02559, in 0.085s\n",
      "[57/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00628, val loss: 0.02561, in 0.083s\n",
      "[58/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00616, val loss: 0.02566, in 0.086s\n",
      "[59/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00605, val loss: 0.02565, in 0.121s\n",
      "[60/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00593, val loss: 0.02562, in 0.125s\n",
      "[61/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00583, val loss: 0.02555, in 0.114s\n",
      "[62/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00572, val loss: 0.02554, in 0.103s\n",
      "Fit 62 trees in 7.445 s, (7817 total leaves)\n",
      "Time spent computing histograms: 3.852s\n",
      "Time spent finding best splits:  0.837s\n",
      "Time spent applying splits:      1.525s\n",
      "Time spent predicting:           0.020s\n",
      "Binning 0.052 GB of training data: 0.260 s\n",
      "Binning 0.006 GB of validation data: 0.008 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 1 tree, 77 leaves, max depth = 17, train loss: 0.10123, val loss: 0.10022, in 0.095s\n",
      "[2/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.07090, val loss: 0.07101, in 0.285s\n",
      "[3/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.05578, val loss: 0.05889, in 0.083s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.04619, val loss: 0.05092, in 0.081s\n",
      "[5/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.03905, val loss: 0.04377, in 0.081s\n",
      "[6/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.03589, val loss: 0.04165, in 0.109s\n",
      "[7/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.03223, val loss: 0.03894, in 0.081s\n",
      "[8/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.02942, val loss: 0.03633, in 0.087s\n",
      "[9/200] 1 tree, 127 leaves, max depth = 42, train loss: 0.02736, val loss: 0.03469, in 0.108s\n",
      "[10/200] 1 tree, 127 leaves, max depth = 50, train loss: 0.02554, val loss: 0.03374, in 0.089s\n",
      "[11/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.02405, val loss: 0.03243, in 0.105s\n",
      "[12/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.02285, val loss: 0.03159, in 0.082s\n",
      "[13/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.02184, val loss: 0.03151, in 0.087s\n",
      "[14/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.02077, val loss: 0.03088, in 0.087s\n",
      "[15/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.01983, val loss: 0.03052, in 0.123s\n",
      "[16/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.01892, val loss: 0.02984, in 0.080s\n",
      "[17/200] 1 tree, 127 leaves, max depth = 46, train loss: 0.01782, val loss: 0.02888, in 0.081s\n",
      "[18/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01707, val loss: 0.02841, in 0.083s\n",
      "[19/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01644, val loss: 0.02814, in 0.108s\n",
      "[20/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01584, val loss: 0.02782, in 0.081s\n",
      "[21/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01530, val loss: 0.02766, in 0.081s\n",
      "[22/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.01478, val loss: 0.02743, in 0.081s\n",
      "[23/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01426, val loss: 0.02729, in 0.107s\n",
      "[24/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01377, val loss: 0.02710, in 0.089s\n",
      "[25/200] 1 tree, 127 leaves, max depth = 42, train loss: 0.01330, val loss: 0.02709, in 0.085s\n",
      "[26/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.01293, val loss: 0.02697, in 0.083s\n",
      "[27/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01256, val loss: 0.02693, in 0.110s\n",
      "[28/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.01220, val loss: 0.02673, in 0.077s\n",
      "[29/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01184, val loss: 0.02656, in 0.082s\n",
      "[30/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.01152, val loss: 0.02649, in 0.092s\n",
      "[31/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01122, val loss: 0.02647, in 0.106s\n",
      "[32/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01095, val loss: 0.02629, in 0.144s\n",
      "[33/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01057, val loss: 0.02622, in 0.105s\n",
      "[34/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01030, val loss: 0.02592, in 0.105s\n",
      "[35/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01003, val loss: 0.02573, in 0.104s\n",
      "[36/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00977, val loss: 0.02555, in 0.120s\n",
      "[37/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00952, val loss: 0.02540, in 0.103s\n",
      "[38/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00930, val loss: 0.02522, in 0.097s\n",
      "[39/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00907, val loss: 0.02522, in 0.107s\n",
      "[40/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00884, val loss: 0.02512, in 0.133s\n",
      "[41/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00865, val loss: 0.02517, in 0.081s\n",
      "[42/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00844, val loss: 0.02486, in 0.083s\n",
      "[43/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00822, val loss: 0.02499, in 0.084s\n",
      "[44/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00803, val loss: 0.02484, in 0.111s\n",
      "[45/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00784, val loss: 0.02471, in 0.078s\n",
      "[46/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00766, val loss: 0.02470, in 0.084s\n",
      "[47/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.00748, val loss: 0.02453, in 0.083s\n",
      "[48/200] 1 tree, 127 leaves, max depth = 49, train loss: 0.00732, val loss: 0.02461, in 0.118s\n",
      "[49/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00716, val loss: 0.02463, in 0.269s\n",
      "[50/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00699, val loss: 0.02479, in 0.089s\n",
      "[51/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00685, val loss: 0.02468, in 0.087s\n",
      "[52/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00670, val loss: 0.02464, in 0.091s\n",
      "[53/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00656, val loss: 0.02460, in 0.110s\n",
      "[54/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.00641, val loss: 0.02441, in 0.089s\n",
      "[55/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00628, val loss: 0.02453, in 0.084s\n",
      "[56/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00616, val loss: 0.02463, in 0.088s\n",
      "[57/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00602, val loss: 0.02489, in 0.120s\n",
      "[58/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00590, val loss: 0.02493, in 0.085s\n",
      "[59/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00578, val loss: 0.02483, in 0.089s\n",
      "[60/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00567, val loss: 0.02460, in 0.091s\n",
      "[61/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00557, val loss: 0.02458, in 0.083s\n",
      "[62/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00545, val loss: 0.02451, in 0.120s\n",
      "[63/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00536, val loss: 0.02442, in 0.094s\n",
      "[64/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00525, val loss: 0.02454, in 0.092s\n",
      "Fit 64 trees in 6.792 s, (8078 total leaves)\n",
      "Time spent computing histograms: 3.580s\n",
      "Time spent finding best splits:  0.737s\n",
      "Time spent applying splits:      1.210s\n",
      "Time spent predicting:           0.020s\n",
      "Binning 0.052 GB of training data: 0.287 s\n",
      "Binning 0.006 GB of validation data: 0.007 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 1 tree, 77 leaves, max depth = 17, train loss: 0.10060, val loss: 0.10281, in 0.061s\n",
      "[2/200] 1 tree, 121 leaves, max depth = 29, train loss: 0.06961, val loss: 0.07185, in 0.130s\n",
      "[3/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.05519, val loss: 0.05832, in 0.101s\n",
      "[4/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.04401, val loss: 0.04727, in 0.097s\n",
      "[5/200] 1 tree, 127 leaves, max depth = 55, train loss: 0.03763, val loss: 0.04148, in 0.117s\n",
      "[6/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.03410, val loss: 0.03877, in 0.087s\n",
      "[7/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.03105, val loss: 0.03604, in 0.122s\n",
      "[8/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.02902, val loss: 0.03459, in 0.094s\n",
      "[9/200] 1 tree, 127 leaves, max depth = 42, train loss: 0.02696, val loss: 0.03291, in 0.083s\n",
      "[10/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.02514, val loss: 0.03148, in 0.087s\n",
      "[11/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.02364, val loss: 0.03060, in 0.110s\n",
      "[12/200] 1 tree, 127 leaves, max depth = 48, train loss: 0.02233, val loss: 0.02974, in 0.104s\n",
      "[13/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.02118, val loss: 0.02919, in 0.106s\n",
      "[14/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.02016, val loss: 0.02854, in 0.083s\n",
      "[15/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01925, val loss: 0.02821, in 0.111s\n",
      "[16/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01829, val loss: 0.02773, in 0.104s\n",
      "[17/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01742, val loss: 0.02732, in 0.093s\n",
      "[18/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01670, val loss: 0.02700, in 0.088s\n",
      "[19/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.01598, val loss: 0.02650, in 0.113s\n",
      "[20/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01538, val loss: 0.02616, in 0.085s\n",
      "[21/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01485, val loss: 0.02599, in 0.117s\n",
      "[22/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.01435, val loss: 0.02567, in 0.110s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01385, val loss: 0.02537, in 0.125s\n",
      "[24/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.01335, val loss: 0.02543, in 0.081s\n",
      "[25/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01289, val loss: 0.02541, in 0.088s\n",
      "[26/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.01253, val loss: 0.02540, in 0.085s\n",
      "[27/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01220, val loss: 0.02545, in 0.084s\n",
      "[28/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01183, val loss: 0.02528, in 0.122s\n",
      "[29/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01147, val loss: 0.02501, in 0.091s\n",
      "[30/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01120, val loss: 0.02502, in 0.096s\n",
      "[31/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.01089, val loss: 0.02498, in 0.092s\n",
      "[32/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.01060, val loss: 0.02492, in 0.280s\n",
      "[33/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01032, val loss: 0.02476, in 0.087s\n",
      "[34/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.01003, val loss: 0.02460, in 0.102s\n",
      "[35/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00979, val loss: 0.02442, in 0.098s\n",
      "[36/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00956, val loss: 0.02407, in 0.127s\n",
      "[37/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.00935, val loss: 0.02398, in 0.087s\n",
      "[38/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00916, val loss: 0.02404, in 0.082s\n",
      "[39/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.00893, val loss: 0.02410, in 0.092s\n",
      "[40/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00868, val loss: 0.02396, in 0.101s\n",
      "[41/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00847, val loss: 0.02390, in 0.115s\n",
      "[42/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.00824, val loss: 0.02371, in 0.097s\n",
      "[43/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00805, val loss: 0.02375, in 0.103s\n",
      "[44/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00786, val loss: 0.02366, in 0.105s\n",
      "[45/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00768, val loss: 0.02382, in 0.127s\n",
      "[46/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00748, val loss: 0.02382, in 0.099s\n",
      "[47/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00732, val loss: 0.02374, in 0.086s\n",
      "[48/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00716, val loss: 0.02390, in 0.093s\n",
      "[49/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00702, val loss: 0.02374, in 0.140s\n",
      "[50/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00684, val loss: 0.02360, in 0.095s\n",
      "[51/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00669, val loss: 0.02358, in 0.081s\n",
      "[52/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00655, val loss: 0.02355, in 0.079s\n",
      "[53/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.00640, val loss: 0.02366, in 0.108s\n",
      "[54/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00627, val loss: 0.02359, in 0.088s\n",
      "[55/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00614, val loss: 0.02347, in 0.112s\n",
      "[56/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00602, val loss: 0.02344, in 0.109s\n",
      "[57/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00590, val loss: 0.02340, in 0.100s\n",
      "[58/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00579, val loss: 0.02345, in 0.118s\n",
      "[59/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00567, val loss: 0.02365, in 0.088s\n",
      "[60/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00556, val loss: 0.02360, in 0.081s\n",
      "[61/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00545, val loss: 0.02345, in 0.084s\n",
      "[62/200] 1 tree, 127 leaves, max depth = 48, train loss: 0.00534, val loss: 0.02329, in 0.119s\n",
      "[63/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00523, val loss: 0.02324, in 0.085s\n",
      "[64/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00514, val loss: 0.02325, in 0.087s\n",
      "[65/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00505, val loss: 0.02326, in 0.084s\n",
      "[66/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00496, val loss: 0.02326, in 0.110s\n",
      "[67/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00487, val loss: 0.02329, in 0.080s\n",
      "[68/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00478, val loss: 0.02331, in 0.082s\n",
      "[69/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00470, val loss: 0.02323, in 0.081s\n",
      "[70/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00461, val loss: 0.02322, in 0.109s\n",
      "[71/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00454, val loss: 0.02339, in 0.082s\n",
      "[72/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00445, val loss: 0.02342, in 0.081s\n",
      "[73/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00438, val loss: 0.02340, in 0.080s\n",
      "[74/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00432, val loss: 0.02342, in 0.086s\n",
      "[75/200] 1 tree, 127 leaves, max depth = 50, train loss: 0.00424, val loss: 0.02337, in 0.114s\n",
      "[76/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00418, val loss: 0.02333, in 0.080s\n",
      "[77/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00411, val loss: 0.02337, in 0.083s\n",
      "[78/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00405, val loss: 0.02348, in 0.078s\n",
      "[79/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00399, val loss: 0.02341, in 0.255s\n",
      "[80/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00392, val loss: 0.02342, in 0.078s\n",
      "Fit 80 trees in 8.477 s, (10104 total leaves)\n",
      "Time spent computing histograms: 4.519s\n",
      "Time spent finding best splits:  0.854s\n",
      "Time spent applying splits:      1.597s\n",
      "Time spent predicting:           0.026s\n",
      "Binning 0.052 GB of training data: 0.284 s\n",
      "Binning 0.006 GB of validation data: 0.008 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 1 tree, 76 leaves, max depth = 16, train loss: 0.10106, val loss: 0.10780, in 0.061s\n",
      "[2/200] 1 tree, 124 leaves, max depth = 30, train loss: 0.07130, val loss: 0.07434, in 0.084s\n",
      "[3/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.05602, val loss: 0.06047, in 0.083s\n",
      "[4/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.04502, val loss: 0.04943, in 0.117s\n",
      "[5/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.03955, val loss: 0.04465, in 0.082s\n",
      "[6/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.03601, val loss: 0.04141, in 0.080s\n",
      "[7/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.03183, val loss: 0.03770, in 0.081s\n",
      "[8/200] 1 tree, 127 leaves, max depth = 47, train loss: 0.02897, val loss: 0.03520, in 0.114s\n",
      "[9/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.02693, val loss: 0.03377, in 0.085s\n",
      "[10/200] 1 tree, 127 leaves, max depth = 48, train loss: 0.02510, val loss: 0.03226, in 0.082s\n",
      "[11/200] 1 tree, 127 leaves, max depth = 42, train loss: 0.02375, val loss: 0.03112, in 0.097s\n",
      "[12/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.02248, val loss: 0.03026, in 0.160s\n",
      "[13/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.02134, val loss: 0.02984, in 0.109s\n",
      "[14/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.02039, val loss: 0.02938, in 0.090s\n",
      "[15/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.01951, val loss: 0.02843, in 0.124s\n",
      "[16/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.01868, val loss: 0.02836, in 0.108s\n",
      "[17/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01785, val loss: 0.02760, in 0.134s\n",
      "[18/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.01726, val loss: 0.02743, in 0.116s\n",
      "[19/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01651, val loss: 0.02674, in 0.127s\n",
      "[20/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01587, val loss: 0.02640, in 0.116s\n",
      "[21/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.01538, val loss: 0.02621, in 0.140s\n",
      "[22/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.01483, val loss: 0.02550, in 0.104s\n",
      "[23/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01435, val loss: 0.02534, in 0.086s\n",
      "[24/200] 1 tree, 127 leaves, max depth = 44, train loss: 0.01386, val loss: 0.02494, in 0.093s\n",
      "[25/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01337, val loss: 0.02490, in 0.150s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.01295, val loss: 0.02469, in 0.111s\n",
      "[27/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01257, val loss: 0.02437, in 0.119s\n",
      "[28/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.01216, val loss: 0.02451, in 0.130s\n",
      "[29/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.01178, val loss: 0.02410, in 0.166s\n",
      "[30/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01144, val loss: 0.02403, in 0.112s\n",
      "[31/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01108, val loss: 0.02379, in 0.133s\n",
      "[32/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01076, val loss: 0.02361, in 0.111s\n",
      "[33/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01047, val loss: 0.02345, in 0.107s\n",
      "[34/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.01018, val loss: 0.02330, in 0.119s\n",
      "[35/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.00988, val loss: 0.02328, in 0.086s\n",
      "[36/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00961, val loss: 0.02328, in 0.083s\n",
      "[37/200] 1 tree, 127 leaves, max depth = 41, train loss: 0.00938, val loss: 0.02316, in 0.083s\n",
      "[38/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00916, val loss: 0.02321, in 0.115s\n",
      "[39/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00895, val loss: 0.02311, in 0.091s\n",
      "[40/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00876, val loss: 0.02308, in 0.089s\n",
      "[41/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00855, val loss: 0.02291, in 0.116s\n",
      "[42/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00833, val loss: 0.02290, in 0.134s\n",
      "[43/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00812, val loss: 0.02284, in 0.085s\n",
      "[44/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00792, val loss: 0.02278, in 0.082s\n",
      "[45/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00774, val loss: 0.02262, in 0.098s\n",
      "[46/200] 1 tree, 127 leaves, max depth = 50, train loss: 0.00753, val loss: 0.02259, in 0.137s\n",
      "[47/200] 1 tree, 127 leaves, max depth = 22, train loss: 0.00738, val loss: 0.02239, in 0.261s\n",
      "[48/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00720, val loss: 0.02236, in 0.097s\n",
      "[49/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00704, val loss: 0.02243, in 0.089s\n",
      "[50/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00691, val loss: 0.02239, in 0.106s\n",
      "[51/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00676, val loss: 0.02230, in 0.135s\n",
      "[52/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00661, val loss: 0.02221, in 0.088s\n",
      "[53/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00649, val loss: 0.02218, in 0.098s\n",
      "[54/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00637, val loss: 0.02213, in 0.112s\n",
      "[55/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00622, val loss: 0.02218, in 0.129s\n",
      "[56/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00607, val loss: 0.02209, in 0.083s\n",
      "[57/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00596, val loss: 0.02216, in 0.087s\n",
      "[58/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00584, val loss: 0.02216, in 0.085s\n",
      "[59/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00573, val loss: 0.02207, in 0.113s\n",
      "[60/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00562, val loss: 0.02204, in 0.084s\n",
      "[61/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00552, val loss: 0.02213, in 0.085s\n",
      "[62/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00542, val loss: 0.02214, in 0.086s\n",
      "[63/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00533, val loss: 0.02213, in 0.118s\n",
      "[64/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00523, val loss: 0.02205, in 0.104s\n",
      "[65/200] 1 tree, 127 leaves, max depth = 52, train loss: 0.00512, val loss: 0.02198, in 0.100s\n",
      "[66/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00503, val loss: 0.02197, in 0.087s\n",
      "[67/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00493, val loss: 0.02190, in 0.084s\n",
      "[68/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00484, val loss: 0.02182, in 0.121s\n",
      "[69/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00477, val loss: 0.02183, in 0.100s\n",
      "[70/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00468, val loss: 0.02185, in 0.112s\n",
      "[71/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00460, val loss: 0.02173, in 0.100s\n",
      "[72/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00452, val loss: 0.02177, in 0.114s\n",
      "[73/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00444, val loss: 0.02184, in 0.084s\n",
      "[74/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00437, val loss: 0.02188, in 0.085s\n",
      "[75/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00429, val loss: 0.02185, in 0.135s\n",
      "[76/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.00423, val loss: 0.02183, in 0.138s\n",
      "[77/200] 1 tree, 127 leaves, max depth = 49, train loss: 0.00416, val loss: 0.02176, in 0.090s\n",
      "[78/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00409, val loss: 0.02169, in 0.130s\n",
      "[79/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00402, val loss: 0.02167, in 0.128s\n",
      "[80/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00396, val loss: 0.02168, in 0.116s\n",
      "[81/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00390, val loss: 0.02169, in 0.082s\n",
      "[82/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.00384, val loss: 0.02171, in 0.111s\n",
      "[83/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00377, val loss: 0.02180, in 0.131s\n",
      "[84/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.00372, val loss: 0.02183, in 0.111s\n",
      "[85/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00367, val loss: 0.02175, in 0.125s\n",
      "[86/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00362, val loss: 0.02185, in 0.086s\n",
      "[87/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00357, val loss: 0.02192, in 0.088s\n",
      "[88/200] 1 tree, 127 leaves, max depth = 51, train loss: 0.00351, val loss: 0.02182, in 0.084s\n",
      "[89/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00346, val loss: 0.02184, in 0.113s\n",
      "Fit 89 trees in 9.943 s, (11249 total leaves)\n",
      "Time spent computing histograms: 5.328s\n",
      "Time spent finding best splits:  0.977s\n",
      "Time spent applying splits:      1.955s\n",
      "Time spent predicting:           0.030s\n",
      "Binning 0.052 GB of training data: 0.303 s\n",
      "Binning 0.006 GB of validation data: 0.008 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 1 tree, 73 leaves, max depth = 17, train loss: 0.10049, val loss: 0.10711, in 0.061s\n",
      "[2/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.06999, val loss: 0.07282, in 0.086s\n",
      "[3/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.05536, val loss: 0.05919, in 0.101s\n",
      "[4/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.04419, val loss: 0.04861, in 0.104s\n",
      "[5/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.04012, val loss: 0.04518, in 0.311s\n",
      "[6/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.03531, val loss: 0.04132, in 0.117s\n",
      "[7/200] 1 tree, 127 leaves, max depth = 54, train loss: 0.03122, val loss: 0.03751, in 0.105s\n",
      "[8/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.02914, val loss: 0.03593, in 0.106s\n",
      "[9/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.02693, val loss: 0.03433, in 0.125s\n",
      "[10/200] 1 tree, 127 leaves, max depth = 43, train loss: 0.02510, val loss: 0.03298, in 0.106s\n",
      "[11/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.02337, val loss: 0.03154, in 0.121s\n",
      "[12/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.02211, val loss: 0.03098, in 0.125s\n",
      "[13/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.02114, val loss: 0.02992, in 0.108s\n",
      "[14/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.02014, val loss: 0.02959, in 0.097s\n",
      "[15/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01932, val loss: 0.02915, in 0.085s\n",
      "[16/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.01854, val loss: 0.02871, in 0.092s\n",
      "[17/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.01774, val loss: 0.02811, in 0.112s\n",
      "[18/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01720, val loss: 0.02791, in 0.141s\n",
      "[19/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.01657, val loss: 0.02747, in 0.115s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.01592, val loss: 0.02713, in 0.096s\n",
      "[21/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.01542, val loss: 0.02705, in 0.099s\n",
      "[22/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.01479, val loss: 0.02669, in 0.140s\n",
      "[23/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01432, val loss: 0.02638, in 0.101s\n",
      "[24/200] 1 tree, 127 leaves, max depth = 46, train loss: 0.01374, val loss: 0.02596, in 0.103s\n",
      "[25/200] 1 tree, 127 leaves, max depth = 36, train loss: 0.01334, val loss: 0.02554, in 0.129s\n",
      "[26/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.01291, val loss: 0.02528, in 0.142s\n",
      "[27/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01255, val loss: 0.02522, in 0.088s\n",
      "[28/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.01221, val loss: 0.02519, in 0.105s\n",
      "[29/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.01184, val loss: 0.02481, in 0.101s\n",
      "[30/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.01145, val loss: 0.02457, in 0.115s\n",
      "[31/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01115, val loss: 0.02456, in 0.091s\n",
      "[32/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.01082, val loss: 0.02429, in 0.109s\n",
      "[33/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.01051, val loss: 0.02434, in 0.118s\n",
      "[34/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.01020, val loss: 0.02397, in 0.108s\n",
      "[35/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00995, val loss: 0.02401, in 0.144s\n",
      "[36/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00968, val loss: 0.02395, in 0.105s\n",
      "[37/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00946, val loss: 0.02390, in 0.106s\n",
      "[38/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00923, val loss: 0.02382, in 0.106s\n",
      "[39/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00898, val loss: 0.02374, in 0.132s\n",
      "[40/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00874, val loss: 0.02382, in 0.086s\n",
      "[41/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00852, val loss: 0.02361, in 0.085s\n",
      "[42/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00833, val loss: 0.02361, in 0.092s\n",
      "[43/200] 1 tree, 127 leaves, max depth = 23, train loss: 0.00814, val loss: 0.02367, in 0.142s\n",
      "[44/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00795, val loss: 0.02355, in 0.107s\n",
      "[45/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00774, val loss: 0.02371, in 0.103s\n",
      "[46/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00757, val loss: 0.02341, in 0.107s\n",
      "[47/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00738, val loss: 0.02323, in 0.115s\n",
      "[48/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00721, val loss: 0.02316, in 0.087s\n",
      "[49/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00704, val loss: 0.02302, in 0.106s\n",
      "[50/200] 1 tree, 127 leaves, max depth = 42, train loss: 0.00689, val loss: 0.02288, in 0.109s\n",
      "[51/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00673, val loss: 0.02288, in 0.111s\n",
      "[52/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00661, val loss: 0.02292, in 0.288s\n",
      "[53/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00647, val loss: 0.02281, in 0.088s\n",
      "[54/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00634, val loss: 0.02286, in 0.086s\n",
      "[55/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00622, val loss: 0.02280, in 0.085s\n",
      "[56/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00610, val loss: 0.02290, in 0.113s\n",
      "[57/200] 1 tree, 127 leaves, max depth = 24, train loss: 0.00598, val loss: 0.02281, in 0.104s\n",
      "[58/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00586, val loss: 0.02271, in 0.109s\n",
      "[59/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00574, val loss: 0.02246, in 0.107s\n",
      "[60/200] 1 tree, 127 leaves, max depth = 37, train loss: 0.00562, val loss: 0.02241, in 0.140s\n",
      "[61/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00553, val loss: 0.02239, in 0.103s\n",
      "[62/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00540, val loss: 0.02253, in 0.105s\n",
      "[63/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00531, val loss: 0.02259, in 0.107s\n",
      "[64/200] 1 tree, 127 leaves, max depth = 39, train loss: 0.00520, val loss: 0.02237, in 0.095s\n",
      "[65/200] 1 tree, 127 leaves, max depth = 49, train loss: 0.00509, val loss: 0.02221, in 0.126s\n",
      "[66/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00499, val loss: 0.02220, in 0.104s\n",
      "[67/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00492, val loss: 0.02218, in 0.221s\n",
      "[68/200] 1 tree, 127 leaves, max depth = 33, train loss: 0.00483, val loss: 0.02218, in 0.217s\n",
      "[69/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00474, val loss: 0.02231, in 0.230s\n",
      "[70/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00467, val loss: 0.02239, in 0.171s\n",
      "[71/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00458, val loss: 0.02241, in 0.198s\n",
      "[72/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00451, val loss: 0.02241, in 0.147s\n",
      "[73/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00442, val loss: 0.02236, in 0.182s\n",
      "[74/200] 1 tree, 127 leaves, max depth = 40, train loss: 0.00434, val loss: 0.02230, in 0.172s\n",
      "[75/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00427, val loss: 0.02235, in 0.134s\n",
      "[76/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00420, val loss: 0.02228, in 0.108s\n",
      "[77/200] 1 tree, 127 leaves, max depth = 38, train loss: 0.00413, val loss: 0.02217, in 0.137s\n",
      "[78/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00407, val loss: 0.02215, in 0.101s\n",
      "[79/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00401, val loss: 0.02213, in 0.105s\n",
      "[80/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00394, val loss: 0.02210, in 0.106s\n",
      "[81/200] 1 tree, 127 leaves, max depth = 25, train loss: 0.00388, val loss: 0.02213, in 0.097s\n",
      "[82/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00382, val loss: 0.02209, in 0.139s\n",
      "[83/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00377, val loss: 0.02213, in 0.106s\n",
      "[84/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00371, val loss: 0.02208, in 0.103s\n",
      "[85/200] 1 tree, 127 leaves, max depth = 34, train loss: 0.00365, val loss: 0.02200, in 0.158s\n",
      "[86/200] 1 tree, 127 leaves, max depth = 27, train loss: 0.00360, val loss: 0.02200, in 0.158s\n",
      "[87/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00354, val loss: 0.02200, in 0.110s\n",
      "[88/200] 1 tree, 127 leaves, max depth = 31, train loss: 0.00349, val loss: 0.02199, in 0.098s\n",
      "[89/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00344, val loss: 0.02203, in 0.233s\n",
      "[90/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00339, val loss: 0.02210, in 0.138s\n",
      "[91/200] 1 tree, 127 leaves, max depth = 29, train loss: 0.00334, val loss: 0.02204, in 0.115s\n",
      "[92/200] 1 tree, 127 leaves, max depth = 35, train loss: 0.00329, val loss: 0.02209, in 0.101s\n",
      "[93/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00324, val loss: 0.02207, in 0.119s\n",
      "[94/200] 1 tree, 127 leaves, max depth = 32, train loss: 0.00319, val loss: 0.02219, in 0.179s\n",
      "[95/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00316, val loss: 0.02212, in 0.108s\n",
      "[96/200] 1 tree, 127 leaves, max depth = 26, train loss: 0.00311, val loss: 0.02204, in 0.089s\n",
      "[97/200] 1 tree, 127 leaves, max depth = 28, train loss: 0.00307, val loss: 0.02200, in 0.115s\n",
      "[98/200] 1 tree, 127 leaves, max depth = 30, train loss: 0.00303, val loss: 0.02204, in 0.187s\n",
      "Fit 98 trees in 12.502 s, (12392 total leaves)\n",
      "Time spent computing histograms: 6.407s\n",
      "Time spent finding best splits:  1.705s\n",
      "Time spent applying splits:      2.428s\n",
      "Time spent predicting:           0.034s\n",
      "train score= 0.999286\n",
      "cv roc auc= 0.997715\n",
      "cv acc= 0.991920\n",
      "cv rec= 0.861620\n",
      "cv prec= 0.918572\n",
      "y train:\n",
      "n pos 4213.000000\n",
      "frac pos 0.037616\n",
      "y train pred:\n",
      "n pos 4183.000000\n",
      "frac pos 0.037348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hp_x = np.array([0.56])\n",
    "hp_metrics = [\n",
    "           'test_accuracy',\n",
    "           'test_recall',\n",
    "           'test_precision'\n",
    "          ]\n",
    "hp_y = np.zeros((hp_x.shape[0],len(hp_metrics)))\n",
    "\n",
    "for j in range(0,hp_x.shape[0]):\n",
    "    \n",
    "    clf_init = HistGradientBoostingClassifier(\n",
    "        learning_rate = hp_x[j],\n",
    "        max_iter=200,\n",
    "        max_leaf_nodes=127,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=3.2,\n",
    "        verbose=10,\n",
    "        random_state=42)\n",
    "    \n",
    "    clf = clf_init.fit(X_train,y_train)\n",
    "    score = clf.score(X_train,y_train)\n",
    "\n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "\n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    hp_y[j][0] = np.mean(cv_results['test_accuracy'])\n",
    "    \n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    hp_y[j][1] = np.mean(cv_results['test_recall'])\n",
    "    \n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    hp_y[j][2] = np.mean(cv_results['test_precision'])\n",
    "\n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "\n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "\n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "\n",
    "    #p_train=clf.predict_proba(X_train)\n",
    "    #print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    #print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "\n",
    "    #print(p_train)\n",
    "\n",
    "    #print(\"y test:\")\n",
    "    #p_test=clf.predict_proba(X_test)\n",
    "    #print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    #print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "\n",
    "    if holdout:\n",
    "        #print(\"holdout metrics:\")\n",
    "        #holdout_auc = roc_auc_score(y_hold,clf.predict_proba(X_hold)[:,1])\n",
    "        #print(\"holdout roc auc= %f\"%holdout_auc)\n",
    "        #holdout_aucs[i] = holdout_auc\n",
    "\n",
    "        y_hold_pred = clf.predict(X_hold)\n",
    "        holdout_acc = accuracy_score(y_hold,y_hold_pred)\n",
    "        print(\"holdout acc= %f\"%holdout_acc)\n",
    "        holdout_rec = recall_score(y_hold,y_hold_pred)\n",
    "        print(\"holdout rec= %f\"%holdout_rec)\n",
    "        holdout_prec = precision_score(y_hold,y_hold_pred)\n",
    "        print(\"holdout prec= %f\"%holdout_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112000, 80)\n",
      "(112000,)\n",
      "(48000, 80)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob sum 0 107806.647868\n",
      "prob sum 1 4193.352132\n"
     ]
    }
   ],
   "source": [
    "p_train=clf.predict_proba(X_train)\n",
    "print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51954317 13.29219084]\n"
     ]
    }
   ],
   "source": [
    "print(n/(2*np.bincount(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y test pred:\n",
      "n pos 1715.000000\n",
      "frac pos 0.015312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"y test pred:\")\n",
    "y_test_pred=clf.predict(X_test)\n",
    "print(\"n pos %f\"%np.sum(y_test_pred))\n",
    "print(\"frac pos %f\"%(np.sum(y_test_pred)/n) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_str = \"\"\n",
    "for i in range(len(y_test_pred)):\n",
    "    result_str+=str(y_test_pred[i])\n",
    "    result_str+=\"\\n\"\n",
    "\n",
    "submission = open(\"sub2.csv\",\"w\")\n",
    "submission.write(result_str)\n",
    "submission.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56]\n",
      "[[0.99191964 0.86161994 0.91857162]]\n"
     ]
    }
   ],
   "source": [
    "#HIST GBDT l2, stepsize experiment, 200 trees/iter\n",
    "\n",
    "print(hp_x)\n",
    "print(hp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "for k in range(0,len(hp_metrics)):\n",
    "    ax1.plot(np.log10(hp_x),hp_y[:,k],label=hp_metrics[k])\n",
    "    \n",
    "f1 = 2*hp_y[:,1]*hp_y[:,2]/(hp_y[:,1]+hp_y[:,2])\n",
    "ax1.plot(np.log10(hp_x),f1,label=\"f1\")\n",
    "    \n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
