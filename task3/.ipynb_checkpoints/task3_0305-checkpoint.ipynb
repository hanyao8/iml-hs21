{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score, recall_score,accuracy_score, precision_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(\"data/sample.csv\")\n",
    "\n",
    "df_train_feats = pd.read_csv(\"data/train_features.csv\")\n",
    "df_train_labels = pd.read_csv(\"data/train_labels.csv\")\n",
    "df_test_feats = pd.read_csv(\"data/test_features.csv\")\n",
    "\n",
    "df_labels_cols = list(df_train_labels)\n",
    "\n",
    "active_feats = [\n",
    " 'Age',\n",
    " 'EtCO2',\n",
    " 'PTT',\n",
    " 'BUN',\n",
    " 'Lactate',\n",
    " 'Temp',\n",
    " 'Hgb',\n",
    " 'HCO3',\n",
    " 'BaseExcess',\n",
    " 'RRate',\n",
    " 'Fibrinogen',\n",
    " 'Phosphate',\n",
    " 'WBC',\n",
    " 'Creatinine',\n",
    " 'PaCO2',\n",
    " 'AST',\n",
    " 'FiO2',\n",
    " 'Platelets',\n",
    " 'SaO2',\n",
    " 'Glucose',\n",
    " 'ABPm',\n",
    " 'Magnesium',\n",
    " 'Potassium',\n",
    " 'ABPd',\n",
    " 'Calcium',\n",
    " 'Alkalinephos',\n",
    " 'SpO2',\n",
    " 'Bilirubin_direct',\n",
    " 'Chloride',\n",
    " 'Hct',\n",
    " 'Heartrate',\n",
    " 'Bilirubin_total',\n",
    " 'TroponinI',\n",
    " 'ABPs',\n",
    " 'pH']\n",
    "\n",
    "subtask3_feats = [\n",
    " 'RRate',\n",
    " 'ABPm',\n",
    " 'SpO2',\n",
    " 'Heartrate']\n",
    "\n",
    "subtask1_labels = [\n",
    " 'LABEL_BaseExcess',\n",
    " 'LABEL_Fibrinogen',\n",
    " 'LABEL_AST',\n",
    " 'LABEL_Alkalinephos',\n",
    " 'LABEL_Bilirubin_total',\n",
    " 'LABEL_Lactate',\n",
    " 'LABEL_TroponinI',\n",
    " 'LABEL_SaO2',\n",
    " 'LABEL_Bilirubin_direct',\n",
    " 'LABEL_EtCO2'\n",
    "        ]\n",
    "\n",
    "subtask2_labels = [\n",
    " 'LABEL_Sepsis'\n",
    " ]\n",
    "\n",
    "subtask3_labels = [\n",
    " 'LABEL_RRate',\n",
    " 'LABEL_ABPm',\n",
    " 'LABEL_SpO2',\n",
    " 'LABEL_Heartrate'\n",
    "        ]\n",
    "\n",
    "\n",
    "def feats_2_X1(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    n_derived_feats = 4\n",
    "\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            patient_feat = patient_feat[~np.isnan(patient_feat)]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "                X[i][j0+1] = np.std(patient_feat)\n",
    "                X[i][j0+2] = np.min(patient_feat)\n",
    "                X[i][j0+3] = np.max(patient_feat)\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n",
    "\n",
    "def feats_2_X2(df_feats,feats_list):\n",
    "    #Subtask2\n",
    "    n_derived_feats = 4\n",
    "    #n_derived_feats = 1\n",
    "\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            patient_feat = patient_feat[~np.isnan(patient_feat)]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "                X[i][j0+1] = np.std(patient_feat)\n",
    "                X[i][j0+2] = np.min(patient_feat)\n",
    "                X[i][j0+3] = np.max(patient_feat)\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n",
    "\n",
    "\n",
    "def feats_2_X3(df_feats,feats_list):\n",
    "    #Subtask3\n",
    "    n_derived_feats = 8\n",
    "\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            nan_mask = np.isnan(patient_feat)\n",
    "            times = (patient_df['Time'].values)[~nan_mask]\n",
    "            times = times.astype('float')\n",
    "            patient_feat = patient_feat[~nan_mask]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            n_measurements = len(patient_feat)\n",
    "            if n_measurements>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "            if n_measurements>1:\n",
    "                X[i][j0+1] = patient_feat[-1]\n",
    "                grad_0 = (patient_feat[-1]-patient_feat[0])/(times[-1]-times[0])\n",
    "                X[i][j0+2] = grad_0\n",
    "            if n_measurements>2:\n",
    "                mid_idx = int((n_measurements-1)/2)\n",
    "                X[i][j0+3] = patient_feat[mid_idx]\n",
    "                \n",
    "                grad_1 = (patient_feat[-1]-patient_feat[-2])/(times[-1]-times[-2])\n",
    "                X[i][j0+4] = grad_1\n",
    "                grad_2 = (patient_feat[-2]-patient_feat[-3])/(times[-2]-times[-3])\n",
    "                X[i][j0+5] = grad_2\n",
    "                \n",
    "                curve_0 = (grad_1-grad_0)/(0.5*times[-2]-0.5*times[0])\n",
    "                X[i][j0+6] = curve_0\n",
    "                curve_1 = (grad_1-grad_2)/(0.5*times[-1]-0.5*times[-3])\n",
    "                X[i][j0+7] = curve_1\n",
    "                \n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return(X_train,X_test)\n",
    "    \n",
    "\n",
    "def impute(X_train,X_test):\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean.fit(X_train)\n",
    "    \n",
    "    X_train = imp_mean.transform(X_train)\n",
    "    X_test = imp_mean.transform(X_test)\n",
    "\n",
    "    #print(X_train_imp)\n",
    "    return(X_train,X_test)\n",
    "\n",
    "def forest_fi(X_train,y_train,X_test):\n",
    "\n",
    "    forest = ExtraTreesClassifier(n_estimators=20,\n",
    "                                  random_state=0)\n",
    "\n",
    "    forest.fit(X_train,y_train)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    X_train = X_train[:,indices[:50]]\n",
    "    X_test = X_test[:,indices[:50]]\n",
    "    return(X_train,X_test)\n",
    "\n",
    "\n",
    "def nystroem(X_train,X_test):\n",
    "    gamma=1.0\n",
    "    n_components=100\n",
    "    print(\"nystroem gamma=%f\"%(gamma))\n",
    "    print(\"nystroem q=%d\"%(n_components))\n",
    "    feature_map_nystroem = Nystroem(gamma=gamma,\n",
    "                                    random_state=42,\n",
    "                                    n_components=n_components)\n",
    "    feature_map_nystroem.fit(X_train)\n",
    "    Q_train = feature_map_nystroem.transform(X_train)\n",
    "    sqrt_k_inv_train = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "    B_train = np.dot(Q_train,sqrt_k_inv_train)\n",
    "    K_train = np.dot(B_train,np.transpose(B_train))\n",
    "\n",
    "    Q_test = feature_map_nystroem.transform(X_test)\n",
    "    sqrt_k_inv_test = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "    B_test = np.dot(Q_test,sqrt_k_inv_test)\n",
    "    K_test = np.dot(B_test,np.transpose(B_train))\n",
    "    return(K_train,K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "def fit_model1_1(X_train,y_train):\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "                                     max_depth=3, random_state=42).fit(X_train,y_train)\n",
    "    #clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n",
    "    #                                 max_depth=5, random_state=42).fit(X_train_imp,y_train)\n",
    "\n",
    "    return(clf)\n",
    "\n",
    "def fit_model1_2(X_train,y_train):\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=10,\n",
    "                  class_weight=\"balanced\",\n",
    "                  decision_function_shape='ovo').fit(X_train,y_train)\n",
    "    return(clf)\n",
    "\n",
    "def fit_model1(X_train,y_train,clf_init,sw_dict={}):\n",
    "    n = np.shape(y_train)[0]\n",
    "    w0 = n/(n-np.sum(y_train))\n",
    "    w1 = n/np.sum(y_train)\n",
    "    \n",
    "    sample_weight = np.zeros(len(y_train))\n",
    "    if not(sw_dict):\n",
    "        print(\"using default sample weights\")\n",
    "        sample_weight[y_train == 0] = w0\n",
    "        sample_weight[y_train == 1] = w1\n",
    "    else:\n",
    "        print(\"using custom sample weights\")\n",
    "        sample_weight[y_train == 0] = sw_dict[0]\n",
    "        sample_weight[y_train == 1] = sw_dict[1]\n",
    "    \n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = clf_init\n",
    "    #clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "    #                                 max_depth=3, random_state=42)\n",
    "    clf.fit(X_train,y_train,sample_weight=sample_weight)\n",
    "    #clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n",
    "    #                                 max_depth=5, random_state=42).fit(X_train_imp,y_train)\n",
    "\n",
    "    return(clf)\n",
    "\n",
    "def fit_model2_1(X_train,y_train,hps):\n",
    "    C = hps[\"C\"]\n",
    "    #class_weight={1:16.5}\n",
    "    class_weight={1:hps[\"w1\"]}\n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=hps[\"C\"],\n",
    "                  kernel=\"rbf\",\n",
    "                  gamma=hps[\"gamma\"],\n",
    "                  class_weight=class_weight,\n",
    "                  decision_function_shape='ovo',\n",
    "                  verbose=True).fit(X_train,y_train)\n",
    "    print(\"C: %f\"%hps[\"C\"])\n",
    "    print(\"gamma: %f\"%hps[\"gamma\"])\n",
    "    print(\"Computed class weight: %s\"%(str(clf.class_weight_)))\n",
    "    return(clf)\n",
    "\n",
    "def fit_model2_2(K_train,y_train):\n",
    "    C = 1.0\n",
    "    class_weight={1:17.5}\n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=C,\n",
    "                  kernel=\"precomputed\",\n",
    "                  class_weight=\"balanced\",\n",
    "                  decision_function_shape='ovo',\n",
    "                  verbose=True).fit(K_train,y_train)\n",
    "    print(\"C: %f\"%C)\n",
    "    print(\"Computed class weight: %s\"%(str(clf.class_weight_)))\n",
    "    return(clf)\n",
    "\n",
    "def fit_model2(X_train,y_train,clf_init,sw_dict={}):\n",
    "    n = np.shape(y_train)[0]\n",
    "    w0 = n/(n-np.sum(y_train))\n",
    "    w1 = n/np.sum(y_train)\n",
    "    \n",
    "    sample_weight = np.zeros(len(y_train))\n",
    "    if not(sw_dict):\n",
    "        print(\"using default sample weights\")\n",
    "        sample_weight[y_train == 0] = w0\n",
    "        sample_weight[y_train == 1] = w1\n",
    "    else:\n",
    "        print(\"using custom sample weights\")\n",
    "        sample_weight[y_train == 0] = sw_dict[0]\n",
    "        sample_weight[y_train == 1] = sw_dict[1]\n",
    "    \n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = clf_init\n",
    "    clf.fit(X_train,y_train,sample_weight=sample_weight)\n",
    "    return(clf)\n",
    "\n",
    "def fit_model3(X_train,y_train):\n",
    "    #print(np.shape(y_train))\n",
    "    #print(np.sum(y_train))\n",
    "    reg = HistGradientBoostingRegressor(random_state=42).fit(X_train,y_train)\n",
    "    #reg = GradientBoostingRegressor(random_state=42).fit(X_train,y_train)\n",
    "    return(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pids = pd.unique(df_test_feats['pid'])\n",
    "\n",
    "df_test_labels = pd.DataFrame(columns=df_labels_cols)\n",
    "df_test_labels['pid'] = test_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "140\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "12.0\n",
      "12664\n",
      "140\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "X_train1_raw = feats_2_X1(df_train_feats,active_feats)\n",
    "X_test1_raw = feats_2_X1(df_test_feats,active_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "5096.0\n",
      "train score= 0.919137\n",
      "cv roc auc= 0.928645\n",
      "cv acc= 0.876599\n",
      "cv rec= 0.756671\n",
      "cv prec= 0.777560\n",
      "y train:\n",
      "n pos 5096.000000\n",
      "frac pos 0.268281\n",
      "y train pred:\n",
      "n pos 4894.000000\n",
      "frac pos 0.257647\n",
      "prob sum 0 13901.346882\n",
      "prob sum 1 5093.653118\n",
      "y test:\n",
      "prob sum 0 9262.081306\n",
      "prob sum 1 3401.918694\n",
      "\n",
      "\n",
      "\n",
      "i=1\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1400.0\n",
      "train score= 0.956568\n",
      "cv roc auc= 0.799989\n",
      "cv acc= 0.936510\n",
      "cv rec= 0.252143\n",
      "cv prec= 0.694805\n",
      "y train:\n",
      "n pos 1400.000000\n",
      "frac pos 0.073704\n",
      "y train pred:\n",
      "n pos 623.000000\n",
      "frac pos 0.032798\n",
      "prob sum 0 17593.553715\n",
      "prob sum 1 1401.446285\n",
      "y test:\n",
      "prob sum 0 11808.218180\n",
      "prob sum 1 855.781820\n",
      "\n",
      "\n",
      "\n",
      "i=2\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4554.0\n",
      "train score= 0.839431\n",
      "cv roc auc= 0.746393\n",
      "cv acc= 0.791261\n",
      "cv rec= 0.240665\n",
      "cv prec= 0.685834\n",
      "y train:\n",
      "n pos 4554.000000\n",
      "frac pos 0.239747\n",
      "y train pred:\n",
      "n pos 1890.000000\n",
      "frac pos 0.099500\n",
      "prob sum 0 14444.025451\n",
      "prob sum 1 4550.974549\n",
      "y test:\n",
      "prob sum 0 9656.241647\n",
      "prob sum 1 3007.758353\n",
      "\n",
      "\n",
      "\n",
      "i=3\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4487.0\n",
      "train score= 0.855120\n",
      "cv roc auc= 0.748770\n",
      "cv acc= 0.793683\n",
      "cv rec= 0.239136\n",
      "cv prec= 0.681177\n",
      "y train:\n",
      "n pos 4487.000000\n",
      "frac pos 0.236220\n",
      "y train pred:\n",
      "n pos 2111.000000\n",
      "frac pos 0.111135\n",
      "prob sum 0 14508.359401\n",
      "prob sum 1 4486.640599\n",
      "y test:\n",
      "prob sum 0 9713.313480\n",
      "prob sum 1 2950.686520\n",
      "\n",
      "\n",
      "\n",
      "i=4\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4570.0\n",
      "train score= 0.823375\n",
      "cv roc auc= 0.749772\n",
      "cv acc= 0.791419\n",
      "cv rec= 0.247265\n",
      "cv prec= 0.685959\n",
      "y train:\n",
      "n pos 4570.000000\n",
      "frac pos 0.240590\n",
      "y train pred:\n",
      "n pos 1725.000000\n",
      "frac pos 0.090813\n",
      "prob sum 0 14421.246079\n",
      "prob sum 1 4573.753921\n",
      "y test:\n",
      "prob sum 0 9634.475671\n",
      "prob sum 1 3029.524329\n",
      "\n",
      "\n",
      "\n",
      "i=5\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "3803.0\n",
      "train score= 0.869597\n",
      "cv roc auc= 0.806809\n",
      "cv acc= 0.836747\n",
      "cv rec= 0.343940\n",
      "cv prec= 0.683413\n",
      "y train:\n",
      "n pos 3803.000000\n",
      "frac pos 0.200211\n",
      "y train pred:\n",
      "n pos 1992.000000\n",
      "frac pos 0.104870\n",
      "prob sum 0 15196.178469\n",
      "prob sum 1 3798.821531\n",
      "y test:\n",
      "prob sum 0 10152.507181\n",
      "prob sum 1 2511.492819\n",
      "\n",
      "\n",
      "\n",
      "i=6\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1895.0\n",
      "train score= 0.946618\n",
      "cv roc auc= 0.895883\n",
      "cv acc= 0.925349\n",
      "cv rec= 0.452770\n",
      "cv prec= 0.692305\n",
      "y train:\n",
      "n pos 1895.000000\n",
      "frac pos 0.099763\n",
      "y train pred:\n",
      "n pos 1295.000000\n",
      "frac pos 0.068176\n",
      "prob sum 0 17116.695979\n",
      "prob sum 1 1878.304021\n",
      "y test:\n",
      "prob sum 0 11440.235315\n",
      "prob sum 1 1223.764685\n",
      "\n",
      "\n",
      "\n",
      "i=7\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4439.0\n",
      "train score= 0.878968\n",
      "cv roc auc= 0.828870\n",
      "cv acc= 0.831535\n",
      "cv rec= 0.477809\n",
      "cv prec= 0.706972\n",
      "y train:\n",
      "n pos 4439.000000\n",
      "frac pos 0.233693\n",
      "y train pred:\n",
      "n pos 3110.000000\n",
      "frac pos 0.163727\n",
      "prob sum 0 14550.575846\n",
      "prob sum 1 4444.424154\n",
      "y test:\n",
      "prob sum 0 9708.973504\n",
      "prob sum 1 2955.026496\n",
      "\n",
      "\n",
      "\n",
      "i=8\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "644.0\n",
      "train score= 0.973361\n",
      "cv roc auc= 0.758276\n",
      "cv acc= 0.967202\n",
      "cv rec= 0.090044\n",
      "cv prec= 0.604787\n",
      "y train:\n",
      "n pos 644.000000\n",
      "frac pos 0.033904\n",
      "y train pred:\n",
      "n pos 162.000000\n",
      "frac pos 0.008529\n",
      "prob sum 0 18340.588541\n",
      "prob sum 1 654.411459\n",
      "y test:\n",
      "prob sum 0 12258.290858\n",
      "prob sum 1 405.709142\n",
      "\n",
      "\n",
      "\n",
      "i=9\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1254.0\n",
      "train score= 0.980521\n",
      "cv roc auc= 0.932952\n",
      "cv acc= 0.962411\n",
      "cv rec= 0.574209\n",
      "cv prec= 0.800177\n",
      "y train:\n",
      "n pos 1254.000000\n",
      "frac pos 0.066017\n",
      "y train pred:\n",
      "n pos 990.000000\n",
      "frac pos 0.052119\n",
      "prob sum 0 17743.116015\n",
      "prob sum 1 1251.883985\n",
      "y test:\n",
      "prob sum 0 11874.409512\n",
      "prob sum 1 789.590488\n",
      "\n",
      "\n",
      "\n",
      "Hold out aucs and mean\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train1_raw))\n",
    "print(np.shape(X_test1_raw))\n",
    "X_train_imp,X_test = impute(X_train1_raw,X_test1_raw)\n",
    "print(np.shape(X_train_imp))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "holdout_aucs = np.zeros(len(subtask1_labels))\n",
    "\n",
    "for i in range(0,len(subtask1_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask1_labels[i]].values\n",
    "    n = float(np.shape(y_train)[0])\n",
    "\n",
    "    if holdout:\n",
    "        #make hold out\n",
    "        X_train,X_hold,y_train,y_hold = train_test_split(\n",
    "            X_train_imp,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "        print(np.shape(X_train))\n",
    "        print(np.shape(X_hold))\n",
    "        print(np.sum(y_train))\n",
    "        print(np.sum(y_hold))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        X_train = X_train_imp\n",
    "\n",
    "    #sample weight\n",
    "    use_custom_sw = True\n",
    "    if use_custom_sw:\n",
    "        #sample weight\n",
    "        n = float(np.shape(y_train)[0])\n",
    "        w1_boost = 0.17\n",
    "\n",
    "        w0 = n/(n-np.sum(y_train))\n",
    "        w1 = (n/np.sum(y_train))*w1_boost\n",
    "        geo_mean = np.sqrt(w0*w1)\n",
    "        w0 /= geo_mean\n",
    "        w1 /= geo_mean\n",
    "    else:\n",
    "        w0=1.0\n",
    "        w1=1.0\n",
    "    \n",
    "    clf_init = HistGradientBoostingClassifier(random_state=42)\n",
    "    clf = fit_model1(X_train,y_train,clf_init,sw_dict={0:w0,1:w1})\n",
    "    score = clf.score(X_train,y_train)\n",
    "    \n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "    \n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "    \n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "    \n",
    "    p_train=clf.predict_proba(X_train)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "    \n",
    "    #print(p_train)\n",
    "    \n",
    "    print(\"y test:\")\n",
    "    p_test=clf.predict_proba(X_test)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "    \n",
    "    if holdout:\n",
    "        print(\"holdout metrics:\")\n",
    "        holdout_auc = roc_auc_score(y_hold,clf.predict_proba(X_hold)[:,1])\n",
    "        print(\"holdout roc auc= %f\"%holdout_auc)\n",
    "        holdout_aucs[i] = holdout_auc\n",
    "\n",
    "        y_hold_pred = clf.predict(X_hold)\n",
    "        holdout_acc = accuracy_score(y_hold,y_hold_pred)\n",
    "        print(\"holdout acc= %f\"%holdout_acc)\n",
    "        holdout_rec = recall_score(y_hold,y_hold_pred)\n",
    "        print(\"holdout rec= %f\"%holdout_rec)\n",
    "        holdout_prec = precision_score(y_hold,y_hold_pred)\n",
    "        print(\"holdout prec= %f\"%holdout_prec)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #y_test[:,1+i] = p_test[:,1]\n",
    "    \n",
    "    df_test_labels[subtask1_labels[i]] = p_test[:,1]\n",
    "    \n",
    "print(\"Hold out aucs and mean\")\n",
    "print(holdout_aucs)\n",
    "print(np.mean(holdout_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "140\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "12.0\n",
      "12664\n",
      "140\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "X_train2_raw = feats_2_X2(df_train_feats,active_feats)\n",
    "X_test2_raw = feats_2_X2(df_test_feats,active_feats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "class weights: 0.597831,1.672713\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1088.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hchoong/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/hchoong/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score= 0.958147\n",
      "cv roc auc= 0.721744\n",
      "cv acc= 0.942459\n",
      "cv rec= 0.003682\n",
      "cv prec= 0.152381\n",
      "y train:\n",
      "n pos 1088.000000\n",
      "frac pos 0.057278\n",
      "y train pred:\n",
      "n pos 319.000000\n",
      "frac pos 0.016794\n",
      "prob sum 0 16737.304339\n",
      "prob sum 1 2257.695661\n",
      "y test:\n",
      "prob sum 0 11181.194214\n",
      "prob sum 1 1482.805786\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train2_raw))\n",
    "print(np.shape(X_test2_raw))\n",
    "\n",
    "X_train_imp,X_test = impute(X_train2_raw,X_test2_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "#X_train,X_test = standardize(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))\n",
    "\n",
    "\n",
    "#X_train,X_test = nystroem(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))\n",
    "\n",
    "hp_x = np.array([0.05])\n",
    "hp_metrics = ['hold_roc_auc',\n",
    "           'hold_accuracy',\n",
    "           'hold_recall',\n",
    "           'hold_precision'\n",
    "          ]\n",
    "hp_y = np.zeros((hp_x.shape[0],len(hp_metrics)))\n",
    "\n",
    "for j in range(0,hp_x.shape[0]):\n",
    "    for i in range(0,len(subtask2_labels)):\n",
    "    #for i in range(0,1):\n",
    "        print(\"i=%d\"%i)\n",
    "        y_train=df_train_labels[subtask2_labels[i]].values\n",
    "        n = float(np.shape(y_train)[0])\n",
    "\n",
    "        if holdout:\n",
    "            #make hold out\n",
    "            X_train,X_hold,y_train,y_hold = train_test_split(\n",
    "                X_train_imp,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "            print(np.shape(X_train))\n",
    "            print(np.shape(X_hold))\n",
    "            print(np.sum(y_train))\n",
    "            print(np.sum(y_hold))\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            X_train = X_train_imp\n",
    "\n",
    "        #sample weight\n",
    "        use_custom_sw = True\n",
    "        if use_custom_sw:\n",
    "            #sample weight\n",
    "            n = float(np.shape(y_train)[0])\n",
    "            w1_boost = 0.17\n",
    "\n",
    "            w0 = n/(n-np.sum(y_train))\n",
    "            w1 = (n/np.sum(y_train))*w1_boost\n",
    "            geo_mean = np.sqrt(w0*w1)\n",
    "            w0 /= geo_mean\n",
    "            w1 /= geo_mean\n",
    "        else:\n",
    "            w0=1.0\n",
    "            w1=1.0\n",
    "            \n",
    "        print(\"class weights: %f,%f\"%(w0,w1))\n",
    "\n",
    "        clf_init = HistGradientBoostingClassifier(\n",
    "            learning_rate=hp_x[j],random_state=42)\n",
    "        clf = fit_model2(X_train,y_train,clf_init,sw_dict={0:w0,1:w1})\n",
    "        score = clf.score(X_train,y_train)\n",
    "\n",
    "        cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "                scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "\n",
    "        print(\"train score= %f\"%score)\n",
    "        print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "        print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "        print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "        print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "\n",
    "        #print(sigmoid(clf.decision_function(X_train)))\n",
    "        #p_train=clf.predict(X_train)\n",
    "\n",
    "        print(\"y train:\")\n",
    "        print(\"n pos %f\"%np.sum(y_train))\n",
    "        print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "\n",
    "        print(\"y train pred:\")\n",
    "        y_train_pred=clf.predict(X_train)\n",
    "        print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "        print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "\n",
    "        p_train=clf.predict_proba(X_train)\n",
    "        print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "        print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "\n",
    "        #print(p_train)\n",
    "\n",
    "        print(\"y test:\")\n",
    "        p_test=clf.predict_proba(X_test)\n",
    "        print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "        print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "\n",
    "        if holdout:\n",
    "\n",
    "            print(\"holdout metrics:\")\n",
    "            holdout_auc = roc_auc_score(y_hold,clf.predict_proba(X_hold)[:,1])\n",
    "            print(\"holdout roc auc= %f\"%holdout_auc)\n",
    "            hp_y[j][0] = holdout_auc\n",
    "\n",
    "            y_hold_pred = clf.predict(X_hold)\n",
    "            holdout_acc = accuracy_score(y_hold,y_hold_pred)\n",
    "            print(\"holdout acc= %f\"%holdout_acc)\n",
    "            hp_y[j][1] = holdout_acc\n",
    "\n",
    "            holdout_rec = recall_score(y_hold,y_hold_pred)\n",
    "            print(\"holdout rec= %f\"%holdout_rec)\n",
    "            hp_y[j][2] = holdout_rec\n",
    "\n",
    "            holdout_prec = precision_score(y_hold,y_hold_pred)\n",
    "            print(\"holdout prec= %f\"%holdout_prec)\n",
    "            hp_y[j][3] = holdout_prec\n",
    "\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        #y_test[:,1+i] = p_test[:,1]\n",
    "\n",
    "        df_test_labels[subtask2_labels[i]] = p_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "for k in range(0,len(hp_metrics)):\n",
    "    ax1.plot(np.log10(hp_x),hp_y[:,k],label=hp_metrics[k])\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.913735</td>\n",
       "      <td>0.702062</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.831818</td>\n",
       "      <td>0.869098</td>\n",
       "      <td>0.280766</td>\n",
       "      <td>0.013758</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.337020</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>0.146291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>0.016663</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.204590</td>\n",
       "      <td>0.165778</td>\n",
       "      <td>0.168813</td>\n",
       "      <td>0.042430</td>\n",
       "      <td>0.101528</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>0.095808</td>\n",
       "      <td>0.029069</td>\n",
       "      <td>0.059008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>0.010984</td>\n",
       "      <td>0.048353</td>\n",
       "      <td>0.099560</td>\n",
       "      <td>0.117169</td>\n",
       "      <td>0.105025</td>\n",
       "      <td>0.142847</td>\n",
       "      <td>0.036623</td>\n",
       "      <td>0.146478</td>\n",
       "      <td>0.084686</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.058987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.084854</td>\n",
       "      <td>0.197733</td>\n",
       "      <td>0.184202</td>\n",
       "      <td>0.235022</td>\n",
       "      <td>0.060112</td>\n",
       "      <td>0.045935</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.035853</td>\n",
       "      <td>0.036303</td>\n",
       "      <td>0.051174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>0.065356</td>\n",
       "      <td>0.044514</td>\n",
       "      <td>0.066941</td>\n",
       "      <td>0.067434</td>\n",
       "      <td>0.071771</td>\n",
       "      <td>0.051637</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.026642</td>\n",
       "      <td>0.034870</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.061610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>9989</td>\n",
       "      <td>0.388578</td>\n",
       "      <td>0.070439</td>\n",
       "      <td>0.116099</td>\n",
       "      <td>0.108199</td>\n",
       "      <td>0.073532</td>\n",
       "      <td>0.271521</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.060059</td>\n",
       "      <td>0.032890</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.154793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>9991</td>\n",
       "      <td>0.149033</td>\n",
       "      <td>0.190217</td>\n",
       "      <td>0.181947</td>\n",
       "      <td>0.100179</td>\n",
       "      <td>0.237351</td>\n",
       "      <td>0.185677</td>\n",
       "      <td>0.037585</td>\n",
       "      <td>0.108925</td>\n",
       "      <td>0.046437</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>0.098283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>9992</td>\n",
       "      <td>0.484737</td>\n",
       "      <td>0.049001</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.039121</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.131843</td>\n",
       "      <td>0.009448</td>\n",
       "      <td>0.445695</td>\n",
       "      <td>0.034798</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.088425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.970250</td>\n",
       "      <td>0.810563</td>\n",
       "      <td>0.909453</td>\n",
       "      <td>0.915812</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.885021</td>\n",
       "      <td>0.029745</td>\n",
       "      <td>0.888407</td>\n",
       "      <td>0.235861</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.362949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.769208</td>\n",
       "      <td>0.034992</td>\n",
       "      <td>0.054978</td>\n",
       "      <td>0.064876</td>\n",
       "      <td>0.079277</td>\n",
       "      <td>0.229972</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.165830</td>\n",
       "      <td>0.039191</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.181601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0          0.913735          0.702062   0.929134   \n",
       "1      10001          0.016663          0.062762   0.204590   \n",
       "2      10003          0.010984          0.048353   0.099560   \n",
       "3      10004          0.009119          0.084854   0.197733   \n",
       "4      10005          0.065356          0.044514   0.066941   \n",
       "...      ...               ...               ...        ...   \n",
       "12659   9989          0.388578          0.070439   0.116099   \n",
       "12660   9991          0.149033          0.190217   0.181947   \n",
       "12661   9992          0.484737          0.049001   0.041990   \n",
       "12662   9994          0.970250          0.810563   0.909453   \n",
       "12663   9997          0.769208          0.034992   0.054978   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.831818               0.869098       0.280766   \n",
       "1                0.165778               0.168813       0.042430   \n",
       "2                0.117169               0.105025       0.142847   \n",
       "3                0.184202               0.235022       0.060112   \n",
       "4                0.067434               0.071771       0.051637   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.108199               0.073532       0.271521   \n",
       "12660            0.100179               0.237351       0.185677   \n",
       "12661            0.039121               0.045614       0.131843   \n",
       "12662            0.915812               0.896154       0.885021   \n",
       "12663            0.064876               0.079277       0.229972   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
       "0             0.013758    0.207912                0.337020     0.008519   \n",
       "1             0.101528    0.034587                0.095808     0.029069   \n",
       "2             0.036623    0.146478                0.084686     0.018265   \n",
       "3             0.045935    0.060969                0.035853     0.036303   \n",
       "4             0.011522    0.026642                0.034870     0.002034   \n",
       "...                ...         ...                     ...          ...   \n",
       "12659         0.006631    0.060059                0.032890     0.001873   \n",
       "12660         0.037585    0.108925                0.046437     0.013807   \n",
       "12661         0.009448    0.445695                0.034798     0.003469   \n",
       "12662         0.029745    0.888407                0.235861     0.008808   \n",
       "12663         0.008775    0.165830                0.039191     0.002687   \n",
       "\n",
       "       LABEL_Sepsis LABEL_RRate LABEL_ABPm LABEL_SpO2 LABEL_Heartrate  \n",
       "0          0.146291         NaN        NaN        NaN             NaN  \n",
       "1          0.059008         NaN        NaN        NaN             NaN  \n",
       "2          0.058987         NaN        NaN        NaN             NaN  \n",
       "3          0.051174         NaN        NaN        NaN             NaN  \n",
       "4          0.061610         NaN        NaN        NaN             NaN  \n",
       "...             ...         ...        ...        ...             ...  \n",
       "12659      0.154793         NaN        NaN        NaN             NaN  \n",
       "12660      0.098283         NaN        NaN        NaN             NaN  \n",
       "12661      0.088425         NaN        NaN        NaN             NaN  \n",
       "12662      0.362949         NaN        NaN        NaN             NaN  \n",
       "12663      0.181601         NaN        NaN        NaN             NaN  \n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "32\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "12.0\n",
      "12664\n",
      "32\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "X_train3_raw = feats_2_X3(df_train_feats,subtask3_feats)\n",
    "X_test3_raw = feats_2_X3(df_test_feats,subtask3_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "train score= 0.527167\n",
      "cv r2= 0.426749\n",
      "18.792951663607788\n",
      "2.325973016402366\n",
      "18.831102072773067\n",
      "2.3290849932542557\n",
      "i=1\n",
      "train score= 0.693247\n",
      "cv r2= 0.612962\n",
      "82.5212865187813\n",
      "10.127038903039843\n",
      "82.40153149064881\n",
      "10.021221154980784\n",
      "i=2\n",
      "train score= 0.496450\n",
      "cv r2= 0.373626\n",
      "96.94883057919724\n",
      "1.3199076363863382\n",
      "96.96889056075428\n",
      "1.2094044952548213\n",
      "i=3\n",
      "train score= 0.717406\n",
      "cv r2= 0.664685\n",
      "84.13505712185025\n",
      "12.008701476051527\n",
      "84.21556070492582\n",
      "11.927824076172005\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train3_raw))\n",
    "print(np.shape(X_test3_raw))\n",
    "\n",
    "X_train_imp,X_test = impute(X_train3_raw,X_test3_raw)\n",
    "print(np.shape(X_train_imp))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "#X_train,X_test = standardize(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))\n",
    "\n",
    "\n",
    "\n",
    "holdout_r2s = np.zeros(len(subtask3_labels))\n",
    "\n",
    "for i in range(0,len(subtask3_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask3_labels[i]].values\n",
    "    \n",
    "    if holdout:\n",
    "        #make hold out\n",
    "        X_train,X_hold,y_train,y_hold = train_test_split(\n",
    "            X_train_imp,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "        print(np.shape(X_train))\n",
    "        print(np.shape(X_hold))\n",
    "        print(np.sum(y_train))\n",
    "        print(np.sum(y_hold))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        X_train = X_train_imp\n",
    "\n",
    "    reg = fit_model3(X_train,y_train)\n",
    "    \n",
    "    score = reg.score(X_train,y_train)\n",
    "    cv_results = cross_validate(reg,X_train,y_train,cv=5,\n",
    "            scoring=[\"r2\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv r2= %f\"%np.mean(cv_results['test_r2']))\n",
    "    \n",
    "    y_train_pred=reg.predict(X_train)\n",
    "    print(np.mean(y_train_pred))\n",
    "    print(np.std(y_train_pred))\n",
    "    \n",
    "    y_test=reg.predict(X_test)\n",
    "    print(np.mean(y_test))\n",
    "    print(np.std(y_test))\n",
    "    \n",
    "    if holdout:\n",
    "        print(\"holdout metrics:\")\n",
    "        holdout_r2 = r2_score(y_hold,reg.predict(X_hold))\n",
    "        print(\"holdout roc auc= %f\"%holdout_r2)\n",
    "        holdout_r2s[i] = holdout_r2\n",
    "        #hp_y[j][0] = holdout_auc\n",
    "    \n",
    "    df_test_labels[subtask3_labels[i]] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.428913</td>\n",
       "      <td>0.076673</td>\n",
       "      <td>0.238620</td>\n",
       "      <td>0.182294</td>\n",
       "      <td>0.141938</td>\n",
       "      <td>0.246540</td>\n",
       "      <td>0.057717</td>\n",
       "      <td>0.333437</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>0.079751</td>\n",
       "      <td>0.146291</td>\n",
       "      <td>15.205757</td>\n",
       "      <td>80.969778</td>\n",
       "      <td>98.626486</td>\n",
       "      <td>86.785509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>0.114039</td>\n",
       "      <td>0.099378</td>\n",
       "      <td>0.410277</td>\n",
       "      <td>0.334511</td>\n",
       "      <td>0.313699</td>\n",
       "      <td>0.164247</td>\n",
       "      <td>0.323023</td>\n",
       "      <td>0.070962</td>\n",
       "      <td>0.027937</td>\n",
       "      <td>0.024150</td>\n",
       "      <td>0.059008</td>\n",
       "      <td>17.348948</td>\n",
       "      <td>91.940430</td>\n",
       "      <td>95.221059</td>\n",
       "      <td>93.932901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>0.248948</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.222940</td>\n",
       "      <td>0.203409</td>\n",
       "      <td>0.222066</td>\n",
       "      <td>0.188787</td>\n",
       "      <td>0.051112</td>\n",
       "      <td>0.169173</td>\n",
       "      <td>0.021639</td>\n",
       "      <td>0.048747</td>\n",
       "      <td>0.058987</td>\n",
       "      <td>16.678518</td>\n",
       "      <td>81.259779</td>\n",
       "      <td>98.310977</td>\n",
       "      <td>89.261237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>0.385536</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.227323</td>\n",
       "      <td>0.260416</td>\n",
       "      <td>0.273898</td>\n",
       "      <td>0.401304</td>\n",
       "      <td>0.039953</td>\n",
       "      <td>0.442299</td>\n",
       "      <td>0.046712</td>\n",
       "      <td>0.027535</td>\n",
       "      <td>0.051174</td>\n",
       "      <td>16.553489</td>\n",
       "      <td>74.720502</td>\n",
       "      <td>95.887770</td>\n",
       "      <td>90.575848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>0.188752</td>\n",
       "      <td>0.046005</td>\n",
       "      <td>0.184015</td>\n",
       "      <td>0.178601</td>\n",
       "      <td>0.174638</td>\n",
       "      <td>0.108432</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>0.181755</td>\n",
       "      <td>0.027422</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>0.061610</td>\n",
       "      <td>19.139172</td>\n",
       "      <td>74.713793</td>\n",
       "      <td>95.859805</td>\n",
       "      <td>61.619486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>9989</td>\n",
       "      <td>0.244173</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.315456</td>\n",
       "      <td>0.338447</td>\n",
       "      <td>0.307277</td>\n",
       "      <td>0.231082</td>\n",
       "      <td>0.054274</td>\n",
       "      <td>0.269226</td>\n",
       "      <td>0.029418</td>\n",
       "      <td>0.053303</td>\n",
       "      <td>0.154793</td>\n",
       "      <td>21.160239</td>\n",
       "      <td>75.744543</td>\n",
       "      <td>95.795182</td>\n",
       "      <td>100.891756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>9991</td>\n",
       "      <td>0.159485</td>\n",
       "      <td>0.058580</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.224857</td>\n",
       "      <td>0.188322</td>\n",
       "      <td>0.233116</td>\n",
       "      <td>0.130566</td>\n",
       "      <td>0.178194</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.335345</td>\n",
       "      <td>0.098283</td>\n",
       "      <td>18.939741</td>\n",
       "      <td>92.730427</td>\n",
       "      <td>98.789721</td>\n",
       "      <td>72.279865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>9992</td>\n",
       "      <td>0.683631</td>\n",
       "      <td>0.063020</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.285657</td>\n",
       "      <td>0.222303</td>\n",
       "      <td>0.517047</td>\n",
       "      <td>0.055911</td>\n",
       "      <td>0.722851</td>\n",
       "      <td>0.027618</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.088425</td>\n",
       "      <td>18.918820</td>\n",
       "      <td>65.309500</td>\n",
       "      <td>97.311729</td>\n",
       "      <td>82.521246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>0.116773</td>\n",
       "      <td>0.233963</td>\n",
       "      <td>0.199209</td>\n",
       "      <td>0.238279</td>\n",
       "      <td>0.172299</td>\n",
       "      <td>0.043926</td>\n",
       "      <td>0.254351</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.077728</td>\n",
       "      <td>0.362949</td>\n",
       "      <td>15.943131</td>\n",
       "      <td>88.679017</td>\n",
       "      <td>98.515389</td>\n",
       "      <td>98.804237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.292847</td>\n",
       "      <td>0.061520</td>\n",
       "      <td>0.202499</td>\n",
       "      <td>0.214380</td>\n",
       "      <td>0.181675</td>\n",
       "      <td>0.195225</td>\n",
       "      <td>0.045722</td>\n",
       "      <td>0.234520</td>\n",
       "      <td>0.026280</td>\n",
       "      <td>0.072842</td>\n",
       "      <td>0.181601</td>\n",
       "      <td>17.698959</td>\n",
       "      <td>75.839768</td>\n",
       "      <td>98.354706</td>\n",
       "      <td>86.511380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0          0.428913          0.076673   0.238620   \n",
       "1      10001          0.114039          0.099378   0.410277   \n",
       "2      10003          0.248948          0.066084   0.222940   \n",
       "3      10004          0.385536          0.055458   0.227323   \n",
       "4      10005          0.188752          0.046005   0.184015   \n",
       "...      ...               ...               ...        ...   \n",
       "12659   9989          0.244173          0.077922   0.315456   \n",
       "12660   9991          0.159485          0.058580   0.224200   \n",
       "12661   9992          0.683631          0.063020   0.234847   \n",
       "12662   9994          0.474915          0.116773   0.233963   \n",
       "12663   9997          0.292847          0.061520   0.202499   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.182294               0.141938       0.246540   \n",
       "1                0.334511               0.313699       0.164247   \n",
       "2                0.203409               0.222066       0.188787   \n",
       "3                0.260416               0.273898       0.401304   \n",
       "4                0.178601               0.174638       0.108432   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.338447               0.307277       0.231082   \n",
       "12660            0.224857               0.188322       0.233116   \n",
       "12661            0.285657               0.222303       0.517047   \n",
       "12662            0.199209               0.238279       0.172299   \n",
       "12663            0.214380               0.181675       0.195225   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
       "0             0.057717    0.333437                0.030696     0.079751   \n",
       "1             0.323023    0.070962                0.027937     0.024150   \n",
       "2             0.051112    0.169173                0.021639     0.048747   \n",
       "3             0.039953    0.442299                0.046712     0.027535   \n",
       "4             0.036697    0.181755                0.027422     0.020422   \n",
       "...                ...         ...                     ...          ...   \n",
       "12659         0.054274    0.269226                0.029418     0.053303   \n",
       "12660         0.130566    0.178194                0.034626     0.335345   \n",
       "12661         0.055911    0.722851                0.027618     0.022486   \n",
       "12662         0.043926    0.254351                0.090457     0.077728   \n",
       "12663         0.045722    0.234520                0.026280     0.072842   \n",
       "\n",
       "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
       "0          0.146291    15.205757   80.969778   98.626486        86.785509  \n",
       "1          0.059008    17.348948   91.940430   95.221059        93.932901  \n",
       "2          0.058987    16.678518   81.259779   98.310977        89.261237  \n",
       "3          0.051174    16.553489   74.720502   95.887770        90.575848  \n",
       "4          0.061610    19.139172   74.713793   95.859805        61.619486  \n",
       "...             ...          ...         ...         ...              ...  \n",
       "12659      0.154793    21.160239   75.744543   95.795182       100.891756  \n",
       "12660      0.098283    18.939741   92.730427   98.789721        72.279865  \n",
       "12661      0.088425    18.918820   65.309500   97.311729        82.521246  \n",
       "12662      0.362949    15.943131   88.679017   98.515389        98.804237  \n",
       "12663      0.181601    17.698959   75.839768   98.354706        86.511380  \n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_labels.to_csv('prediction_1904_8.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
