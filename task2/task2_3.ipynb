{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(\"data/sample.csv\")\n",
    "\n",
    "df_train_feats = pd.read_csv(\"data/train_features.csv\")\n",
    "df_train_labels = pd.read_csv(\"data/train_labels.csv\")\n",
    "df_test_feats = pd.read_csv(\"data/test_features.csv\")\n",
    "\n",
    "df_labels_cols = list(df_train_labels)\n",
    "\n",
    "active_feats = [\n",
    " 'Age',\n",
    " 'EtCO2',\n",
    " 'PTT',\n",
    " 'BUN',\n",
    " 'Lactate',\n",
    " 'Temp',\n",
    " 'Hgb',\n",
    " 'HCO3',\n",
    " 'BaseExcess',\n",
    " 'RRate',\n",
    " 'Fibrinogen',\n",
    " 'Phosphate',\n",
    " 'WBC',\n",
    " 'Creatinine',\n",
    " 'PaCO2',\n",
    " 'AST',\n",
    " 'FiO2',\n",
    " 'Platelets',\n",
    " 'SaO2',\n",
    " 'Glucose',\n",
    " 'ABPm',\n",
    " 'Magnesium',\n",
    " 'Potassium',\n",
    " 'ABPd',\n",
    " 'Calcium',\n",
    " 'Alkalinephos',\n",
    " 'SpO2',\n",
    " 'Bilirubin_direct',\n",
    " 'Chloride',\n",
    " 'Hct',\n",
    " 'Heartrate',\n",
    " 'Bilirubin_total',\n",
    " 'TroponinI',\n",
    " 'ABPs',\n",
    " 'pH']\n",
    "\n",
    "subtask3_feats = [\n",
    " 'RRate',\n",
    " 'ABPm',\n",
    " 'SpO2',\n",
    " 'Heartrate']\n",
    "\n",
    "subtask1_labels = [\n",
    " 'LABEL_BaseExcess',\n",
    " 'LABEL_Fibrinogen',\n",
    " 'LABEL_AST',\n",
    " 'LABEL_Alkalinephos',\n",
    " 'LABEL_Bilirubin_total',\n",
    " 'LABEL_Lactate',\n",
    " 'LABEL_TroponinI',\n",
    " 'LABEL_SaO2',\n",
    " 'LABEL_Bilirubin_direct',\n",
    " 'LABEL_EtCO2'\n",
    "        ]\n",
    "\n",
    "subtask2_labels = [\n",
    " 'LABEL_Sepsis'\n",
    " ]\n",
    "\n",
    "subtask3_labels = [\n",
    " 'LABEL_RRate',\n",
    " 'LABEL_ABPm',\n",
    " 'LABEL_SpO2',\n",
    " 'LABEL_Heartrate'\n",
    "        ]\n",
    "\n",
    "\n",
    "def feats_2_X1(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    n_derived_feats = 4\n",
    "\n",
    "    #pids = np.unique(df_train_feats['pid'].values)\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "    #df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            patient_feat = patient_feat[~np.isnan(patient_feat)]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "                X[i][j0+1] = np.std(patient_feat)\n",
    "                X[i][j0+2] = np.min(patient_feat)\n",
    "                X[i][j0+3] = np.max(patient_feat)\n",
    "        #print(patient_df)\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n",
    "\n",
    "def feats_2_X2(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    #n_derived_feats = 4\n",
    "    n_derived_feats = 1\n",
    "\n",
    "    #pids = np.unique(df_train_feats['pid'].values)\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "    #df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            patient_feat = patient_feat[~np.isnan(patient_feat)]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "                #X[i][j0+1] = np.std(patient_feat)\n",
    "                #X[i][j0+2] = np.min(patient_feat)\n",
    "                #X[i][j0+3] = np.max(patient_feat)\n",
    "        #print(patient_df)\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n",
    "\n",
    "#print(df_train_feats.describe())\n",
    "#a = df_train_feats[df_train_feats['Time']>24]\n",
    "#print(a.shape)\n",
    "\n",
    "#print(np.mean(a[~np.isnan(a)]))\n",
    "\n",
    "def feats_2_X3(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    n_derived_feats = 4\n",
    "\n",
    "    #pids = np.unique(df_train_feats['pid'].values)\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "    #df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            nan_mask = np.isnan(patient_feat)\n",
    "            times = (patient_df['Time'].values)[~nan_mask]\n",
    "            patient_feat = patient_feat[~nan_mask]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "            if len(patient_feat)>1:\n",
    "                X[i][j0+1] = patient_feat[-1]\n",
    "                X[i][j0+2] = (patient_feat[-1]-patient_feat[0])/(times[-1]-times[0])\n",
    "            if len(patient_feat)>2:\n",
    "                grad_end = (patient_feat[-1]-patient_feat[-2])/(times[-1]-times[-2])\n",
    "                X[i][j0+3] = grad_end\n",
    "        #print(patient_df)\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return(X_train,X_test)\n",
    "    \n",
    "\n",
    "def impute(X_train,X_test):\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean.fit(X_train)\n",
    "    \n",
    "    X_train = imp_mean.transform(X_train)\n",
    "    X_test = imp_mean.transform(X_test)\n",
    "\n",
    "    #print(X_train_imp)\n",
    "    return(X_train,X_test)\n",
    "\n",
    "def forest_fi(X_train,y_train,X_test):\n",
    "\n",
    "    forest = ExtraTreesClassifier(n_estimators=20,\n",
    "                                  random_state=0)\n",
    "\n",
    "    forest.fit(X_train,y_train)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    X_train = X_train[:,indices[:50]]\n",
    "    X_test = X_test[:,indices[:50]]\n",
    "    return(X_train,X_test)\n",
    "\n",
    "\n",
    "def nystroem(X_train,X_test):\n",
    "    gamma=1.0\n",
    "    n_components=100\n",
    "    print(\"nystroem gamma=%f\"%(gamma))\n",
    "    print(\"nystroem q=%d\"%(n_components))\n",
    "    feature_map_nystroem = Nystroem(gamma=gamma,\n",
    "                                    random_state=42,\n",
    "                                    n_components=n_components)\n",
    "    feature_map_nystroem.fit(X_train)\n",
    "    Q_train = feature_map_nystroem.transform(X_train)\n",
    "    sqrt_k_inv_train = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "    B_train = np.dot(Q_train,sqrt_k_inv_train)\n",
    "    K_train = np.dot(B_train,np.transpose(B_train))\n",
    "\n",
    "    Q_test = feature_map_nystroem.transform(X_test)\n",
    "    sqrt_k_inv_test = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "    B_test = np.dot(Q_test,sqrt_k_inv_test)\n",
    "    K_test = np.dot(B_test,np.transpose(B_train))\n",
    "    return(K_train,K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "def fit_model1_1(X_train,y_train):\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "                                     max_depth=3, random_state=42).fit(X_train,y_train)\n",
    "    #clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n",
    "    #                                 max_depth=5, random_state=42).fit(X_train_imp,y_train)\n",
    "\n",
    "    return(clf)\n",
    "\n",
    "def fit_model1_2(X_train,y_train):\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=10,\n",
    "                  class_weight=\"balanced\",\n",
    "                  decision_function_shape='ovo').fit(X_train,y_train)\n",
    "    return(clf)\n",
    "\n",
    "def fit_model1(X_train,y_train,clf_init,sw_dict={}):\n",
    "    n = np.shape(y_train)[0]\n",
    "    w0 = n/(n-np.sum(y_train))\n",
    "    w1 = n/np.sum(y_train)\n",
    "    \n",
    "    sample_weight = np.zeros(len(y_train))\n",
    "    if not(sw_dict):\n",
    "        print(\"using default sample weights\")\n",
    "        sample_weight[y_train == 0] = w0\n",
    "        sample_weight[y_train == 1] = w1\n",
    "    else:\n",
    "        print(\"using custom sample weights\")\n",
    "        sample_weight[y_train == 0] = sw_dict[0]\n",
    "        sample_weight[y_train == 1] = sw_dict[1]\n",
    "    \n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = clf_init\n",
    "    #clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "    #                                 max_depth=3, random_state=42)\n",
    "    clf.fit(X_train,y_train,sample_weight=sample_weight)\n",
    "    #clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n",
    "    #                                 max_depth=5, random_state=42).fit(X_train_imp,y_train)\n",
    "\n",
    "    return(clf)\n",
    "\n",
    "\n",
    "\n",
    "def fit_model2(K_train,y_train):\n",
    "    C = 1.0\n",
    "    class_weight={1:17.5}\n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=C,\n",
    "                  kernel=\"precomputed\",\n",
    "                  class_weight=\"balanced\",\n",
    "                  decision_function_shape='ovo',\n",
    "                  verbose=True).fit(K_train,y_train)\n",
    "    print(\"C: %f\"%C)\n",
    "    print(\"Computed class weight: %s\"%(str(clf.class_weight_)))\n",
    "    return(clf)\n",
    "\n",
    "\n",
    "def fit_model3(X_train,y_train):\n",
    "    #print(np.shape(y_train))\n",
    "    #print(np.sum(y_train))\n",
    "    reg = GradientBoostingRegressor(random_state=42).fit(X_train,y_train)\n",
    "\n",
    "    return(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pids = pd.unique(df_test_feats['pid'])\n",
    "\n",
    "df_test_labels = pd.DataFrame(columns=df_labels_cols)\n",
    "df_test_labels['pid'] = test_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "12.0\n",
      "12664\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "12.0\n",
      "(18995, 140)\n",
      "(12664, 140)\n",
      "(18995, 140)\n",
      "(12664, 140)\n"
     ]
    }
   ],
   "source": [
    "X_train1_raw = feats_2_X1(df_train_feats,active_feats)\n",
    "X_test1_raw = feats_2_X1(df_test_feats,active_feats)\n",
    "\n",
    "print(np.shape(X_train1_raw))\n",
    "print(np.shape(X_test1_raw))\n",
    "X_train,X_test = impute(X_train1_raw,X_test1_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "18995\n",
      "18995\n",
      "18995\n",
      "1322\n",
      "1322\n",
      "1322\n",
      "1322\n",
      "7816\n",
      "7816\n",
      "7816\n",
      "7816\n",
      "13966\n",
      "13966\n",
      "13966\n",
      "13966\n",
      "4872\n",
      "4872\n",
      "4872\n",
      "4872\n",
      "18552\n",
      "18552\n",
      "18552\n",
      "18552\n",
      "13975\n",
      "13975\n",
      "13975\n",
      "13975\n",
      "7837\n",
      "7837\n",
      "7837\n",
      "7837\n",
      "5697\n",
      "5697\n",
      "5697\n",
      "5697\n",
      "18843\n",
      "18843\n",
      "18843\n",
      "18843\n",
      "1714\n",
      "1714\n",
      "1714\n",
      "1714\n",
      "9329\n",
      "9329\n",
      "9329\n",
      "9329\n",
      "13341\n",
      "13341\n",
      "13341\n",
      "13341\n",
      "13727\n",
      "13727\n",
      "13727\n",
      "13727\n",
      "7772\n",
      "7772\n",
      "7772\n",
      "7772\n",
      "4796\n",
      "4796\n",
      "4796\n",
      "4796\n",
      "7678\n",
      "7678\n",
      "7678\n",
      "7678\n",
      "13541\n",
      "13541\n",
      "13541\n",
      "13541\n",
      "5055\n",
      "5055\n",
      "5055\n",
      "5055\n",
      "16330\n",
      "16330\n",
      "16330\n",
      "16330\n",
      "18888\n",
      "18888\n",
      "18888\n",
      "18888\n",
      "12326\n",
      "12326\n",
      "12326\n",
      "12326\n",
      "14997\n",
      "14997\n",
      "14997\n",
      "14997\n",
      "15113\n",
      "15113\n",
      "15113\n",
      "15113\n",
      "12266\n",
      "12266\n",
      "12266\n",
      "12266\n",
      "4744\n",
      "4744\n",
      "4744\n",
      "4744\n",
      "18973\n",
      "18973\n",
      "18973\n",
      "18973\n",
      "623\n",
      "623\n",
      "623\n",
      "623\n",
      "8351\n",
      "8351\n",
      "8351\n",
      "8351\n",
      "14639\n",
      "14639\n",
      "14639\n",
      "14639\n",
      "18988\n",
      "18988\n",
      "18988\n",
      "18988\n",
      "4711\n",
      "4711\n",
      "4711\n",
      "4711\n",
      "2792\n",
      "2792\n",
      "2792\n",
      "2792\n",
      "18597\n",
      "18597\n",
      "18597\n",
      "18597\n",
      "7994\n",
      "7994\n",
      "7994\n",
      "7994\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(np.shape(X_train1_raw)[1]):\n",
    "    a = np.count_nonzero(~np.isnan(X_train1_raw[:,i]))\n",
    "    print(a)\n",
    "    if a>0:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "5096.0\n",
      "train score= 0.919137\n",
      "cv roc auc= 0.928645\n",
      "cv acc= 0.876599\n",
      "cv rec= 0.756671\n",
      "cv prec= 0.777560\n",
      "y train:\n",
      "n pos 5096.000000\n",
      "frac pos 0.268281\n",
      "y train pred:\n",
      "n pos 4894.000000\n",
      "frac pos 0.257647\n",
      "prob sum 0 13901.346882\n",
      "prob sum 1 5093.653118\n",
      "y test:\n",
      "prob sum 0 9262.081306\n",
      "prob sum 1 3401.918694\n",
      "\n",
      "\n",
      "\n",
      "i=1\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1400.0\n",
      "train score= 0.956568\n",
      "cv roc auc= 0.799989\n",
      "cv acc= 0.936510\n",
      "cv rec= 0.252143\n",
      "cv prec= 0.694805\n",
      "y train:\n",
      "n pos 1400.000000\n",
      "frac pos 0.073704\n",
      "y train pred:\n",
      "n pos 623.000000\n",
      "frac pos 0.032798\n",
      "prob sum 0 17593.553715\n",
      "prob sum 1 1401.446285\n",
      "y test:\n",
      "prob sum 0 11808.218180\n",
      "prob sum 1 855.781820\n",
      "\n",
      "\n",
      "\n",
      "i=2\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4554.0\n",
      "train score= 0.839431\n",
      "cv roc auc= 0.746393\n",
      "cv acc= 0.791261\n",
      "cv rec= 0.240665\n",
      "cv prec= 0.685834\n",
      "y train:\n",
      "n pos 4554.000000\n",
      "frac pos 0.239747\n",
      "y train pred:\n",
      "n pos 1890.000000\n",
      "frac pos 0.099500\n",
      "prob sum 0 14444.025451\n",
      "prob sum 1 4550.974549\n",
      "y test:\n",
      "prob sum 0 9656.241647\n",
      "prob sum 1 3007.758353\n",
      "\n",
      "\n",
      "\n",
      "i=3\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4487.0\n",
      "train score= 0.855120\n",
      "cv roc auc= 0.748770\n",
      "cv acc= 0.793683\n",
      "cv rec= 0.239136\n",
      "cv prec= 0.681177\n",
      "y train:\n",
      "n pos 4487.000000\n",
      "frac pos 0.236220\n",
      "y train pred:\n",
      "n pos 2111.000000\n",
      "frac pos 0.111135\n",
      "prob sum 0 14508.359401\n",
      "prob sum 1 4486.640599\n",
      "y test:\n",
      "prob sum 0 9713.313480\n",
      "prob sum 1 2950.686520\n",
      "\n",
      "\n",
      "\n",
      "i=4\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4570.0\n",
      "train score= 0.823375\n",
      "cv roc auc= 0.749772\n",
      "cv acc= 0.791419\n",
      "cv rec= 0.247265\n",
      "cv prec= 0.685959\n",
      "y train:\n",
      "n pos 4570.000000\n",
      "frac pos 0.240590\n",
      "y train pred:\n",
      "n pos 1725.000000\n",
      "frac pos 0.090813\n",
      "prob sum 0 14421.246079\n",
      "prob sum 1 4573.753921\n",
      "y test:\n",
      "prob sum 0 9634.475671\n",
      "prob sum 1 3029.524329\n",
      "\n",
      "\n",
      "\n",
      "i=5\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "3803.0\n",
      "train score= 0.869597\n",
      "cv roc auc= 0.806809\n",
      "cv acc= 0.836747\n",
      "cv rec= 0.343940\n",
      "cv prec= 0.683413\n",
      "y train:\n",
      "n pos 3803.000000\n",
      "frac pos 0.200211\n",
      "y train pred:\n",
      "n pos 1992.000000\n",
      "frac pos 0.104870\n",
      "prob sum 0 15196.178469\n",
      "prob sum 1 3798.821531\n",
      "y test:\n",
      "prob sum 0 10152.507181\n",
      "prob sum 1 2511.492819\n",
      "\n",
      "\n",
      "\n",
      "i=6\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1895.0\n",
      "train score= 0.946618\n",
      "cv roc auc= 0.895883\n",
      "cv acc= 0.925349\n",
      "cv rec= 0.452770\n",
      "cv prec= 0.692305\n",
      "y train:\n",
      "n pos 1895.000000\n",
      "frac pos 0.099763\n",
      "y train pred:\n",
      "n pos 1295.000000\n",
      "frac pos 0.068176\n",
      "prob sum 0 17116.695979\n",
      "prob sum 1 1878.304021\n",
      "y test:\n",
      "prob sum 0 11440.235315\n",
      "prob sum 1 1223.764685\n",
      "\n",
      "\n",
      "\n",
      "i=7\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4439.0\n",
      "train score= 0.878968\n",
      "cv roc auc= 0.828870\n",
      "cv acc= 0.831535\n",
      "cv rec= 0.477809\n",
      "cv prec= 0.706972\n",
      "y train:\n",
      "n pos 4439.000000\n",
      "frac pos 0.233693\n",
      "y train pred:\n",
      "n pos 3110.000000\n",
      "frac pos 0.163727\n",
      "prob sum 0 14550.575846\n",
      "prob sum 1 4444.424154\n",
      "y test:\n",
      "prob sum 0 9708.973504\n",
      "prob sum 1 2955.026496\n",
      "\n",
      "\n",
      "\n",
      "i=8\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "644.0\n",
      "train score= 0.973361\n",
      "cv roc auc= 0.758276\n",
      "cv acc= 0.967202\n",
      "cv rec= 0.090044\n",
      "cv prec= 0.604787\n",
      "y train:\n",
      "n pos 644.000000\n",
      "frac pos 0.033904\n",
      "y train pred:\n",
      "n pos 162.000000\n",
      "frac pos 0.008529\n",
      "prob sum 0 18340.588541\n",
      "prob sum 1 654.411459\n",
      "y test:\n",
      "prob sum 0 12258.290858\n",
      "prob sum 1 405.709142\n",
      "\n",
      "\n",
      "\n",
      "i=9\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1254.0\n",
      "train score= 0.980521\n",
      "cv roc auc= 0.932952\n",
      "cv acc= 0.962411\n",
      "cv rec= 0.574209\n",
      "cv prec= 0.800177\n",
      "y train:\n",
      "n pos 1254.000000\n",
      "frac pos 0.066017\n",
      "y train pred:\n",
      "n pos 990.000000\n",
      "frac pos 0.052119\n",
      "prob sum 0 17743.116015\n",
      "prob sum 1 1251.883985\n",
      "y test:\n",
      "prob sum 0 11874.409512\n",
      "prob sum 1 789.590488\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(subtask1_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask1_labels[i]].values\n",
    "    n = float(np.shape(y_train)[0])\n",
    "    \n",
    "    #sample weight\n",
    "    use_custom_sw = False\n",
    "    if use_custom_sw:\n",
    "        #sample weight\n",
    "        n = float(np.shape(y_train)[0])\n",
    "        w1_boost = 10.0\n",
    "\n",
    "        w0 = n/(n-np.sum(y_train))\n",
    "        w1 = (n/np.sum(y_train))*w1_boost\n",
    "        geo_mean = np.sqrt(w0*w1)\n",
    "        w0 /= geo_mean\n",
    "        w1 /= geo_mean\n",
    "    else:\n",
    "        w0=1.0\n",
    "        w1=1.0\n",
    "    \n",
    "    clf_init = HistGradientBoostingClassifier(random_state=42)\n",
    "    clf = fit_model1(X_train,y_train,clf_init,sw_dict={0:w0,1:w1})\n",
    "    score = clf.score(X_train,y_train)\n",
    "    \n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "    \n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "    \n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "    \n",
    "    p_train=clf.predict_proba(X_train)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "    \n",
    "    #print(p_train)\n",
    "    \n",
    "    print(\"y test:\")\n",
    "    p_test=clf.predict_proba(X_test)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #y_test[:,1+i] = p_test[:,1]\n",
    "    \n",
    "    df_test_labels[subtask1_labels[i]] = p_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=9\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1254.0\n",
      "train score= 0.983627\n",
      "cv roc auc= 0.934273\n",
      "cv acc= 0.963359\n",
      "cv rec= 0.580586\n",
      "cv prec= 0.810752\n",
      "y train:\n",
      "n pos 1254.000000\n",
      "frac pos 0.066017\n",
      "y train pred:\n",
      "n pos 993.000000\n",
      "frac pos 0.052277\n",
      "prob sum 0 17747.732897\n",
      "prob sum 1 1247.267103\n",
      "y test:\n",
      "prob sum 0 11882.576354\n",
      "prob sum 1 781.423646\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#hp_x = np.array([1,2,3,4,5,6,7,8,16])\n",
    "#hp_x = np.array([1,2,3,4,5,6,7,8,16])\n",
    "hp_x = [0]\n",
    "metrics = ['test_roc_auc',\n",
    "           'test_accuracy',\n",
    "           'test_recall',\n",
    "           'test_precision'\n",
    "          ]\n",
    "hp_y = np.zeros((len(hp_x),len(metrics)))\n",
    "\n",
    "i=9\n",
    "for j in range(0,len(hp_x)):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask1_labels[i]].values\n",
    "\n",
    "    use_custom_sw = False\n",
    "    \n",
    "    if use_custom_sw:\n",
    "        #sample weight\n",
    "        n = float(np.shape(y_train)[0])\n",
    "        w1_boost = 10.0\n",
    "\n",
    "        w0 = n/(n-np.sum(y_train))\n",
    "        w1 = (n/np.sum(y_train))*w1_boost\n",
    "        geo_mean = np.sqrt(w0*w1)\n",
    "        w0 /= geo_mean\n",
    "        w1 /= geo_mean\n",
    "    else:\n",
    "        w0=1.0\n",
    "        w1=1.0\n",
    "    \n",
    "    \n",
    "    #clf_init = GradientBoostingClassifier(n_estimators=hp_x[j], learning_rate=1.0, \n",
    "    #                                 max_depth=3, random_state=42)\n",
    "    clf_init = HistGradientBoostingClassifier(random_state=42)\n",
    "    clf = fit_model1(X_train,y_train,clf_init,sw_dict={0:w0,1:w1})\n",
    "    \n",
    "    score = clf.score(X_train,y_train)\n",
    "    \n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    for k in range(0,len(metrics)):\n",
    "        hp_y[j][k] = np.mean(cv_results[metrics[k]])\n",
    "        \n",
    "    \n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "    \n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "    \n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "    \n",
    "    p_train=clf.predict_proba(X_train)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "    \n",
    "    #print(p_train)\n",
    "    \n",
    "    print(\"y test:\")\n",
    "    p_test=clf.predict_proba(X_test)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #y_test[:,1+i] = p_test[:,1]\n",
    "    \n",
    "    df_test_labels[subtask1_labels[i]] = p_test[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "for k in range(0,len(metrics)):\n",
    "    ax1.plot(np.log10(hp_x),hp_y[:,k],label=metrics[k])\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.922230</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.876636</td>\n",
       "      <td>0.942709</td>\n",
       "      <td>0.818287</td>\n",
       "      <td>0.413834</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>0.327305</td>\n",
       "      <td>0.073881</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>0.029884</td>\n",
       "      <td>0.024146</td>\n",
       "      <td>0.339080</td>\n",
       "      <td>0.279901</td>\n",
       "      <td>0.251734</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>0.087575</td>\n",
       "      <td>0.050337</td>\n",
       "      <td>0.053138</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>0.015405</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.161865</td>\n",
       "      <td>0.170520</td>\n",
       "      <td>0.227277</td>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.216313</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.316502</td>\n",
       "      <td>0.334136</td>\n",
       "      <td>0.290121</td>\n",
       "      <td>0.071723</td>\n",
       "      <td>0.041944</td>\n",
       "      <td>0.087275</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>0.100838</td>\n",
       "      <td>0.041141</td>\n",
       "      <td>0.153471</td>\n",
       "      <td>0.117237</td>\n",
       "      <td>0.125790</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>0.006810</td>\n",
       "      <td>0.045313</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>9989</td>\n",
       "      <td>0.493078</td>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.195267</td>\n",
       "      <td>0.134382</td>\n",
       "      <td>0.133590</td>\n",
       "      <td>0.344067</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.147624</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>9991</td>\n",
       "      <td>0.240444</td>\n",
       "      <td>0.112526</td>\n",
       "      <td>0.143815</td>\n",
       "      <td>0.165256</td>\n",
       "      <td>0.186950</td>\n",
       "      <td>0.257387</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.221607</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>9992</td>\n",
       "      <td>0.699523</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.046210</td>\n",
       "      <td>0.054838</td>\n",
       "      <td>0.172692</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.684390</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.990294</td>\n",
       "      <td>0.592081</td>\n",
       "      <td>0.848351</td>\n",
       "      <td>0.924976</td>\n",
       "      <td>0.927498</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>0.030560</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>0.269929</td>\n",
       "      <td>0.006093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.887361</td>\n",
       "      <td>0.018318</td>\n",
       "      <td>0.139694</td>\n",
       "      <td>0.099819</td>\n",
       "      <td>0.106114</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.202355</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0          0.922230          0.564706   0.876636   \n",
       "1      10001          0.029884          0.024146   0.339080   \n",
       "2      10003          0.015405          0.024254   0.212800   \n",
       "3      10004          0.019618          0.087000   0.316502   \n",
       "4      10005          0.100838          0.041141   0.153471   \n",
       "...      ...               ...               ...        ...   \n",
       "12659   9989          0.493078          0.021283   0.195267   \n",
       "12660   9991          0.240444          0.112526   0.143815   \n",
       "12661   9992          0.699523          0.027226   0.079635   \n",
       "12662   9994          0.990294          0.592081   0.848351   \n",
       "12663   9997          0.887361          0.018318   0.139694   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.942709               0.818287       0.413834   \n",
       "1                0.279901               0.251734       0.052736   \n",
       "2                0.161865               0.170520       0.227277   \n",
       "3                0.334136               0.290121       0.071723   \n",
       "4                0.117237               0.125790       0.085006   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.134382               0.133590       0.344067   \n",
       "12660            0.165256               0.186950       0.257387   \n",
       "12661            0.046210               0.054838       0.172692   \n",
       "12662            0.924976               0.927498       0.877358   \n",
       "12663            0.099819               0.106114       0.370453   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
       "0             0.004664    0.327305                0.073881     0.006220   \n",
       "1             0.087575    0.050337                0.053138     0.014200   \n",
       "2             0.025159    0.216313                0.018399     0.010059   \n",
       "3             0.041944    0.087275                0.010719     0.022491   \n",
       "4             0.006810    0.045313                0.010978     0.001147   \n",
       "...                ...         ...                     ...          ...   \n",
       "12659         0.003779    0.147624                0.006330     0.001229   \n",
       "12660         0.020217    0.221607                0.012400     0.010231   \n",
       "12661         0.006970    0.684390                0.010117     0.001838   \n",
       "12662         0.030560    0.865731                0.269929     0.006093   \n",
       "12663         0.006248    0.202355                0.010231     0.001288   \n",
       "\n",
       "      LABEL_Sepsis LABEL_RRate LABEL_ABPm LABEL_SpO2 LABEL_Heartrate  \n",
       "0              NaN         NaN        NaN        NaN             NaN  \n",
       "1              NaN         NaN        NaN        NaN             NaN  \n",
       "2              NaN         NaN        NaN        NaN             NaN  \n",
       "3              NaN         NaN        NaN        NaN             NaN  \n",
       "4              NaN         NaN        NaN        NaN             NaN  \n",
       "...            ...         ...        ...        ...             ...  \n",
       "12659          NaN         NaN        NaN        NaN             NaN  \n",
       "12660          NaN         NaN        NaN        NaN             NaN  \n",
       "12661          NaN         NaN        NaN        NaN             NaN  \n",
       "12662          NaN         NaN        NaN        NaN             NaN  \n",
       "12663          NaN         NaN        NaN        NaN             NaN  \n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "12.0\n",
      "12664\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "X_train2_raw = feats_2_X2(df_train_feats,active_feats)\n",
    "X_test2_raw = feats_2_X2(df_test_feats,active_feats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 140)\n",
      "(12664, 140)\n",
      "(18995, 35)\n",
      "(12664, 35)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train2_raw))\n",
    "print(np.shape(X_test2_raw))\n",
    "\n",
    "X_train,X_test = impute(X_train2_raw,X_test2_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "#X_train,X_test = standardize(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))\n",
    "\n",
    "\n",
    "#X_train,X_test = nystroem(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 36 (0.013107)\n",
      "2. feature 38 (0.012164)\n",
      "3. feature 37 (0.011283)\n",
      "4. feature 20 (0.011242)\n",
      "5. feature 82 (0.010902)\n",
      "6. feature 39 (0.010877)\n",
      "7. feature 81 (0.010720)\n",
      "8. feature 122 (0.010658)\n",
      "9. feature 105 (0.010565)\n",
      "10. feature 104 (0.010515)\n",
      "11. feature 21 (0.010502)\n",
      "12. feature 120 (0.010440)\n",
      "13. feature 133 (0.010368)\n",
      "14. feature 121 (0.010107)\n",
      "15. feature 83 (0.010041)\n",
      "16. feature 135 (0.010013)\n",
      "17. feature 93 (0.009698)\n",
      "18. feature 2 (0.009675)\n",
      "19. feature 94 (0.009641)\n",
      "20. feature 106 (0.009594)\n",
      "21. feature 78 (0.009561)\n",
      "22. feature 80 (0.009519)\n",
      "23. feature 23 (0.009426)\n",
      "24. feature 76 (0.009424)\n",
      "25. feature 8 (0.009352)\n",
      "26. feature 95 (0.009322)\n",
      "27. feature 132 (0.009313)\n",
      "28. feature 134 (0.009184)\n",
      "29. feature 123 (0.009163)\n",
      "30. feature 0 (0.009141)\n",
      "31. feature 92 (0.009024)\n",
      "32. feature 3 (0.008970)\n",
      "33. feature 22 (0.008880)\n",
      "34. feature 91 (0.008879)\n",
      "35. feature 88 (0.008861)\n",
      "36. feature 79 (0.008787)\n",
      "37. feature 90 (0.008767)\n",
      "38. feature 67 (0.008571)\n",
      "39. feature 96 (0.008567)\n",
      "40. feature 14 (0.008485)\n",
      "41. feature 86 (0.008446)\n",
      "42. feature 87 (0.008437)\n",
      "43. feature 10 (0.008399)\n",
      "44. feature 70 (0.008327)\n",
      "45. feature 116 (0.008325)\n",
      "46. feature 77 (0.008318)\n",
      "47. feature 71 (0.008243)\n",
      "48. feature 48 (0.008200)\n",
      "49. feature 107 (0.008145)\n",
      "50. feature 98 (0.008039)\n",
      "51. feature 66 (0.007963)\n",
      "52. feature 68 (0.007948)\n",
      "53. feature 24 (0.007860)\n",
      "54. feature 84 (0.007842)\n",
      "55. feature 118 (0.007832)\n",
      "56. feature 119 (0.007818)\n",
      "57. feature 112 (0.007785)\n",
      "58. feature 50 (0.007769)\n",
      "59. feature 99 (0.007756)\n",
      "60. feature 44 (0.007683)\n",
      "61. feature 64 (0.007606)\n",
      "62. feature 11 (0.007494)\n",
      "63. feature 12 (0.007428)\n",
      "64. feature 31 (0.007405)\n",
      "65. feature 26 (0.007321)\n",
      "66. feature 139 (0.007277)\n",
      "67. feature 19 (0.007261)\n",
      "68. feature 52 (0.007207)\n",
      "69. feature 117 (0.007147)\n",
      "70. feature 15 (0.007145)\n",
      "71. feature 9 (0.007144)\n",
      "72. feature 51 (0.007079)\n",
      "73. feature 136 (0.007043)\n",
      "74. feature 57 (0.007011)\n",
      "75. feature 46 (0.007011)\n",
      "76. feature 27 (0.006934)\n",
      "77. feature 16 (0.006878)\n",
      "78. feature 54 (0.006852)\n",
      "79. feature 47 (0.006812)\n",
      "80. feature 25 (0.006795)\n",
      "81. feature 30 (0.006751)\n",
      "82. feature 17 (0.006727)\n",
      "83. feature 115 (0.006681)\n",
      "84. feature 114 (0.006616)\n",
      "85. feature 49 (0.006594)\n",
      "86. feature 58 (0.006528)\n",
      "87. feature 97 (0.006508)\n",
      "88. feature 69 (0.006476)\n",
      "89. feature 85 (0.006384)\n",
      "90. feature 28 (0.006344)\n",
      "91. feature 55 (0.006341)\n",
      "92. feature 18 (0.006278)\n",
      "93. feature 65 (0.006257)\n",
      "94. feature 138 (0.006253)\n",
      "95. feature 100 (0.006217)\n",
      "96. feature 35 (0.006193)\n",
      "97. feature 45 (0.006105)\n",
      "98. feature 34 (0.006008)\n",
      "99. feature 89 (0.006005)\n",
      "100. feature 137 (0.005851)\n",
      "101. feature 56 (0.005760)\n",
      "102. feature 73 (0.005758)\n",
      "103. feature 13 (0.005740)\n",
      "104. feature 113 (0.005668)\n",
      "105. feature 72 (0.005652)\n",
      "106. feature 53 (0.005625)\n",
      "107. feature 59 (0.005572)\n",
      "108. feature 63 (0.005547)\n",
      "109. feature 32 (0.005374)\n",
      "110. feature 127 (0.005186)\n",
      "111. feature 103 (0.005145)\n",
      "112. feature 33 (0.005086)\n",
      "113. feature 60 (0.005047)\n",
      "114. feature 29 (0.005012)\n",
      "115. feature 75 (0.004888)\n",
      "116. feature 126 (0.004857)\n",
      "117. feature 124 (0.004839)\n",
      "118. feature 62 (0.004762)\n",
      "119. feature 4 (0.004650)\n",
      "120. feature 6 (0.004600)\n",
      "121. feature 130 (0.004516)\n",
      "122. feature 102 (0.004426)\n",
      "123. feature 42 (0.004405)\n",
      "124. feature 5 (0.004378)\n",
      "125. feature 125 (0.004317)\n",
      "126. feature 74 (0.004305)\n",
      "127. feature 41 (0.004295)\n",
      "128. feature 7 (0.004142)\n",
      "129. feature 101 (0.003933)\n",
      "130. feature 43 (0.003825)\n",
      "131. feature 129 (0.003803)\n",
      "132. feature 131 (0.003674)\n",
      "133. feature 61 (0.003659)\n",
      "134. feature 40 (0.003646)\n",
      "135. feature 128 (0.003317)\n",
      "136. feature 108 (0.002985)\n",
      "137. feature 110 (0.002872)\n",
      "138. feature 111 (0.002344)\n",
      "139. feature 109 (0.001979)\n",
      "140. feature 1 (0.000000)\n",
      "(18995, 50)\n",
      "(12664, 50)\n"
     ]
    }
   ],
   "source": [
    "y_train=df_train_labels[subtask2_labels[0]].values\n",
    "\n",
    "X_train,X_test = forest_fi(X_train,y_train,X_test)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1088.0\n",
      "[LibSVM]C: 0.020000\n",
      "Computed class weight: [ 1.  16.5]\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]train score= 0.903238\n",
      "cv roc auc= 0.614251\n",
      "cv roc acc= 0.889866\n",
      "cv roc rec= 0.158990\n",
      "cv roc prec= 0.128391\n",
      "[0.49164373 0.32760594 0.45562297 ... 0.39078787 0.38425937 0.3171537 ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.26747633 0.35315686 0.36767042 ... 0.38633546 0.41357981 0.39068914]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_model2_1(X_train,y_train,hps):\n",
    "    C = hps[\"C\"]\n",
    "    #class_weight={1:16.5}\n",
    "    class_weight={1:hps[\"w1\"]}\n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=hps[\"C\"],\n",
    "                  kernel=\"rbf\",\n",
    "                  gamma=hps[\"gamma\"],\n",
    "                  class_weight=class_weight,\n",
    "                  decision_function_shape='ovo',\n",
    "                  verbose=True).fit(X_train,y_train)\n",
    "    print(\"C: %f\"%hps[\"C\"])\n",
    "    print(\"gamma: %f\"%hps[\"gamma\"])\n",
    "    print(\"Computed class weight: %s\"%(str(clf.class_weight_)))\n",
    "    return(clf)\n",
    "\n",
    "for i in range(0,len(subtask2_labels)):\n",
    "#for i in range(0,1):\n",
    "    y_train=df_train_labels[subtask2_labels[i]].values\n",
    "    \n",
    "    #X_train,X_test = forest_fi(X_train,y_train,X_test)\n",
    "    #print(np.shape(X_train))\n",
    "    #print(np.shape(X_test))\n",
    "    \n",
    "    hps = {\"C\":0.01,\"w1\":16.5}\n",
    "    clf = fit_model2_1(X_train,y_train,hps)\n",
    "    \n",
    "    score = clf.score(X_train,y_train)\n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    p_train_pred = sigmoid(clf.decision_function(X_train))\n",
    "    print(p_train_pred)\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(y_train_pred)\n",
    "\n",
    "    p_test = sigmoid(clf.decision_function(X_test))\n",
    "    print(p_test)\n",
    "    y_test=clf.predict(X_test)\n",
    "    print(y_test)    \n",
    "    \n",
    "    \n",
    "    df_test_labels[subtask2_labels[i]] = p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1088.0\n",
      "train score= 0.947355\n",
      "cv roc auc= 0.713952\n",
      "cv acc= 0.942459\n",
      "cv rec= 0.005509\n",
      "cv prec= 0.306667\n",
      "y train:\n",
      "n pos 1088.000000\n",
      "frac pos 0.057278\n",
      "y train pred:\n",
      "n pos 90.000000\n",
      "frac pos 0.004738\n",
      "prob sum 0 17905.767893\n",
      "prob sum 1 1089.232107\n",
      "y test:\n",
      "prob sum 0 11965.876904\n",
      "prob sum 1 698.123096\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(subtask2_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask2_labels[i]].values\n",
    "    n = float(np.shape(y_train)[0])\n",
    "    \n",
    "    #sample weight\n",
    "    use_custom_sw = False\n",
    "    if use_custom_sw:\n",
    "        #sample weight\n",
    "        n = float(np.shape(y_train)[0])\n",
    "        w1_boost = 10.0\n",
    "\n",
    "        w0 = n/(n-np.sum(y_train))\n",
    "        w1 = (n/np.sum(y_train))*w1_boost\n",
    "        geo_mean = np.sqrt(w0*w1)\n",
    "        w0 /= geo_mean\n",
    "        w1 /= geo_mean\n",
    "    else:\n",
    "        w0=1.0\n",
    "        w1=1.0\n",
    "    \n",
    "    clf_init = HistGradientBoostingClassifier(random_state=42)\n",
    "    clf = fit_model1(X_train,y_train,clf_init,sw_dict={0:w0,1:w1})\n",
    "    score = clf.score(X_train,y_train)\n",
    "    \n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "    \n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "    \n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "    \n",
    "    p_train=clf.predict_proba(X_train)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "    \n",
    "    #print(p_train)\n",
    "    \n",
    "    print(\"y test:\")\n",
    "    p_test=clf.predict_proba(X_test)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #y_test[:,1+i] = p_test[:,1]\n",
    "    \n",
    "    df_test_labels[subtask2_labels[i]] = p_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9149813 , 0.0850187 ],\n",
       "       [0.9754607 , 0.0245393 ],\n",
       "       [0.95058718, 0.04941282],\n",
       "       ...,\n",
       "       [0.95687934, 0.04312066],\n",
       "       [0.87758738, 0.12241262],\n",
       "       [0.8645211 , 0.1354789 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.54553485, 0.50528193, 0.3668673 , 0.43168187, 0.30331087]),\n",
       " 'score_time': array([0.0098021 , 0.01191616, 0.00901985, 0.00987816, 0.009166  ]),\n",
       " 'test_roc_auc': array([0.69340494, 0.71512066, 0.7203135 , 0.73855581, 0.70236454]),\n",
       " 'test_accuracy': array([0.94261648, 0.94261648, 0.94261648, 0.9418268 , 0.94261648]),\n",
       " 'test_recall': array([0.00460829, 0.        , 0.00458716, 0.00458716, 0.01376147]),\n",
       " 'test_precision': array([0.33333333, 0.        , 0.5       , 0.2       , 0.5       ])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.mean of 0        0.085019\n",
       "1        0.024539\n",
       "2        0.049413\n",
       "3        0.028747\n",
       "4        0.030725\n",
       "           ...   \n",
       "12659    0.025250\n",
       "12660    0.149124\n",
       "12661    0.043121\n",
       "12662    0.122413\n",
       "12663    0.135479\n",
       "Name: LABEL_Sepsis, Length: 12664, dtype: float64>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels['LABEL_Sepsis'].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p_train_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ffdd68083d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_train_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p_train_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_train[:100])\n",
    "print(y_train_pred[:100])\n",
    "print(p_train_pred[:100])\n",
    "\n",
    "print(len(y_train_pred))\n",
    "print(np.sum(y_train))\n",
    "print(np.sum(p_train_pred))\n",
    "print(np.sum(y_train_pred))\n",
    "\n",
    "print(len(y_test))\n",
    "print(np.sum(p_test))\n",
    "print(np.sum(y_test))\n",
    "\n",
    "\n",
    "print(np.max(p_train_pred))\n",
    "print(np.max(p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394.0\n",
      "1088.0\n",
      "1538.0\n",
      "0.36213235294117646\n"
     ]
    }
   ],
   "source": [
    "a = np.sum(y_train_pred*y_train)\n",
    "b = np.sum(y_train)\n",
    "c = np.sum(y_train_pred)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pid', 'LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2', 'LABEL_Sepsis', 'LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
      "0        0.666441\n",
      "1        0.281373\n",
      "2        0.271900\n",
      "3        0.226549\n",
      "4        0.301836\n",
      "           ...   \n",
      "12659    0.446852\n",
      "12660    0.666563\n",
      "12661    0.430392\n",
      "12662    0.668956\n",
      "12663    0.482715\n",
      "Name: LABEL_Sepsis, Length: 12664, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(list(df_test_labels))\n",
    "print(df_test_labels['LABEL_Sepsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 140)\n",
      "(12664, 140)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "16\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "12.0\n",
      "12664\n",
      "16\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train3_raw = feats_2_X3(df_train_feats,subtask3_feats)\n",
    "X_test3_raw = feats_2_X3(df_test_feats,subtask3_feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 16)\n",
      "(12664, 16)\n",
      "(18995, 16)\n",
      "(12664, 16)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train3_raw))\n",
    "print(np.shape(X_test3_raw))\n",
    "\n",
    "X_train,X_test = impute(X_train3_raw,X_test3_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "#X_train,X_test = standardize(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "train score= 0.469106\n",
      "cv r2= 0.428798\n",
      "18.795959655719553\n",
      "2.2803903770349776\n",
      "18.832137890197153\n",
      "2.3008640847688953\n",
      "i=1\n",
      "train score= 0.640672\n",
      "cv r2= 0.615590\n",
      "82.51117094000382\n",
      "9.993382202155903\n",
      "82.3970899475064\n",
      "9.930920910148375\n",
      "i=2\n",
      "train score= 0.532303\n",
      "cv r2= 0.394851\n",
      "96.94731096288042\n",
      "1.444596558594917\n",
      "96.97107388474392\n",
      "1.187288986422359\n",
      "i=3\n",
      "train score= 0.686608\n",
      "cv r2= 0.666088\n",
      "84.11971626206535\n",
      "11.95757177949422\n",
      "84.19049395127627\n",
      "11.89587415622568\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(subtask3_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask3_labels[i]].values\n",
    "    reg = fit_model3(X_train,y_train)\n",
    "    \n",
    "    score = reg.score(X_train,y_train)\n",
    "    cv_results = cross_validate(reg,X_train,y_train,cv=5,\n",
    "            scoring=[\"r2\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv r2= %f\"%np.mean(cv_results['test_r2']))\n",
    "    \n",
    "    y_train_pred=reg.predict(X_train)\n",
    "    print(np.mean(y_train_pred))\n",
    "    print(np.std(y_train_pred))\n",
    "    \n",
    "    y_test=reg.predict(X_test)\n",
    "    print(np.mean(y_test))\n",
    "    print(np.std(y_test))\n",
    "    \n",
    "    df_test_labels[subtask3_labels[i]] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 10001 10003 ...  9992  9994  9997]\n",
      "[86.47186679 93.90677518 87.82147566 ... 82.41350651 97.39654798\n",
      " 86.88534173]\n",
      "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
      "0          0          0.994959          0.999994   0.981411   \n",
      "1      10001          0.049875          0.029881   0.384405   \n",
      "2      10003          0.027805          0.029881   0.122498   \n",
      "3      10004          0.108550          0.029881   0.298896   \n",
      "4      10005          0.100144          0.022194   0.108666   \n",
      "...      ...               ...               ...        ...   \n",
      "12659   9989          0.633362          0.022194   0.158406   \n",
      "12660   9991          0.210318          0.104080   0.096905   \n",
      "12661   9992          0.457877          0.029881   0.121389   \n",
      "12662   9994          0.981817          0.035027   0.919459   \n",
      "12663   9997          0.946191          0.022194   0.155260   \n",
      "\n",
      "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
      "0                0.941621               0.970611       0.647479   \n",
      "1                0.318839               0.348683       0.090423   \n",
      "2                0.181366               0.197771       0.576009   \n",
      "3                0.395096               0.374378       0.063629   \n",
      "4                0.108329               0.101088       0.069175   \n",
      "...                   ...                    ...            ...   \n",
      "12659            0.149537               0.123795       0.325749   \n",
      "12660            0.183593               0.092373       0.035034   \n",
      "12661            0.028937               0.118944       0.158103   \n",
      "12662            0.674309               0.847690       0.990193   \n",
      "12663            0.147542               0.121502       0.361958   \n",
      "\n",
      "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
      "0             0.026046    0.007740                0.165367     0.015844   \n",
      "1             0.047606    0.088610                0.026345     0.015844   \n",
      "2             0.047606    0.244693                0.026345     0.015844   \n",
      "3             0.052066    0.100864                0.013509     0.015844   \n",
      "4             0.011201    0.064041                0.026345     0.015844   \n",
      "...                ...         ...                     ...          ...   \n",
      "12659         0.008643    0.051403                0.013509     0.015844   \n",
      "12660         0.012848    0.001777                0.013509     0.111137   \n",
      "12661         0.007246    0.596600                0.013509     0.015844   \n",
      "12662         0.023573    0.954520                0.026345     0.015844   \n",
      "12663         0.007246    0.107377                0.026345     0.015844   \n",
      "\n",
      "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
      "0          0.666441    15.329609   79.318640   98.606996        86.471867  \n",
      "1          0.281373    16.918916   92.538808   94.955577        93.906775  \n",
      "2          0.271900    17.337657   80.111442   98.458517        87.821476  \n",
      "3          0.226549    16.666955   73.934671   95.608431        90.706453  \n",
      "4          0.301836    19.331331   73.934132   95.785930        60.906435  \n",
      "...             ...          ...         ...         ...              ...  \n",
      "12659      0.446852    20.552725   76.127907   95.736979       101.729947  \n",
      "12660      0.666563    18.370153   92.993427   98.733752        73.281689  \n",
      "12661      0.430392    19.357589   67.160455   97.452558        82.413507  \n",
      "12662      0.668956    16.095408   92.898073   97.372465        97.396548  \n",
      "12663      0.482715    17.718050   75.597544   98.292672        86.885342  \n",
      "\n",
      "[12664 rows x 16 columns]\n",
      "3397.76318937941\n"
     ]
    }
   ],
   "source": [
    "print(test_pids)\n",
    "print(y_test)\n",
    "print(df_test_labels)\n",
    "print(np.sum(df_test_labels['LABEL_BaseExcess']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.922230</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.876636</td>\n",
       "      <td>0.942709</td>\n",
       "      <td>0.818287</td>\n",
       "      <td>0.413834</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>0.327305</td>\n",
       "      <td>0.073881</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.085019</td>\n",
       "      <td>15.349950</td>\n",
       "      <td>79.818595</td>\n",
       "      <td>98.549891</td>\n",
       "      <td>87.666998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>0.029884</td>\n",
       "      <td>0.024146</td>\n",
       "      <td>0.339080</td>\n",
       "      <td>0.279901</td>\n",
       "      <td>0.251734</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>0.087575</td>\n",
       "      <td>0.050337</td>\n",
       "      <td>0.053138</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.024539</td>\n",
       "      <td>17.358841</td>\n",
       "      <td>93.463357</td>\n",
       "      <td>94.866827</td>\n",
       "      <td>94.786770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>0.015405</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.161865</td>\n",
       "      <td>0.170520</td>\n",
       "      <td>0.227277</td>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.216313</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>0.049413</td>\n",
       "      <td>17.209282</td>\n",
       "      <td>80.547162</td>\n",
       "      <td>98.497192</td>\n",
       "      <td>88.064269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.316502</td>\n",
       "      <td>0.334136</td>\n",
       "      <td>0.290121</td>\n",
       "      <td>0.071723</td>\n",
       "      <td>0.041944</td>\n",
       "      <td>0.087275</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.028747</td>\n",
       "      <td>16.653610</td>\n",
       "      <td>74.887420</td>\n",
       "      <td>95.527397</td>\n",
       "      <td>90.925549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>0.100838</td>\n",
       "      <td>0.041141</td>\n",
       "      <td>0.153471</td>\n",
       "      <td>0.117237</td>\n",
       "      <td>0.125790</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>0.006810</td>\n",
       "      <td>0.045313</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.030725</td>\n",
       "      <td>19.378653</td>\n",
       "      <td>74.532106</td>\n",
       "      <td>95.780562</td>\n",
       "      <td>62.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>9989</td>\n",
       "      <td>0.493078</td>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.195267</td>\n",
       "      <td>0.134382</td>\n",
       "      <td>0.133590</td>\n",
       "      <td>0.344067</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.147624</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.025250</td>\n",
       "      <td>20.897191</td>\n",
       "      <td>75.958978</td>\n",
       "      <td>95.773175</td>\n",
       "      <td>101.268688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>9991</td>\n",
       "      <td>0.240444</td>\n",
       "      <td>0.112526</td>\n",
       "      <td>0.143815</td>\n",
       "      <td>0.165256</td>\n",
       "      <td>0.186950</td>\n",
       "      <td>0.257387</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.221607</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.149124</td>\n",
       "      <td>18.527361</td>\n",
       "      <td>93.730542</td>\n",
       "      <td>98.840583</td>\n",
       "      <td>73.722729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>9992</td>\n",
       "      <td>0.699523</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.046210</td>\n",
       "      <td>0.054838</td>\n",
       "      <td>0.172692</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.684390</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.043121</td>\n",
       "      <td>18.809579</td>\n",
       "      <td>66.258219</td>\n",
       "      <td>97.337714</td>\n",
       "      <td>83.741593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.990294</td>\n",
       "      <td>0.592081</td>\n",
       "      <td>0.848351</td>\n",
       "      <td>0.924976</td>\n",
       "      <td>0.927498</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>0.030560</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>0.269929</td>\n",
       "      <td>0.006093</td>\n",
       "      <td>0.122413</td>\n",
       "      <td>16.036526</td>\n",
       "      <td>92.612451</td>\n",
       "      <td>98.657214</td>\n",
       "      <td>98.119461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.887361</td>\n",
       "      <td>0.018318</td>\n",
       "      <td>0.139694</td>\n",
       "      <td>0.099819</td>\n",
       "      <td>0.106114</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.202355</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.135479</td>\n",
       "      <td>17.615258</td>\n",
       "      <td>75.478707</td>\n",
       "      <td>98.396046</td>\n",
       "      <td>87.254120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0          0.922230          0.564706   0.876636   \n",
       "1      10001          0.029884          0.024146   0.339080   \n",
       "2      10003          0.015405          0.024254   0.212800   \n",
       "3      10004          0.019618          0.087000   0.316502   \n",
       "4      10005          0.100838          0.041141   0.153471   \n",
       "...      ...               ...               ...        ...   \n",
       "12659   9989          0.493078          0.021283   0.195267   \n",
       "12660   9991          0.240444          0.112526   0.143815   \n",
       "12661   9992          0.699523          0.027226   0.079635   \n",
       "12662   9994          0.990294          0.592081   0.848351   \n",
       "12663   9997          0.887361          0.018318   0.139694   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.942709               0.818287       0.413834   \n",
       "1                0.279901               0.251734       0.052736   \n",
       "2                0.161865               0.170520       0.227277   \n",
       "3                0.334136               0.290121       0.071723   \n",
       "4                0.117237               0.125790       0.085006   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.134382               0.133590       0.344067   \n",
       "12660            0.165256               0.186950       0.257387   \n",
       "12661            0.046210               0.054838       0.172692   \n",
       "12662            0.924976               0.927498       0.877358   \n",
       "12663            0.099819               0.106114       0.370453   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
       "0             0.004664    0.327305                0.073881     0.006220   \n",
       "1             0.087575    0.050337                0.053138     0.014200   \n",
       "2             0.025159    0.216313                0.018399     0.010059   \n",
       "3             0.041944    0.087275                0.010719     0.022491   \n",
       "4             0.006810    0.045313                0.010978     0.001147   \n",
       "...                ...         ...                     ...          ...   \n",
       "12659         0.003779    0.147624                0.006330     0.001229   \n",
       "12660         0.020217    0.221607                0.012400     0.010231   \n",
       "12661         0.006970    0.684390                0.010117     0.001838   \n",
       "12662         0.030560    0.865731                0.269929     0.006093   \n",
       "12663         0.006248    0.202355                0.010231     0.001288   \n",
       "\n",
       "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
       "0          0.085019    15.349950   79.818595   98.549891        87.666998  \n",
       "1          0.024539    17.358841   93.463357   94.866827        94.786770  \n",
       "2          0.049413    17.209282   80.547162   98.497192        88.064269  \n",
       "3          0.028747    16.653610   74.887420   95.527397        90.925549  \n",
       "4          0.030725    19.378653   74.532106   95.780562        62.004454  \n",
       "...             ...          ...         ...         ...              ...  \n",
       "12659      0.025250    20.897191   75.958978   95.773175       101.268688  \n",
       "12660      0.149124    18.527361   93.730542   98.840583        73.722729  \n",
       "12661      0.043121    18.809579   66.258219   97.337714        83.741593  \n",
       "12662      0.122413    16.036526   92.612451   98.657214        98.119461  \n",
       "12663      0.135479    17.615258   75.478707   98.396046        87.254120  \n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_labels.to_csv('prediction_1904_2.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "X, y = datasets.load_digits(n_class=9, return_X_y=True)\n",
    "data = X / 16.\n",
    "\n",
    "clf1 = svm.SVC()\n",
    "clf1.fit(data,y)\n",
    "print(clf1.score(data,y))\n",
    "\n",
    "feature_map_nystroem = Nystroem(gamma=.2,\n",
    "                                random_state=1,\n",
    "                                n_components=100)\n",
    "feature_map_nystroem.fit(data)\n",
    "Q = feature_map_nystroem.transform(data)\n",
    "sqrt_k_inv = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "B = np.dot(Q,sqrt_k_inv)\n",
    "K = np.dot(B,np.transpose(B))\n",
    "\n",
    "clf2 = svm.SVC(kernel=\"precomputed\")\n",
    "clf2.fit(K,y)\n",
    "print(clf2.score(K,y))\n",
    "\n",
    "#clf.fit(data_transformed, y)\n",
    "#clf.score(data_transformed, y)\n",
    "\n",
    "\n",
    "\n",
    "a = np.array([1,np.nan,2])\n",
    "print(np.mean(a[~np.isnan(a)]))\n",
    "\n",
    "b = np.array([np.nan,np.nan,np.nan])\n",
    "print(np.min(b[~np.isnan(b)]))\n",
    "\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "X, y = datasets.load_digits(n_class=9, return_X_y=True)\n",
    "data = X / 16.\n",
    "clf = svm.LinearSVC()\n",
    "feature_map_nystroem = Nystroem(gamma=.2,\n",
    "                                random_state=1,\n",
    "                                n_components=300)\n",
    "data_transformed = feature_map_nystroem.fit_transform(data)\n",
    "clf.fit(data_transformed, y)\n",
    "\n",
    "clf.score(data_transformed, y)\n",
    "\n",
    "print(np.shape(data))\n",
    "print(np.shape(data_transformed))\n",
    "\n",
    "print(np.shape(data))\n",
    "print(np.shape(data_transformed))\n",
    "\n",
    "\n",
    "#X_train2_raw = feats_2_X2(df_train_feats,active_feats)\n",
    "df_feats = df_train_feats\n",
    "feats_list = subtask3_feats\n",
    "n_derived_feats = 2\n",
    "\n",
    "pids = pd.unique(df_feats['pid'])\n",
    "n_patients = len(pids)\n",
    "#df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "print(n_patients)\n",
    "print(n_derived_feats*len(feats_list))\n",
    "X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "patient_df_sizes = np.zeros(n_patients)\n",
    "for i in range(0,1):\n",
    "#for i in range(0,n_patients):\n",
    "    patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "    patient_df_sizes[i] = patient_df.shape[0]\n",
    "    for j in range(0,1):\n",
    "        patient_feat = patient_df[feats_list[j]].values\n",
    "        times = (patient_df['Time'].values)[~np.isnan(patient_feat)]\n",
    "        patient_feat = patient_feat[~np.isnan(patient_feat)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
