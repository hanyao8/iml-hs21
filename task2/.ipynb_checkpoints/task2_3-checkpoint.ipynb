{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_sample = pd.read_csv(\"data/sample.csv\")\n",
    "#sample = open(\"data/sample.csv\")\n",
    "#sample = open(os.path.join(os.getcwd(),\"data/sample.csv\"))\n",
    "\n",
    "df_train_feats = pd.read_csv(\"data/train_features.csv\")\n",
    "df_train_labels = pd.read_csv(\"data/train_labels.csv\")\n",
    "df_test_feats = pd.read_csv(\"data/test_features.csv\")\n",
    "\n",
    "df_labels_cols = list(df_train_labels)\n",
    "\n",
    "active_feats = [\n",
    " 'Age',\n",
    " 'EtCO2',\n",
    " 'PTT',\n",
    " 'BUN',\n",
    " 'Lactate',\n",
    " 'Temp',\n",
    " 'Hgb',\n",
    " 'HCO3',\n",
    " 'BaseExcess',\n",
    " 'RRate',\n",
    " 'Fibrinogen',\n",
    " 'Phosphate',\n",
    " 'WBC',\n",
    " 'Creatinine',\n",
    " 'PaCO2',\n",
    " 'AST',\n",
    " 'FiO2',\n",
    " 'Platelets',\n",
    " 'SaO2',\n",
    " 'Glucose',\n",
    " 'ABPm',\n",
    " 'Magnesium',\n",
    " 'Potassium',\n",
    " 'ABPd',\n",
    " 'Calcium',\n",
    " 'Alkalinephos',\n",
    " 'SpO2',\n",
    " 'Bilirubin_direct',\n",
    " 'Chloride',\n",
    " 'Hct',\n",
    " 'Heartrate',\n",
    " 'Bilirubin_total',\n",
    " 'TroponinI',\n",
    " 'ABPs',\n",
    " 'pH']\n",
    "\n",
    "subtask3_feats = [\n",
    " 'RRate',\n",
    " 'ABPm',\n",
    " 'SpO2',\n",
    " 'Heartrate']\n",
    "\n",
    "subtask1_labels = [\n",
    " 'LABEL_BaseExcess',\n",
    " 'LABEL_Fibrinogen',\n",
    " 'LABEL_AST',\n",
    " 'LABEL_Alkalinephos',\n",
    " 'LABEL_Bilirubin_total',\n",
    " 'LABEL_Lactate',\n",
    " 'LABEL_TroponinI',\n",
    " 'LABEL_SaO2',\n",
    " 'LABEL_Bilirubin_direct',\n",
    " 'LABEL_EtCO2'\n",
    "        ]\n",
    "\n",
    "subtask2_labels = [\n",
    " 'LABEL_Sepsis'\n",
    " ]\n",
    "\n",
    "subtask3_labels = [\n",
    " 'LABEL_RRate',\n",
    " 'LABEL_ABPm',\n",
    " 'LABEL_SpO2',\n",
    " 'LABEL_Heartrate'\n",
    "        ]\n",
    "\n",
    "\n",
    "def feats_2_X1(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    n_derived_feats = 4\n",
    "\n",
    "    #pids = np.unique(df_train_feats['pid'].values)\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "    #df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            patient_feat = patient_feat[~np.isnan(patient_feat)]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "                X[i][j0+1] = np.std(patient_feat)\n",
    "                X[i][j0+2] = np.min(patient_feat)\n",
    "                X[i][j0+3] = np.max(patient_feat)\n",
    "        #print(patient_df)\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n",
    "\n",
    "def feats_2_X2(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    n_derived_feats = 4\n",
    "\n",
    "    #pids = np.unique(df_train_feats['pid'].values)\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "    #df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            patient_feat = patient_feat[~np.isnan(patient_feat)]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "                #X[i][j0+1] = np.std(patient_feat)\n",
    "                #X[i][j0+2] = np.min(patient_feat)\n",
    "                #X[i][j0+3] = np.max(patient_feat)\n",
    "        #print(patient_df)\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n",
    "\n",
    "#print(df_train_feats.describe())\n",
    "#a = df_train_feats[df_train_feats['Time']>24]\n",
    "#print(a.shape)\n",
    "\n",
    "#print(np.mean(a[~np.isnan(a)]))\n",
    "\n",
    "def feats_2_X3(df_feats,feats_list):\n",
    "    #Subtask1\n",
    "    n_derived_feats = 4\n",
    "\n",
    "    #pids = np.unique(df_train_feats['pid'].values)\n",
    "    pids = pd.unique(df_feats['pid'])\n",
    "    n_patients = len(pids)\n",
    "    #df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "    print(n_patients)\n",
    "    print(n_derived_feats*len(feats_list))\n",
    "    X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "    patient_df_sizes = np.zeros(n_patients)\n",
    "    for i in range(0,n_patients):\n",
    "        patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "        patient_df_sizes[i] = patient_df.shape[0]\n",
    "        for j in range(0,len(feats_list)):\n",
    "            patient_feat = patient_df[feats_list[j]].values\n",
    "            nan_mask = np.isnan(patient_feat)\n",
    "            times = (patient_df['Time'].values)[~nan_mask]\n",
    "            patient_feat = patient_feat[~nan_mask]\n",
    "            #print(len(patient_feat))\n",
    "            j0 = j*n_derived_feats\n",
    "            \n",
    "            if len(patient_feat)>0:\n",
    "                X[i][j0+0] = np.mean(patient_feat)\n",
    "            if len(patient_feat)>1:\n",
    "                X[i][j0+1] = patient_feat[-1]\n",
    "                X[i][j0+2] = (patient_feat[-1]-patient_feat[0])/(times[-1]-times[0])\n",
    "            if len(patient_feat)>2:\n",
    "                grad_end = (patient_feat[-1]-patient_feat[-2])/(times[-1]-times[-2])\n",
    "                X[i][j0+3] = grad_end\n",
    "        #print(patient_df)\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print(np.max(patient_df_sizes))\n",
    "    return(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def standardize(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return(X_train,X_test)\n",
    "    \n",
    "\n",
    "def impute(X_train,X_test):\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean.fit(X_train)\n",
    "    \n",
    "    X_train = imp_mean.transform(X_train)\n",
    "    X_test = imp_mean.transform(X_test)\n",
    "\n",
    "    #print(X_train_imp)\n",
    "    return(X_train,X_test)\n",
    "\n",
    "def forest_fi(X_train,y_train,X_test):\n",
    "\n",
    "    forest = ExtraTreesClassifier(n_estimators=20,\n",
    "                                  random_state=0)\n",
    "\n",
    "    forest.fit(X_train,y_train)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    X_train = X_train[:,indices[:50]]\n",
    "    X_test = X_test[:,indices[:50]]\n",
    "    return(X_train,X_test)\n",
    "\n",
    "\n",
    "def nystroem(X_train,X_test):\n",
    "    gamma=1.0\n",
    "    n_components=100\n",
    "    print(\"nystroem gamma=%f\"%(gamma))\n",
    "    print(\"nystroem q=%d\"%(n_components))\n",
    "    feature_map_nystroem = Nystroem(gamma=gamma,\n",
    "                                    random_state=42,\n",
    "                                    n_components=n_components)\n",
    "    feature_map_nystroem.fit(X_train)\n",
    "    Q_train = feature_map_nystroem.transform(X_train)\n",
    "    sqrt_k_inv_train = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "    B_train = np.dot(Q_train,sqrt_k_inv_train)\n",
    "    K_train = np.dot(B_train,np.transpose(B_train))\n",
    "\n",
    "    Q_test = feature_map_nystroem.transform(X_test)\n",
    "    sqrt_k_inv_test = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "    B_test = np.dot(Q_test,sqrt_k_inv_test)\n",
    "    K_test = np.dot(B_test,np.transpose(B_train))\n",
    "    return(K_train,K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "def fit_model1_1(X_train,y_train):\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "                                     max_depth=3, random_state=42).fit(X_train,y_train)\n",
    "    #clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n",
    "    #                                 max_depth=5, random_state=42).fit(X_train_imp,y_train)\n",
    "\n",
    "    return(clf)\n",
    "\n",
    "def fit_model1_2(X_train,y_train):\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=10,\n",
    "                  class_weight=\"balanced\",\n",
    "                  decision_function_shape='ovo').fit(X_train,y_train)\n",
    "    return(clf)\n",
    "\n",
    "def fit_model1(X_train,y_train,clf_init,sw_dict={}):\n",
    "    n = np.shape(y_train)[0]\n",
    "    w0 = n/(n-np.sum(y_train))\n",
    "    w1 = n/np.sum(y_train)\n",
    "    \n",
    "    sample_weight = np.zeros(len(y_train))\n",
    "    if not(sw_dict):\n",
    "        print(\"using default sample weights\")\n",
    "        sample_weight[y_train == 0] = w0\n",
    "        sample_weight[y_train == 1] = w1\n",
    "    else:\n",
    "        print(\"using custom sample weights\")\n",
    "        sample_weight[y_train == 0] = sw_dict[0]\n",
    "        sample_weight[y_train == 1] = sw_dict[1]\n",
    "    \n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    clf = clf_init\n",
    "    #clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, \n",
    "    #                                 max_depth=3, random_state=42)\n",
    "    clf.fit(X_train,y_train,sample_weight=sample_weight)\n",
    "    #clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n",
    "    #                                 max_depth=5, random_state=42).fit(X_train_imp,y_train)\n",
    "\n",
    "    return(clf)\n",
    "\n",
    "\n",
    "\n",
    "def fit_model2(K_train,y_train):\n",
    "    C = 1.0\n",
    "    class_weight={1:17.5}\n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=C,\n",
    "                  kernel=\"precomputed\",\n",
    "                  class_weight=\"balanced\",\n",
    "                  decision_function_shape='ovo',\n",
    "                  verbose=True).fit(K_train,y_train)\n",
    "    print(\"C: %f\"%C)\n",
    "    print(\"Computed class weight: %s\"%(str(clf.class_weight_)))\n",
    "    return(clf)\n",
    "\n",
    "\n",
    "def fit_model3(X_train,y_train):\n",
    "    #print(np.shape(y_train))\n",
    "    #print(np.sum(y_train))\n",
    "    reg = GradientBoostingRegressor(random_state=42).fit(X_train,y_train)\n",
    "\n",
    "    return(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "if {0:1}:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(y_train))\n",
    "\n",
    "test_pids = pd.unique(df_test_feats['pid'])\n",
    "#y_test = np.zeros((len(test_pids),1+len(subtask1_labels)))\n",
    "#y_test[:,0] = test_pids\n",
    "\n",
    "df_test_labels = pd.DataFrame(columns=df_labels_cols)\n",
    "df_test_labels['pid'] = test_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "12.0\n",
      "12664\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "12.0\n",
      "(18995, 140)\n",
      "(12664, 140)\n",
      "(18995, 140)\n",
      "(12664, 140)\n"
     ]
    }
   ],
   "source": [
    "X_train1_raw = feats_2_X1(df_train_feats,active_feats)\n",
    "X_test1_raw = feats_2_X1(df_test_feats,active_feats)\n",
    "\n",
    "print(np.shape(X_train1_raw))\n",
    "print(np.shape(X_test1_raw))\n",
    "X_train,X_test = impute(X_train1_raw,X_test1_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "18995\n",
      "18995\n",
      "18995\n",
      "1322\n",
      "1322\n",
      "1322\n",
      "1322\n",
      "7816\n",
      "7816\n",
      "7816\n",
      "7816\n",
      "13966\n",
      "13966\n",
      "13966\n",
      "13966\n",
      "4872\n",
      "4872\n",
      "4872\n",
      "4872\n",
      "18552\n",
      "18552\n",
      "18552\n",
      "18552\n",
      "13975\n",
      "13975\n",
      "13975\n",
      "13975\n",
      "7837\n",
      "7837\n",
      "7837\n",
      "7837\n",
      "5697\n",
      "5697\n",
      "5697\n",
      "5697\n",
      "18843\n",
      "18843\n",
      "18843\n",
      "18843\n",
      "1714\n",
      "1714\n",
      "1714\n",
      "1714\n",
      "9329\n",
      "9329\n",
      "9329\n",
      "9329\n",
      "13341\n",
      "13341\n",
      "13341\n",
      "13341\n",
      "13727\n",
      "13727\n",
      "13727\n",
      "13727\n",
      "7772\n",
      "7772\n",
      "7772\n",
      "7772\n",
      "4796\n",
      "4796\n",
      "4796\n",
      "4796\n",
      "7678\n",
      "7678\n",
      "7678\n",
      "7678\n",
      "13541\n",
      "13541\n",
      "13541\n",
      "13541\n",
      "5055\n",
      "5055\n",
      "5055\n",
      "5055\n",
      "16330\n",
      "16330\n",
      "16330\n",
      "16330\n",
      "18888\n",
      "18888\n",
      "18888\n",
      "18888\n",
      "12326\n",
      "12326\n",
      "12326\n",
      "12326\n",
      "14997\n",
      "14997\n",
      "14997\n",
      "14997\n",
      "15113\n",
      "15113\n",
      "15113\n",
      "15113\n",
      "12266\n",
      "12266\n",
      "12266\n",
      "12266\n",
      "4744\n",
      "4744\n",
      "4744\n",
      "4744\n",
      "18973\n",
      "18973\n",
      "18973\n",
      "18973\n",
      "623\n",
      "623\n",
      "623\n",
      "623\n",
      "8351\n",
      "8351\n",
      "8351\n",
      "8351\n",
      "14639\n",
      "14639\n",
      "14639\n",
      "14639\n",
      "18988\n",
      "18988\n",
      "18988\n",
      "18988\n",
      "4711\n",
      "4711\n",
      "4711\n",
      "4711\n",
      "2792\n",
      "2792\n",
      "2792\n",
      "2792\n",
      "18597\n",
      "18597\n",
      "18597\n",
      "18597\n",
      "7994\n",
      "7994\n",
      "7994\n",
      "7994\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(np.shape(X_train1_raw)[1]):\n",
    "    a = np.count_nonzero(~np.isnan(X_train1_raw[:,i]))\n",
    "    print(a)\n",
    "    if a>0:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "5096.0\n",
      "train score= 0.632166\n",
      "cv roc auc= 0.917255\n",
      "cv acc= 0.870808\n",
      "cv rec= 0.745681\n",
      "cv prec= 0.766467\n",
      "y train:\n",
      "n pos 5096.000000\n",
      "frac pos 0.268281\n",
      "y train pred:\n",
      "n pos 11893.000000\n",
      "frac pos 0.626112\n",
      "prob sum 0 7110.741088\n",
      "prob sum 1 11884.258912\n",
      "y test:\n",
      "prob sum 0 4722.337004\n",
      "prob sum 1 7941.662996\n",
      "\n",
      "\n",
      "\n",
      "i=1\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1400.0\n",
      "train score= 0.163622\n",
      "cv roc auc= 0.735051\n",
      "cv acc= 0.932193\n",
      "cv rec= 0.255000\n",
      "cv prec= 0.595222\n",
      "y train:\n",
      "n pos 1400.000000\n",
      "frac pos 0.073704\n",
      "y train pred:\n",
      "n pos 17287.000000\n",
      "frac pos 0.910082\n",
      "prob sum 0 4674.762768\n",
      "prob sum 1 14320.237232\n",
      "y test:\n",
      "prob sum 0 3141.957562\n",
      "prob sum 1 9522.042438\n",
      "\n",
      "\n",
      "\n",
      "i=2\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4554.0\n",
      "train score= 0.271124\n",
      "cv roc auc= 0.718304\n",
      "cv acc= 0.782206\n",
      "cv rec= 0.251868\n",
      "cv prec= 0.611808\n",
      "y train:\n",
      "n pos 4554.000000\n",
      "frac pos 0.239747\n",
      "y train pred:\n",
      "n pos 18387.000000\n",
      "frac pos 0.967992\n",
      "prob sum 0 2921.249337\n",
      "prob sum 1 16073.750663\n",
      "y test:\n",
      "prob sum 0 1954.307135\n",
      "prob sum 1 10709.692865\n",
      "\n",
      "\n",
      "\n",
      "i=3\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4487.0\n",
      "train score= 0.267544\n",
      "cv roc auc= 0.721231\n",
      "cv acc= 0.783680\n",
      "cv rec= 0.253619\n",
      "cv prec= 0.600167\n",
      "y train:\n",
      "n pos 4487.000000\n",
      "frac pos 0.236220\n",
      "y train pred:\n",
      "n pos 18382.000000\n",
      "frac pos 0.967728\n",
      "prob sum 0 2950.711345\n",
      "prob sum 1 16044.288655\n",
      "y test:\n",
      "prob sum 0 1969.800040\n",
      "prob sum 1 10694.199960\n",
      "\n",
      "\n",
      "\n",
      "i=4\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4570.0\n",
      "train score= 0.273756\n",
      "cv roc auc= 0.717689\n",
      "cv acc= 0.778994\n",
      "cv rec= 0.253392\n",
      "cv prec= 0.596613\n",
      "y train:\n",
      "n pos 4570.000000\n",
      "frac pos 0.240590\n",
      "y train pred:\n",
      "n pos 18351.000000\n",
      "frac pos 0.966096\n",
      "prob sum 0 2974.748385\n",
      "prob sum 1 16020.251615\n",
      "y test:\n",
      "prob sum 0 2004.092732\n",
      "prob sum 1 10659.907268\n",
      "\n",
      "\n",
      "\n",
      "i=5\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "3803.0\n",
      "train score= 0.252698\n",
      "cv roc auc= 0.789524\n",
      "cv acc= 0.828376\n",
      "cv rec= 0.372073\n",
      "cv prec= 0.618667\n",
      "y train:\n",
      "n pos 3803.000000\n",
      "frac pos 0.200211\n",
      "y train pred:\n",
      "n pos 17978.000000\n",
      "frac pos 0.946460\n",
      "prob sum 0 3848.154512\n",
      "prob sum 1 15146.845488\n",
      "y test:\n",
      "prob sum 0 2559.328328\n",
      "prob sum 1 10104.671672\n",
      "\n",
      "\n",
      "\n",
      "i=6\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1895.0\n",
      "train score= 0.476915\n",
      "cv roc auc= 0.788665\n",
      "cv acc= 0.920558\n",
      "cv rec= 0.426913\n",
      "cv prec= 0.657507\n",
      "y train:\n",
      "n pos 1895.000000\n",
      "frac pos 0.099763\n",
      "y train pred:\n",
      "n pos 11813.000000\n",
      "frac pos 0.621901\n",
      "prob sum 0 8166.951988\n",
      "prob sum 1 10828.048012\n",
      "y test:\n",
      "prob sum 0 5494.067599\n",
      "prob sum 1 7169.932401\n",
      "\n",
      "\n",
      "\n",
      "i=7\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "4439.0\n",
      "train score= 0.313714\n",
      "cv roc auc= 0.807055\n",
      "cv acc= 0.824164\n",
      "cv rec= 0.480290\n",
      "cv prec= 0.674060\n",
      "y train:\n",
      "n pos 4439.000000\n",
      "frac pos 0.233693\n",
      "y train pred:\n",
      "n pos 17443.000000\n",
      "frac pos 0.918294\n",
      "prob sum 0 4057.455887\n",
      "prob sum 1 14937.544113\n",
      "y test:\n",
      "prob sum 0 2677.381985\n",
      "prob sum 1 9986.618015\n",
      "\n",
      "\n",
      "\n",
      "i=8\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "644.0\n",
      "train score= 0.260384\n",
      "cv roc auc= 0.641653\n",
      "cv acc= 0.963833\n",
      "cv rec= 0.096294\n",
      "cv prec= 0.371049\n",
      "y train:\n",
      "n pos 644.000000\n",
      "frac pos 0.033904\n",
      "y train pred:\n",
      "n pos 14693.000000\n",
      "frac pos 0.773519\n",
      "prob sum 0 5986.965886\n",
      "prob sum 1 13008.034114\n",
      "y test:\n",
      "prob sum 0 4108.359140\n",
      "prob sum 1 8555.640860\n",
      "\n",
      "\n",
      "\n",
      "i=9\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1254.0\n",
      "train score= 0.579837\n",
      "cv roc auc= 0.544633\n",
      "cv acc= 0.943880\n",
      "cv rec= 0.341508\n",
      "cv prec= 0.620084\n",
      "y train:\n",
      "n pos 1254.000000\n",
      "frac pos 0.066017\n",
      "y train pred:\n",
      "n pos 9229.000000\n",
      "frac pos 0.485865\n",
      "prob sum 0 10590.355342\n",
      "prob sum 1 8404.644658\n",
      "y test:\n",
      "prob sum 0 7119.023826\n",
      "prob sum 1 5544.976174\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(subtask1_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask1_labels[i]].values\n",
    "\n",
    "    #sample weight\n",
    "    n = float(np.shape(y_train)[0])\n",
    "    w1_boost = 10.0\n",
    "    \n",
    "    w0 = n/(n-np.sum(y_train))\n",
    "    w1 = (n/np.sum(y_train))*w1_boost\n",
    "    geo_mean = np.sqrt(w0*w1)\n",
    "    w0 /= geo_mean\n",
    "    w1 /= geo_mean\n",
    "    \n",
    "    clf = fit_model1(X_train,y_train,sw_dict={0:w0,1:w1})\n",
    "    \n",
    "    score = clf.score(X_train,y_train)\n",
    "    \n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "    \n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "    \n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "    \n",
    "    p_train=clf.predict_proba(X_train)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "    \n",
    "    #print(p_train)\n",
    "    \n",
    "    print(\"y test:\")\n",
    "    p_test=clf.predict_proba(X_test)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #y_test[:,1+i] = p_test[:,1]\n",
    "    \n",
    "    df_test_labels[subtask1_labels[i]] = p_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=9\n",
      "using custom sample weights\n",
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1254.0\n",
      "train score= 0.983627\n",
      "cv roc auc= 0.934273\n",
      "cv acc= 0.963359\n",
      "cv rec= 0.580586\n",
      "cv prec= 0.810752\n",
      "y train:\n",
      "n pos 1254.000000\n",
      "frac pos 0.066017\n",
      "y train pred:\n",
      "n pos 993.000000\n",
      "frac pos 0.052277\n",
      "prob sum 0 17747.732897\n",
      "prob sum 1 1247.267103\n",
      "y test:\n",
      "prob sum 0 11882.576354\n",
      "prob sum 1 781.423646\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#hp_x = np.array([1,2,3,4,5,6,7,8,16])\n",
    "#hp_x = np.array([1,2,3,4,5,6,7,8,16])\n",
    "hp_x = [0]\n",
    "metrics = ['test_roc_auc',\n",
    "           'test_accuracy',\n",
    "           'test_recall',\n",
    "           'test_precision'\n",
    "          ]\n",
    "hp_y = np.zeros((len(hp_x),len(metrics)))\n",
    "\n",
    "i=9\n",
    "for j in range(0,len(hp_x)):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask1_labels[i]].values\n",
    "\n",
    "    use_custom_sw = False\n",
    "    \n",
    "    if use_custom_sw:\n",
    "        #sample weight\n",
    "        n = float(np.shape(y_train)[0])\n",
    "        w1_boost = 10.0\n",
    "\n",
    "        w0 = n/(n-np.sum(y_train))\n",
    "        w1 = (n/np.sum(y_train))*w1_boost\n",
    "        geo_mean = np.sqrt(w0*w1)\n",
    "        w0 /= geo_mean\n",
    "        w1 /= geo_mean\n",
    "    else:\n",
    "        w0=1.0\n",
    "        w1=1.0\n",
    "    \n",
    "    \n",
    "    #clf_init = GradientBoostingClassifier(n_estimators=hp_x[j], learning_rate=1.0, \n",
    "    #                                 max_depth=3, random_state=42)\n",
    "    clf_init = HistGradientBoostingClassifier(random_state=42)\n",
    "    clf = fit_model1(X_train,y_train,clf_init,sw_dict={0:w0,1:w1})\n",
    "    \n",
    "    score = clf.score(X_train,y_train)\n",
    "    \n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    for k in range(0,len(metrics)):\n",
    "        hp_y[j][k] = np.mean(cv_results[metrics[k]])\n",
    "        \n",
    "    \n",
    "    #print(sigmoid(clf.decision_function(X_train)))\n",
    "    #p_train=clf.predict(X_train)\n",
    "    \n",
    "    print(\"y train:\")\n",
    "    print(\"n pos %f\"%np.sum(y_train))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train)/n))\n",
    "    \n",
    "    print(\"y train pred:\")\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(\"n pos %f\"%np.sum(y_train_pred))\n",
    "    print(\"frac pos %f\"%(np.sum(y_train_pred)/n) ) \n",
    "    \n",
    "    p_train=clf.predict_proba(X_train)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_train[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_train[:,1]))\n",
    "    \n",
    "    #print(p_train)\n",
    "    \n",
    "    print(\"y test:\")\n",
    "    p_test=clf.predict_proba(X_test)\n",
    "    print(\"prob sum 0 %f\"%np.sum(p_test[:,0]))\n",
    "    print(\"prob sum 1 %f\"%np.sum(p_test[:,1]))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #y_test[:,1+i] = p_test[:,1]\n",
    "    \n",
    "    df_test_labels[subtask1_labels[i]] = p_test[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "for k in range(0,len(metrics)):\n",
    "    ax1.plot(np.log10(hp_x),hp_y[:,k],label=metrics[k])\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.222483394903913"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "12.0\n",
      "12664\n",
      "140\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "X_train2_raw = feats_2_X2(df_train_feats,active_feats)\n",
    "X_test2_raw = feats_2_X2(df_test_feats,active_feats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 140)\n",
      "(12664, 140)\n",
      "(18995, 140)\n",
      "(12664, 140)\n",
      "(18995, 140)\n",
      "(12664, 140)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train2_raw))\n",
    "print(np.shape(X_test2_raw))\n",
    "\n",
    "X_train,X_test = impute(X_train2_raw,X_test2_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "X_train,X_test = standardize(X_train,X_test)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "\n",
    "#X_train,X_test = nystroem(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 36 (0.013107)\n",
      "2. feature 38 (0.012164)\n",
      "3. feature 37 (0.011283)\n",
      "4. feature 20 (0.011242)\n",
      "5. feature 82 (0.010902)\n",
      "6. feature 39 (0.010877)\n",
      "7. feature 81 (0.010720)\n",
      "8. feature 122 (0.010658)\n",
      "9. feature 105 (0.010565)\n",
      "10. feature 104 (0.010515)\n",
      "11. feature 21 (0.010502)\n",
      "12. feature 120 (0.010440)\n",
      "13. feature 133 (0.010368)\n",
      "14. feature 121 (0.010107)\n",
      "15. feature 83 (0.010041)\n",
      "16. feature 135 (0.010013)\n",
      "17. feature 93 (0.009698)\n",
      "18. feature 2 (0.009675)\n",
      "19. feature 94 (0.009641)\n",
      "20. feature 106 (0.009594)\n",
      "21. feature 78 (0.009561)\n",
      "22. feature 80 (0.009519)\n",
      "23. feature 23 (0.009426)\n",
      "24. feature 76 (0.009424)\n",
      "25. feature 8 (0.009352)\n",
      "26. feature 95 (0.009322)\n",
      "27. feature 132 (0.009313)\n",
      "28. feature 134 (0.009184)\n",
      "29. feature 123 (0.009163)\n",
      "30. feature 0 (0.009141)\n",
      "31. feature 92 (0.009024)\n",
      "32. feature 3 (0.008970)\n",
      "33. feature 22 (0.008880)\n",
      "34. feature 91 (0.008879)\n",
      "35. feature 88 (0.008861)\n",
      "36. feature 79 (0.008787)\n",
      "37. feature 90 (0.008767)\n",
      "38. feature 67 (0.008571)\n",
      "39. feature 96 (0.008567)\n",
      "40. feature 14 (0.008485)\n",
      "41. feature 86 (0.008446)\n",
      "42. feature 87 (0.008437)\n",
      "43. feature 10 (0.008399)\n",
      "44. feature 70 (0.008327)\n",
      "45. feature 116 (0.008325)\n",
      "46. feature 77 (0.008318)\n",
      "47. feature 71 (0.008243)\n",
      "48. feature 48 (0.008200)\n",
      "49. feature 107 (0.008145)\n",
      "50. feature 98 (0.008039)\n",
      "51. feature 66 (0.007963)\n",
      "52. feature 68 (0.007948)\n",
      "53. feature 24 (0.007860)\n",
      "54. feature 84 (0.007842)\n",
      "55. feature 118 (0.007832)\n",
      "56. feature 119 (0.007818)\n",
      "57. feature 112 (0.007785)\n",
      "58. feature 50 (0.007769)\n",
      "59. feature 99 (0.007756)\n",
      "60. feature 44 (0.007683)\n",
      "61. feature 64 (0.007606)\n",
      "62. feature 11 (0.007494)\n",
      "63. feature 12 (0.007428)\n",
      "64. feature 31 (0.007405)\n",
      "65. feature 26 (0.007321)\n",
      "66. feature 139 (0.007277)\n",
      "67. feature 19 (0.007261)\n",
      "68. feature 52 (0.007207)\n",
      "69. feature 117 (0.007147)\n",
      "70. feature 15 (0.007145)\n",
      "71. feature 9 (0.007144)\n",
      "72. feature 51 (0.007079)\n",
      "73. feature 136 (0.007043)\n",
      "74. feature 57 (0.007011)\n",
      "75. feature 46 (0.007011)\n",
      "76. feature 27 (0.006934)\n",
      "77. feature 16 (0.006878)\n",
      "78. feature 54 (0.006852)\n",
      "79. feature 47 (0.006812)\n",
      "80. feature 25 (0.006795)\n",
      "81. feature 30 (0.006751)\n",
      "82. feature 17 (0.006727)\n",
      "83. feature 115 (0.006681)\n",
      "84. feature 114 (0.006616)\n",
      "85. feature 49 (0.006594)\n",
      "86. feature 58 (0.006528)\n",
      "87. feature 97 (0.006508)\n",
      "88. feature 69 (0.006476)\n",
      "89. feature 85 (0.006384)\n",
      "90. feature 28 (0.006344)\n",
      "91. feature 55 (0.006341)\n",
      "92. feature 18 (0.006278)\n",
      "93. feature 65 (0.006257)\n",
      "94. feature 138 (0.006253)\n",
      "95. feature 100 (0.006217)\n",
      "96. feature 35 (0.006193)\n",
      "97. feature 45 (0.006105)\n",
      "98. feature 34 (0.006008)\n",
      "99. feature 89 (0.006005)\n",
      "100. feature 137 (0.005851)\n",
      "101. feature 56 (0.005760)\n",
      "102. feature 73 (0.005758)\n",
      "103. feature 13 (0.005740)\n",
      "104. feature 113 (0.005668)\n",
      "105. feature 72 (0.005652)\n",
      "106. feature 53 (0.005625)\n",
      "107. feature 59 (0.005572)\n",
      "108. feature 63 (0.005547)\n",
      "109. feature 32 (0.005374)\n",
      "110. feature 127 (0.005186)\n",
      "111. feature 103 (0.005145)\n",
      "112. feature 33 (0.005086)\n",
      "113. feature 60 (0.005047)\n",
      "114. feature 29 (0.005012)\n",
      "115. feature 75 (0.004888)\n",
      "116. feature 126 (0.004857)\n",
      "117. feature 124 (0.004839)\n",
      "118. feature 62 (0.004762)\n",
      "119. feature 4 (0.004650)\n",
      "120. feature 6 (0.004600)\n",
      "121. feature 130 (0.004516)\n",
      "122. feature 102 (0.004426)\n",
      "123. feature 42 (0.004405)\n",
      "124. feature 5 (0.004378)\n",
      "125. feature 125 (0.004317)\n",
      "126. feature 74 (0.004305)\n",
      "127. feature 41 (0.004295)\n",
      "128. feature 7 (0.004142)\n",
      "129. feature 101 (0.003933)\n",
      "130. feature 43 (0.003825)\n",
      "131. feature 129 (0.003803)\n",
      "132. feature 131 (0.003674)\n",
      "133. feature 61 (0.003659)\n",
      "134. feature 40 (0.003646)\n",
      "135. feature 128 (0.003317)\n",
      "136. feature 108 (0.002985)\n",
      "137. feature 110 (0.002872)\n",
      "138. feature 111 (0.002344)\n",
      "139. feature 109 (0.001979)\n",
      "140. feature 1 (0.000000)\n",
      "(18995, 50)\n",
      "(12664, 50)\n"
     ]
    }
   ],
   "source": [
    "y_train=df_train_labels[subtask2_labels[0]].values\n",
    "\n",
    "X_train,X_test = forest_fi(X_train,y_train,X_test)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape and sum of y_train\n",
      "(18995,)\n",
      "1088.0\n",
      "[LibSVM]C: 0.020000\n",
      "Computed class weight: [ 1.  16.5]\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]train score= 0.903238\n",
      "cv roc auc= 0.614251\n",
      "cv roc acc= 0.889866\n",
      "cv roc rec= 0.158990\n",
      "cv roc prec= 0.128391\n",
      "[0.49164373 0.32760594 0.45562297 ... 0.39078787 0.38425937 0.3171537 ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.26747633 0.35315686 0.36767042 ... 0.38633546 0.41357981 0.39068914]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_model2_1(X_train,y_train,hps):\n",
    "    C = hps[\"C\"]\n",
    "    #class_weight={1:16.5}\n",
    "    class_weight={1:hps[\"w1\"]}\n",
    "    print(\"Shape and sum of y_train\")\n",
    "    print(np.shape(y_train))\n",
    "    print(np.sum(y_train))\n",
    "    \n",
    "    clf = svm.SVC(C=hps[\"C\"],\n",
    "                  kernel=\"rbf\",\n",
    "                  gamma=hps[\"gamma\"],\n",
    "                  class_weight=class_weight,\n",
    "                  decision_function_shape='ovo',\n",
    "                  verbose=True).fit(X_train,y_train)\n",
    "    print(\"C: %f\"%hps[\"C\"])\n",
    "    print(\"gamma: %f\"%hps[\"gamma\"])\n",
    "    print(\"Computed class weight: %s\"%(str(clf.class_weight_)))\n",
    "    return(clf)\n",
    "\n",
    "for i in range(0,len(subtask2_labels)):\n",
    "#for i in range(0,1):\n",
    "    y_train=df_train_labels[subtask2_labels[i]].values\n",
    "    \n",
    "    #X_train,X_test = forest_fi(X_train,y_train,X_test)\n",
    "    #print(np.shape(X_train))\n",
    "    #print(np.shape(X_test))\n",
    "    \n",
    "    hps = {\"C\":0.01,\"w1\":16.5}\n",
    "    clf = fit_model2_1(X_train,y_train,hps)\n",
    "    \n",
    "    score = clf.score(X_train,y_train)\n",
    "    cv_results = cross_validate(clf,X_train,y_train,cv=5,\n",
    "            scoring=[\"roc_auc\",\"accuracy\",\"recall\",\"precision\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv roc auc= %f\"%np.mean(cv_results['test_roc_auc']))\n",
    "    print(\"cv acc= %f\"%np.mean(cv_results['test_accuracy']))\n",
    "    print(\"cv rec= %f\"%np.mean(cv_results['test_recall']))\n",
    "    print(\"cv prec= %f\"%np.mean(cv_results['test_precision']))\n",
    "    \n",
    "    p_train_pred = sigmoid(clf.decision_function(X_train))\n",
    "    print(p_train_pred)\n",
    "    y_train_pred=clf.predict(X_train)\n",
    "    print(y_train_pred)\n",
    "\n",
    "    p_test = sigmoid(clf.decision_function(X_test))\n",
    "    print(p_test)\n",
    "    y_test=clf.predict(X_test)\n",
    "    print(y_test)    \n",
    "    \n",
    "    \n",
    "    df_test_labels[subtask2_labels[i]] = p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([12.77615499, 12.41247392, 13.07640409, 12.80517101, 13.12212396]),\n",
       " 'score_time': array([4.23895717, 4.22012997, 4.27067518, 4.14411497, 4.33628631]),\n",
       " 'test_roc_auc': array([0.6044624 , 0.62895249, 0.60144775, 0.63497972, 0.60141317]),\n",
       " 'test_accuracy': array([0.89260332, 0.88839168, 0.88865491, 0.89523559, 0.88444327]),\n",
       " 'test_recall': array([0.15207373, 0.14746544, 0.14220183, 0.19266055, 0.16055046])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0.49164373 0.32760594 0.45562297 0.46058998 0.40306392 0.50380012\n",
      " 0.39336716 0.31725881 0.40259307 0.40036175 0.67684214 0.35693574\n",
      " 0.31235669 0.64348676 0.37036891 0.27736634 0.21949271 0.20142098\n",
      " 0.41215118 0.36596041 0.33392901 0.4886425  0.51236136 0.38721617\n",
      " 0.31835596 0.44380282 0.36386215 0.44960421 0.27343254 0.37054091\n",
      " 0.80081044 0.73106033 0.2689407  0.35723011 0.37566261 0.26166432\n",
      " 0.38987883 0.20417919 0.26894171 0.29704054 0.34187004 0.32944449\n",
      " 0.35843485 0.32525159 0.27484312 0.32779779 0.26305    0.35662632\n",
      " 0.37846933 0.42261419 0.7509593  0.29796927 0.52051577 0.4558208\n",
      " 0.40334248 0.36366171 0.33484385 0.30170661 0.40393645 0.33366598\n",
      " 0.34785376 0.43036587 0.3667502  0.36831506 0.36498857 0.28979863\n",
      " 0.48090698 0.34147915 0.47218947 0.38446218 0.28134941 0.35240147\n",
      " 0.34025911 0.49736136 0.36968103 0.35140865 0.40582689 0.47808805\n",
      " 0.36363915 0.40169562 0.08596893 0.37862457 0.4313935  0.511751\n",
      " 0.34160409 0.36603612 0.31385569 0.36680531 0.40812583 0.35900572\n",
      " 0.46361364 0.36274629 0.36138135 0.38929942 0.30485985 0.35745188\n",
      " 0.32657575 0.08688126 0.39414227 0.42090754]\n",
      "18995\n",
      "1088.0\n",
      "7053.120332020358\n",
      "1538.0\n",
      "12664\n",
      "4723.418315258319\n",
      "1007.0\n",
      "0.9995467449084064\n",
      "0.9999961891073489\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:100])\n",
    "print(y_train_pred[:100])\n",
    "print(p_train_pred[:100])\n",
    "\n",
    "print(len(y_train_pred))\n",
    "print(np.sum(y_train))\n",
    "print(np.sum(p_train_pred))\n",
    "print(np.sum(y_train_pred))\n",
    "\n",
    "print(len(y_test))\n",
    "print(np.sum(p_test))\n",
    "print(np.sum(y_test))\n",
    "\n",
    "\n",
    "print(np.max(p_train_pred))\n",
    "print(np.max(p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394.0\n",
      "1088.0\n",
      "1538.0\n",
      "0.36213235294117646\n"
     ]
    }
   ],
   "source": [
    "a = np.sum(y_train_pred*y_train)\n",
    "b = np.sum(y_train)\n",
    "c = np.sum(y_train_pred)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pid', 'LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2', 'LABEL_Sepsis', 'LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
      "0        0.666441\n",
      "1        0.281373\n",
      "2        0.271900\n",
      "3        0.226549\n",
      "4        0.301836\n",
      "           ...   \n",
      "12659    0.446852\n",
      "12660    0.666563\n",
      "12661    0.430392\n",
      "12662    0.668956\n",
      "12663    0.482715\n",
      "Name: LABEL_Sepsis, Length: 12664, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(list(df_test_labels))\n",
    "print(df_test_labels['LABEL_Sepsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 140)\n",
      "(12664, 140)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18995\n",
      "16\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "12.0\n",
      "12664\n",
      "16\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train3_raw = feats_2_X3(df_train_feats,subtask3_feats)\n",
    "X_test3_raw = feats_2_X3(df_test_feats,subtask3_feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 16)\n",
      "(12664, 16)\n",
      "(18995, 16)\n",
      "(12664, 16)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train3_raw))\n",
    "print(np.shape(X_test3_raw))\n",
    "\n",
    "X_train,X_test = impute(X_train3_raw,X_test3_raw)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "#X_train,X_test = standardize(X_train,X_test)\n",
    "#print(np.shape(X_train))\n",
    "#print(np.shape(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "train score= 0.469659\n",
      "cv r2= 0.429349\n",
      "18.795959655719553\n",
      "2.2643897367955055\n",
      "18.831825937686546\n",
      "2.301117941599186\n",
      "i=1\n",
      "train score= 0.641700\n",
      "cv r2= 0.614812\n",
      "82.51117094000384\n",
      "9.874874156379732\n",
      "82.40448697650443\n",
      "9.823841059696093\n",
      "i=2\n",
      "train score= 0.533334\n",
      "cv r2= 0.407090\n",
      "96.94731096288041\n",
      "1.4393394576192375\n",
      "96.96782075164192\n",
      "1.1876907781679957\n",
      "i=3\n",
      "train score= 0.688220\n",
      "cv r2= 0.664918\n",
      "84.11971626206534\n",
      "11.873442425327944\n",
      "84.185106462612\n",
      "11.826091584426768\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(subtask3_labels)):\n",
    "#for i in range(0,1):\n",
    "    print(\"i=%d\"%i)\n",
    "    y_train=df_train_labels[subtask3_labels[i]].values\n",
    "    reg = fit_model3(X_train,y_train)\n",
    "    \n",
    "    score = reg.score(X_train,y_train)\n",
    "    cv_results = cross_validate(reg,X_train,y_train,cv=5,\n",
    "            scoring=[\"r2\"])\n",
    "    \n",
    "    print(\"train score= %f\"%score)\n",
    "    print(\"cv r2= %f\"%np.mean(cv_results['test_r2']))\n",
    "    \n",
    "    y_train_pred=reg.predict(X_train)\n",
    "    print(np.mean(y_train_pred))\n",
    "    print(np.std(y_train_pred))\n",
    "    \n",
    "    y_test=reg.predict(X_test)\n",
    "    print(np.mean(y_test))\n",
    "    print(np.std(y_test))\n",
    "    \n",
    "    df_test_labels[subtask3_labels[i]] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 10001 10003 ...  9992  9994  9997]\n",
      "[86.47186679 93.90677518 87.82147566 ... 82.41350651 97.39654798\n",
      " 86.88534173]\n",
      "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
      "0          0          0.994959          0.999994   0.981411   \n",
      "1      10001          0.049875          0.029881   0.384405   \n",
      "2      10003          0.027805          0.029881   0.122498   \n",
      "3      10004          0.108550          0.029881   0.298896   \n",
      "4      10005          0.100144          0.022194   0.108666   \n",
      "...      ...               ...               ...        ...   \n",
      "12659   9989          0.633362          0.022194   0.158406   \n",
      "12660   9991          0.210318          0.104080   0.096905   \n",
      "12661   9992          0.457877          0.029881   0.121389   \n",
      "12662   9994          0.981817          0.035027   0.919459   \n",
      "12663   9997          0.946191          0.022194   0.155260   \n",
      "\n",
      "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
      "0                0.941621               0.970611       0.647479   \n",
      "1                0.318839               0.348683       0.090423   \n",
      "2                0.181366               0.197771       0.576009   \n",
      "3                0.395096               0.374378       0.063629   \n",
      "4                0.108329               0.101088       0.069175   \n",
      "...                   ...                    ...            ...   \n",
      "12659            0.149537               0.123795       0.325749   \n",
      "12660            0.183593               0.092373       0.035034   \n",
      "12661            0.028937               0.118944       0.158103   \n",
      "12662            0.674309               0.847690       0.990193   \n",
      "12663            0.147542               0.121502       0.361958   \n",
      "\n",
      "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
      "0             0.026046    0.007740                0.165367     0.015844   \n",
      "1             0.047606    0.088610                0.026345     0.015844   \n",
      "2             0.047606    0.244693                0.026345     0.015844   \n",
      "3             0.052066    0.100864                0.013509     0.015844   \n",
      "4             0.011201    0.064041                0.026345     0.015844   \n",
      "...                ...         ...                     ...          ...   \n",
      "12659         0.008643    0.051403                0.013509     0.015844   \n",
      "12660         0.012848    0.001777                0.013509     0.111137   \n",
      "12661         0.007246    0.596600                0.013509     0.015844   \n",
      "12662         0.023573    0.954520                0.026345     0.015844   \n",
      "12663         0.007246    0.107377                0.026345     0.015844   \n",
      "\n",
      "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
      "0          0.666441    15.329609   79.318640   98.606996        86.471867  \n",
      "1          0.281373    16.918916   92.538808   94.955577        93.906775  \n",
      "2          0.271900    17.337657   80.111442   98.458517        87.821476  \n",
      "3          0.226549    16.666955   73.934671   95.608431        90.706453  \n",
      "4          0.301836    19.331331   73.934132   95.785930        60.906435  \n",
      "...             ...          ...         ...         ...              ...  \n",
      "12659      0.446852    20.552725   76.127907   95.736979       101.729947  \n",
      "12660      0.666563    18.370153   92.993427   98.733752        73.281689  \n",
      "12661      0.430392    19.357589   67.160455   97.452558        82.413507  \n",
      "12662      0.668956    16.095408   92.898073   97.372465        97.396548  \n",
      "12663      0.482715    17.718050   75.597544   98.292672        86.885342  \n",
      "\n",
      "[12664 rows x 16 columns]\n",
      "3397.76318937941\n"
     ]
    }
   ],
   "source": [
    "print(test_pids)\n",
    "print(y_test)\n",
    "print(df_test_labels)\n",
    "print(np.sum(df_test_labels['LABEL_BaseExcess']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_labels.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "X, y = datasets.load_digits(n_class=9, return_X_y=True)\n",
    "data = X / 16.\n",
    "\n",
    "clf1 = svm.SVC()\n",
    "clf1.fit(data,y)\n",
    "print(clf1.score(data,y))\n",
    "\n",
    "feature_map_nystroem = Nystroem(gamma=.2,\n",
    "                                random_state=1,\n",
    "                                n_components=100)\n",
    "feature_map_nystroem.fit(data)\n",
    "Q = feature_map_nystroem.transform(data)\n",
    "sqrt_k_inv = np.linalg.inv(feature_map_nystroem.normalization_)\n",
    "B = np.dot(Q,sqrt_k_inv)\n",
    "K = np.dot(B,np.transpose(B))\n",
    "\n",
    "clf2 = svm.SVC(kernel=\"precomputed\")\n",
    "clf2.fit(K,y)\n",
    "print(clf2.score(K,y))\n",
    "\n",
    "#clf.fit(data_transformed, y)\n",
    "#clf.score(data_transformed, y)\n",
    "\n",
    "\n",
    "\n",
    "a = np.array([1,np.nan,2])\n",
    "print(np.mean(a[~np.isnan(a)]))\n",
    "\n",
    "b = np.array([np.nan,np.nan,np.nan])\n",
    "print(np.min(b[~np.isnan(b)]))\n",
    "\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "X, y = datasets.load_digits(n_class=9, return_X_y=True)\n",
    "data = X / 16.\n",
    "clf = svm.LinearSVC()\n",
    "feature_map_nystroem = Nystroem(gamma=.2,\n",
    "                                random_state=1,\n",
    "                                n_components=300)\n",
    "data_transformed = feature_map_nystroem.fit_transform(data)\n",
    "clf.fit(data_transformed, y)\n",
    "\n",
    "clf.score(data_transformed, y)\n",
    "\n",
    "print(np.shape(data))\n",
    "print(np.shape(data_transformed))\n",
    "\n",
    "print(np.shape(data))\n",
    "print(np.shape(data_transformed))\n",
    "\n",
    "\n",
    "#X_train2_raw = feats_2_X2(df_train_feats,active_feats)\n",
    "df_feats = df_train_feats\n",
    "feats_list = subtask3_feats\n",
    "n_derived_feats = 2\n",
    "\n",
    "pids = pd.unique(df_feats['pid'])\n",
    "n_patients = len(pids)\n",
    "#df_train_feats2 = pd.DataFrame(data={'pid':pids})\n",
    "\n",
    "print(n_patients)\n",
    "print(n_derived_feats*len(feats_list))\n",
    "X = np.nan*np.ones((n_patients,n_derived_feats*len(feats_list)))\n",
    "\n",
    "patient_df_sizes = np.zeros(n_patients)\n",
    "for i in range(0,1):\n",
    "#for i in range(0,n_patients):\n",
    "    patient_df = df_feats[df_feats['pid']==pids[i]]\n",
    "    patient_df_sizes[i] = patient_df.shape[0]\n",
    "    for j in range(0,1):\n",
    "        patient_feat = patient_df[feats_list[j]].values\n",
    "        times = (patient_df['Time'].values)[~np.isnan(patient_feat)]\n",
    "        patient_feat = patient_feat[~np.isnan(patient_feat)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
